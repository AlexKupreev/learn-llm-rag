[
  {
    "original_id": "41962808",
    "title": "An Ed-Tech Tragedy?",
    "url": "https://www.unesco.org/en/digital-education/ed-tech-tragedy",
    "score": 1,
    "timestamp": "2024-10-27T15:24:05",
    "source": "Hacker News",
    "content": "The COVID-19 pandemic pushed education from schools to educational technologies at a pace and scale with no historical precedent. For hundreds of millions of students formal learning became fully dependent on technology \u2013 whether internet-connected digital devices, televisions or radios. An Ed-Tech Tragedy? examines the numerous adverse and unintended consequences of the shift to ed-tech. It documents how technology-first solutions left a global majority of learners behind and details the many ways education was diminished even when technology was available and worked as intended.In unpacking what went wrong, the book extracts lessons and recommendations to ensure that technology facilitates, rather than subverts, efforts to ensure the universal provision of inclusive, equitable and human-centred public education.",
    "comments": [],
    "description": "A new book about experiences with educational technologies during the COVID-19 pandemic and the implications for the future of learning",
    "document_uid": "289f56e4e6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962798",
    "title": "Australia-to-Singapore clean energy cable gets green light",
    "url": "https://newatlas.com/energy/suncable-australia-asia-power-link-approval-update/",
    "score": 1,
    "timestamp": "2024-10-27T15:21:45",
    "source": "Hacker News",
    "content": "The world's largest renewable energy and transmission project has received key approval from government officials. The Australia-Asia Power Link project will send Australian solar power to Singapore via 4,300 kilometer-long undersea cables.The AAPowerLink project is being led by SunCable, and will start by constructing a mammoth solar farm in Australia's Northern Territory to transmit around-the-clock clean power to Darwin, and also export \"reliable, cost-competitive renewable energy\" to Singapore.The principal environmental approval recently obtained from the Northern Territory Government rubber stamps the building of a solar farm at Powell Creek with a clean energy generation capacity of up to 10 gigawatts, plus utility scale onsite storage. It also green lights an 800-km (~500-mile) overhead transmission line between the solar precinct and Murrumujuk near Darwin.A converter facility will convert electricity from high-voltage direct current to high-voltage alternating current to supply Darwin \u2013 with the setup expected to supply \"up to 4GW of 24/7 green electricity to green industrial customers.\" This will be rolled out over two stages, the first delivering 900 megawatts, and then second adding a further 3 gigawatts. The Australia-Asia Power Link project will export solar power generated from an enormous clean energy precinct in the Northern Territories via subsea cables to SingaporeSunCable The project also aims to convert another 1.75 GW of power from AC to DC and send it through 4,300 km (over 2,670 miles) of subsea cabling to Singapore. The environmental approval will allow the company to lay cable from Darwin converter station past the end of Australian territorial waters and up to the Indonesian border.The company, which was acquired by billionaire Mike Cannon-Brookes last year after a bidding war with former project partner Andrew Forrest, still has a number of large hurdles to jump over before the AAPowerLink project really gets going though. These include negotiating land use with traditional owners, nailing down agreements with other bodies along the route and even actually financing the ambitious project.\"SunCable is delighted to receive environmental approval from the Northern Territory Government to proceed with our flagship Australia-Asia Power Link project,\" said company MD, Cameron Garnsworthy. \"This approval allows us to progress the development, commercial, and engineering activities required to advance the project to Final Investment Decision targeted in 2027.\"If all of the dominoes line up perfectly, supply of the first clean electricity is estimated to start in the early 2030s. An overview graphic on the project page (reproduced above) shows that the eventual end game for the Powell Creek development appears to be the generation of up to 20 GW of peak solar power and have some 36-42 GWh of battery storage on site. The video below has more.Source: SunCable",
    "comments": [],
    "description": "The world's largest renewable energy and transmission project has received key approval from government officials. The Australia-Asia Power Link project will send Australian solar power to Singapore via 4,300 kilometer-long undersea cables.",
    "document_uid": "adb4c4f1f9",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962796",
    "title": "A database for dietary AGEs and associated exposure assessment",
    "url": "https://www.sciencedirect.com/science/article/pii/S221345302400096X",
    "score": 1,
    "timestamp": "2024-10-27T15:21:32",
    "source": "Hacker News",
    "content": "! There was a problem providing the content you requested Please contact us via our support center for more information and provide the details below. Reference Number: 8d935b135b60bbd6 IP Address: 78.11.201.230 User Agent: Timestamp: ::CLOUDFLARE_ERROR_1000S_BOX::",
    "comments": [],
    "description": "No description available.",
    "document_uid": "a83e46a4df",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962775",
    "title": "'Aerial arms race' with birds may have turned ancient cicadas into ace fliers",
    "url": "https://www.science.org/content/article/aerial-arms-race-birds-may-have-turned-ancient-cicadas-ace-fliers",
    "score": 1,
    "timestamp": "2024-10-27T15:16:26",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "e57a204f75",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962771",
    "title": "Recurrent Neural Networks for Deception Detection in Videos",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-03884-6_29",
    "score": 1,
    "timestamp": "2024-10-27T15:15:33",
    "source": "Hacker News",
    "content": "Avola, D., Cinque, L., Foresti, G.L., Pannone, D.: Automatic deception detection in RGB videos using facial action units. In: ICDSC, pp. 1\u20136. ACM (2019) Google Scholar Benedict, S.R., Kumar, J.S.: Geometric shaped facial feature extraction for face recognition. In: ICACA, pp. 275\u2013278 (2016) Google Scholar Chao, L., Tao, J., Yang, M., Li, Y., Wen, Z.: Long short term memory recurrent neural network based encoding method for emotion recognition in video. In: ICASSP, pp. 2752\u20132756. IEEE (2016) Google Scholar Cho, K., et al.: Learning phrase representations using RNN encoder-decoder for statistical machine translation. In: EMNLP. ACL (2014) Google Scholar Chung, J., G\u00fcl\u00e7ehre, \u00c7., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural networks on sequence modeling. In: NeurIPS Workshops (2014) Google Scholar Curci, A., Lanciano, T., Battista, F., Guaragno, S., Ribatti, R.M.: Accuracy, confidence, and experiential criteria for lie detection through a videotaped interview. Front. Psychiatry 9, 748 (2019) Google Scholar Datta, A.K., Datta, M., Banerjee, P.K.: Face Detection and Recognition: Theory and Practice. CRC Press (2015) Google Scholar Ganis, G., Rosenfeld, J.P., Meixner, J., Kievit, R.A., Schendan, H.E.: Lying in the scanner: covert countermeasures disrupt deception detection by functional magnetic resonance imaging. NeuroImage 55(1), 312\u2013319 (2011) Google Scholar Goodfellow, I.J., Bengio, Y., Courville, A.C.: Deep Learning. MIT Press, Adaptive computation and machine learning (2016)MATH Google Scholar Graves, A.: Supervised Sequence Labelling with Recurrent Neural Networks. SCI, vol. 385, pp. 5\u201313. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-24797-2Book MATH Google Scholar Han, H., Zhu, X., Li, Y.: Generalizing long short-term memory network for deep learning from generic data. ACM Trans. Knowl. Discov. Data 14(2), 1\u201328 (2020) Google Scholar Huang, J., Ling, C.X.: Using AUC and accuracy in evaluating learning algorithms. IEEE Trans. Knowl. Data Eng. 17(3), 299\u2013310 (2005) Google Scholar Karimi, H., Tang, J., Li, Y.: Toward end-to-end deception detection in videos. In: IEEE BigData, pp. 1278\u20131283. IEEE (2018) Google Scholar Khan, W., Crockett, K.A., O\u2019Shea, J., Hussain, A., Khan, B.M.: Deception in the eyes of deceiver: A computer vision and machine learning based automated deception detection. Expert Syst. Appl. 169, 114341 (2021) Google Scholar Krishnamurthy, G., Majumder, N., Poria, S., Cambria, E.: A deep learning approach for multimodal deception detection. In: CICLing (2018) Google Scholar Leon-Urbano, C., Ugarte, W.: End-to-end electroencephalogram (EEG) motor imagery classification with long short-term. In: SSCI, pp. 2814\u20132820. IEEE (2020) Google Scholar Lloyd, E.P., Deska, J.C., Hugenberg, K., McConnell, A.R., Humphrey, B.T., Kunstman, J.W.: Miami university deception detection database. Behav. Res. Meth. 51 (2019) Google Scholar Masip, J.: Deception detection: state of the art and future prospects. Psicothema 29(2), pp. 149\u2013159 (2017) Google Scholar O\u2019Shea, J., Crockett, K.A., Khan, W., Kindynis, P., Antoniades, A., Boultadakis, G.: Intelligent deception detection through machine based interviewing. In: IJCNN. IEEE, pp. 1\u20138 (2018) Google Scholar Rosebrock, A.: Deep learning for computer vision with Python. PyImageSearch (2017) Google Scholar Roulin, N., Ternes, M.: Is it time to kill the detection wizard? emotional intelligence does not facilitate deception detection. Pers. Individ. Diff. 137, 131\u2013138 (2019) Google Scholar Sagonas, C., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces in-the-wild challenge: the first facial landmark localization challenge. In: ICCV Workshops, pp. 397\u2013403. IEEE Computer Society (2013) Google Scholar Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE Trans. Sign. Process. 45(11), 2673\u20132681 (1997) Google Scholar Stewart, S.L., Wright, C., Atherton, C.: Deception detection and truth detection are dependent on different cognitive and emotional traits: an investigation of emotional intelligence, theory of mind, and attention. Pers. Soc. Psychol. Bull. 45(5), 794\u2013807 (2019) Google Scholar Sun, B., Cao, S., Li, D., He, J., Yu, L.: Dynamic micro-expression recognition using knowledge distillation. IEEE Trans. Affect. Comput. (2020) Google Scholar Ugarte, W., Boizumault, P., Loudni, S., Cr\u00e9milleux, B., Lepailleur, A.: Soft constraints for pattern mining. J. Intell. Inf. Syst. 44(2), 193\u2013221 (2013). https://doi.org/10.1007/s10844-013-0281-4Article MATH Google Scholar Venkatesh, S., Raghavendra, R., Bours, P.: Robust algorithm for multimodal deception detection. In: MIPR. IEEE, pp. 534\u2013537 (2019) Google Scholar Wu, Z., Singh, B., Davis, L.S., Subrahmanian, V.S.: Deception detection in videos. In: AAAI, vol. 32, no. 1 (2018) Google Scholar Yan, W.J., Wu, Q., Liang, J., Chen, Y.H., Fu, X.: How fast are the leaked facial expressions: the duration of micro-expressions. J. Nonverbal Behav. 37(4), 217\u2013230 (2013) Google Scholar Zhao, G., Li, X.: Automatic micro-expression analysis: open challenges. Front. Psychol. 10, 1833 (2019) Google Scholar",
    "comments": [],
    "description": "Deception detection has always been of subject of interest. After all, determining if a person is telling the truth or not could be detrimental in many real-world cases. Current methods to discern deceptions require expensive equipment that need specialists to read...",
    "document_uid": "03cabe209f",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962757",
    "title": "The Knowledge Graph: things, not strings (2012)",
    "url": "https://blog.google/products/search/introducing-knowledge-graph-things-not/",
    "score": 1,
    "timestamp": "2024-10-27T15:11:54",
    "source": "Hacker News",
    "content": "Search is a lot about discovery\u2014the basic human need to learn and broaden your horizons. But searching still requires a lot of hard work by you, the user. So today I\u2019m really excited to launch the Knowledge Graph, which will help you discover new information quickly and easily.Take a query like [taj mahal]. For more than four decades, search has essentially been about matching keywords to queries. To a search engine the words [taj mahal] have been just that\u2014two words.But we all know that [taj mahal] has a much richer meaning. You might think of one of the world\u2019s most beautiful monuments, or a Grammy Award-winning musician, or possibly even a casino in Atlantic City, NJ. Or, depending on when you last ate, the nearest Indian restaurant. It\u2019s why we\u2019ve been working on an intelligent model\u2014in geek-speak, a \u201cgraph\u201d\u2014that understands real-world entities and their relationships to one another: things, not strings.The Knowledge Graph enables you to search for things, people or places that Google knows about\u2014landmarks, celebrities, cities, sports teams, buildings, geographical features, movies, celestial objects, works of art and more\u2014and instantly get information that\u2019s relevant to your query. This is a critical first step towards building the next generation of search, which taps into the collective intelligence of the web and understands the world a bit more like people do.Google\u2019s Knowledge Graph isn\u2019t just rooted in public sources such as Freebase, Wikipedia and the CIA World Factbook. It\u2019s also augmented at a much larger scale\u2014because we\u2019re focused on comprehensive breadth and depth. It currently contains more than 500 million objects, as well as more than 3.5 billion facts about and relationships between these different objects. And it\u2019s tuned based on what people search for, and what we find out on the web.The Knowledge Graph enhances Google Search in three main ways to start:1. Find the right thingLanguage can be ambiguous\u2014do you mean Taj Mahal the monument, or Taj Mahal the musician? Now Google understands the difference, and can narrow your search results just to the one you mean\u2014just click on one of the links to see that particular slice of results:",
    "comments": [],
    "description": "We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.",
    "document_uid": "1e64e21ec9",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962750",
    "title": "50 Years Ago, Sugar Industry Paid Scientists to Point Blame at Fat",
    "url": "https://www.npr.org/sections/thetwo-way/2016/09/13/493739074/50-years-ago-sugar-industry-quietly-paid-scientists-to-point-blame-at-fat",
    "score": 4,
    "timestamp": "2024-10-27T15:10:47",
    "source": "Hacker News",
    "content": "A newly discovered cache of internal documents reveals that the sugar industry downplayed the risks of sugar in the 1960s. Luis Ascui/Getty Images hide caption toggle caption Luis Ascui/Getty Images A newly discovered cache of internal documents reveals that the sugar industry downplayed the risks of sugar in the 1960s. Luis Ascui/Getty Images In the 1960s, the sugar industry funded research that downplayed the risks of sugar and highlighted the hazards of fat, according to a newly published article in JAMA Internal Medicine. The article draws on internal documents to show that an industry group called the Sugar Research Foundation wanted to \"refute\" concerns about sugar's possible role in heart disease. The SRF then sponsored research by Harvard scientists that did just that. The result was published in the New England Journal of Medicine in 1967, with no disclosure of the sugar industry funding. The sugar-funded project in question was a literature review, examining a variety of studies and experiments. It suggested there were major problems with all the studies that implicated sugar, and concluded that cutting fat out of American diets was the best way to address coronary heart disease. The authors of the new article say that for the past five decades, the sugar industry has been attempting to influence the scientific debate over the relative risks of sugar and fat. \"It was a very smart thing the sugar industry did, because review papers, especially if you get them published in a very prominent journal, tend to shape the overall scientific discussion,\" co-author Stanton Glantz told The New York Times. Money on the line In the article, published Monday, authors Glantz, Cristin Kearns and Laura Schmidt aren't trying make the case for a link between sugar and coronary heart disease. Their interest is in the process. They say the documents reveal the sugar industry attempting to influence scientific inquiry and debate. The researchers note that they worked under some limitations \u2014 \"We could not interview key actors involved in this historical episode because they have died,\" they write. Other organizations were also advocating concerns about fat, they note. There's no evidence that the SRF directly edited the manuscript published by the Harvard scientists in 1967, but there is \"circumstantial\" evidence that the interests of the sugar lobby shaped the conclusions of the review, the researchers say. For one thing, there's motivation and intent. In 1954, the researchers note, the president of the SRF gave a speech describing a great business opportunity. If Americans could be persuaded to eat a lower-fat diet \u2014 for the sake of their health \u2014 they would need to replace that fat with something else. America's per capita sugar consumption could go up by a third. But in the '60s, the SRF became aware of \"flowing reports that sugar is a less desirable dietary source of calories than other carbohydrates,\" as John Hickson, SRF vice president and director of research, put it in one document. He recommended that the industry fund its own studies \u2014 \"Then we can publish the data and refute our detractors.\" The next year, after several scientific articles were published suggesting a link between sucrose and coronary heart disease, the SRF approved the literature-review project. It wound up paying approximately $50,000 in today's dollars for the research. One of the researchers was the chairman of Harvard's Public Health Nutrition Department \u2014 and an ad hoc member of SRF's board. \"A different standard\" for different studies Glantz, Kearns and Schmidt say many of the articles examined in the review were hand-selected by SRF, and it was implied that the sugar industry would expect them to be critiqued. In a letter, SRF's Hickson said that the organization's \"particular interest\" was in evaluating studies focused on \"carbohydrates in the form of sucrose.\" \"We are well aware,\" one of the scientists replied, \"and will cover this as well as we can.\" The project wound up taking longer than expected, because more and more studies were being released that suggested sugar might be linked to coronary heart disease. But it was finally published in 1967. Hickson was certainly happy with the result: \"Let me assure you this is quite what we had in mind and we look forward to its appearance in print,\" he told one of the scientists. The review minimized the significance of research that suggested sugar could play a role in coronary heart disease. In some cases the scientists alleged investigator incompetence or flawed methodology. \"It is always appropriate to question the validity of individual studies,\" Kearns told Bloomberg via email. But, she says, \"the authors applied a different standard\" to different studies \u2014 looking very critically at research that implicated sugar, and ignoring problems with studies that found dangers in fat. Epidemiological studies of sugar consumption \u2014 which look at patterns of health and disease in the real world \u2014 were dismissed for having too many possible factors getting in the way. Experimental studies were dismissed for being too dissimilar to real life. One study that found a health benefit when people ate less sugar and more vegetables was dismissed because that dietary change was not feasible. Another study, in which rats were given a diet low in fat and high in sugar, was rejected because \"such diets are rarely consumed by man.\" The Harvard researchers then turned to studies that examined risks of fat \u2014 which included the same kind of epidemiological studies they had dismissed when it came to sugar. Citing \"few study characteristics and no quantitative results,\" as Kearns, Glantz and Schmidt put it, they concluded that cutting out fat was \"no doubt\" the best dietary intervention to prevent coronary heart disease. Sugar lobby: \"Transparency standards were not the norm\" In a statement, the Sugar Association \u2014 which evolved out of the SRF \u2014 said it is challenging to comment on events from so long ago. \"We acknowledge that the Sugar Research Foundation should have exercised greater transparency in all of its research activities, however, when the studies",
    "comments": [
      {
        "author": "zeristor",
        "text": "Sugar industry, tobacco industry, oil industry.<p>Which other industries have distorted reality, and which future ones will be revealed in the coming decades?",
        "time": "2024-10-27T15:12:34"
      }
    ],
    "description": "Documents show that in the '60s, the sugar industry funded Harvard researchers who, examining risk factors of heart disease, dismissed concerns about sugar and doubled down on the dangers of fat.",
    "document_uid": "e1e13da90b",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962732",
    "title": "Lab-grown diamonds: Popular with Gen Z, millennials",
    "url": "https://fortune.com/article/disadvantages-lab-grown-diamonds-gen-z-millennials-environment-cost-price-coal-china/",
    "score": 2,
    "timestamp": "2024-10-27T15:08:01",
    "source": "Hacker News",
    "content": "The muted sounds of hammering and sanding drift down to the first floor of Bario Neal, a jewelry store in Philadelphia, where rustic artwork that mimics nature hangs on warmly-lit walls. Waiting for one of those rings is Haley Farlow, a 28-year-old second grade teacher who has been designing her three-stone engagement ring with her boyfriend. They care about price and also don\u2019t want jewelry that takes a toll on the Earth, or exploits people in mining. So they\u2019re planning on buying diamonds grown in a laboratory. \u201cMost of my friends all have lab-grown. And I think it just fits our lifestyle and, you know, the economy and what we\u2019re living through,\u201d said Farlow. In the U.S., lab-grown diamond sales jumped 16% in 2023 from 2022, according to Edahn Golan, an industry analyst. They cost a fraction of the stones formed naturally underground. Social media posts show millennials and Generation Zs proudly explaining the purchase of their lab-grown diamonds for sustainability and ethical reasons. But how sustainable they are is questionable, since making a diamond requires an enormous amount of energy and many major manufacturers are not transparent about their operations. Farlow said the choice of lab-grown makes her ring \u201cmore special and fulfilling\u201d because the materials are sourced from reputable companies. All of the lab diamonds at Bario Neal are either made with renewable energy or have the emissions that go into making them countered with carbon credits, which pay for activities like planting trees, which capture carbon. But that\u2019s not the norm for lab-grown diamonds. The disadvantages of lab-grown diamonds Many companies are based in India, where about 75% of electricity comes from burning coal. They use words like \u201csustainable\u201d and \u201cenvironmentally-friendly\u201d on their websites, but don\u2019t post their environmental impact reports and aren\u2019t certified by third parties. Cupid Diamonds, for example, says on its website that it produces diamonds in \u201can environmentally friendly manner,\u201d but did not respond to questions about what makes its diamonds sustainable. Solar energy is rapidly expanding in India and there are some companies, such as Greenlab Diamonds, that utilize renewables in their manufacturing processes. China is the other major diamond manufacturing country. Henan Huanghe Whirlwind, Zhuhai Zhong Na Diamond, HeNan LiLiang Diamond, Starsgem Co. and Ningbo Crysdiam are among the largest producers. None returned requests for comment nor post details about where it gets its electricity. More than half of China\u2019s electricity came from coal in 2023. In the United States, one company, VRAI, whose parent company is Diamond Foundry, operates what it says is a zero-emissions foundry in Wenatchee, Washington, running on hydropower from the Columbia River. Martin Roscheisen, CEO and founder of Diamond Foundry, said via email the power VRAI uses to grow a diamond is \u201cabout one tenth of the energy required for mining.\u201d But Paul Zimnisky, a diamond industry expert, said companies that are transparent about their supply chain and use renewable energy like this \u201crepresent a very small portion of production.\u201d \u201cIt seems like there are a lot of companies that are riding on this coattail that it\u2019s an environmentally-friendly product when they aren\u2019t really doing anything that\u2019s environmentally friendly,\u201d said Zimnisky. How are lab-grown diamonds made? Lab diamonds are often made over several weeks, subjecting carbon to high pressure and high temperature that mimic natural conditions that form diamonds beneath the Earth\u2019s surface. The technology has been around since the 1950\u2019s, but the diamonds produced were mostly used in industries like stone cutting, mining and dentistry tools. Over time the laboratories, or foundries, have gotten better at growing stones with minimal flaws. Production costs have dropped as technology improves. That means diamond growers can manufacture as many stones as they want and choose their size and quality, which is causing prices to fall rapidly. Natural diamonds take billions of years to form and are difficult to find, making their price more stable. Lab-created vs natural diamonds Diamonds, whether lab-grown or natural, are chemically identical and entirely made out of carbon. But experts can distinguish between the two, using lasers to pinpoint telltale signs in atomic structure. The Gemological Institute of America grades millions of diamonds annually. With lower prices for lab-grown and young people increasingly preferring them, the new diamonds have cut into the market share for natural stones. Globally, lab-grown diamonds are now 5-6% of the market and the traditional industry is not taking it sitting down. The marketing battle is on. The mined diamond industry and some analysts warn lab-grown diamonds won\u2019t hold value over time. \u201cFive to ten years into the future, I think there\u2019s going to be very few customers that are willing to spend thousands of dollars for a lab diamond. I think almost all of it\u2019s going to sell in the $100 price point or even below,\u201d said Zimnisky. He predicts that natural diamonds will continue to sell in the thousands and tens of thousands of dollars for engagement rings. Are lab-grown diamonds worth it? Some cultures view engagement rings as investments and choose natural diamonds for their value over the long term. That\u2019s particularly true in China and India, Zimnisky said. It\u2019s also still true in more rural areas of the United States, while lab-grown diamonds have taken off more in the cities. Paying thousands of dollars for something that drops most of its value in just a few years can leave the buyer feeling cheated, which Golan said is an element that is currently working against the lab-grown sector. \u201cWhen you buy a natural diamond, there\u2019s a story that it is three billion years in the making by Mother Earth. This wondrous creation of nature \u2026 you cannot tell that story with a lab-grown,\u201d said Golan. \u201cYou very quickly make the connection between forever and the longevity of the love.\u201d \u201cIf we really want to get technical here, the greenest diamond is a repurposed or recycled diamond because that uses no energy,\u201d Zimnisky said. Page Neal said she co-founded Bario Neal in 2008 to \u201ccreate jewelry of lasting",
    "comments": [],
    "description": "Most don't know they're made in China and India from electricity generated by burning coal.",
    "document_uid": "5f5c0b79e3",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962698",
    "title": "Ethical Framework Principles for Climate Intervention Research",
    "url": "https://www.agu.org/ethicalframeworkprinciples",
    "score": 1,
    "timestamp": "2024-10-27T15:02:29",
    "source": "Hacker News",
    "content": "Ethical Framework Principles for Climate Intervention Research As interest in climate intervention, or geoengineering, rapidly grows in the urgency to address climate change, this Ethical Framework provides guidance for researchers, funders, and policymakers.",
    "comments": [],
    "description": "As interest in climate intervention, or geoengineering, rapidly grows in the urgency to address climate change, this Ethical Framework provides guidance for researchers, funders, and policymakers.",
    "document_uid": "983a7d1341",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962678",
    "title": "What 100-Year-Old Restaurant Menus Tell Us About Inflation",
    "url": "https://www.bloomberg.com/news/features/2024-10-25/what-100-year-old-restaurant-menu-prices-tell-us-about-inflation",
    "score": 2,
    "timestamp": "2024-10-27T14:59:44",
    "source": "Hacker News",
    "content": "We've detected unusual activity from your computer network To continue, please click the box below to let us know you're not a robot.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "daf5a3da56",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962627",
    "title": "Privacy4Cars",
    "url": "https://privacy4cars.com/",
    "score": 1,
    "timestamp": "2024-10-27T14:52:26",
    "source": "Hacker News",
    "content": "Privacy4Cars enables the automotive ecosystem to delete personal information from vehicles in a fast, traceable, and cost-effective manner to reduce liability, meet regulatory requirements and improve customer satisfaction",
    "comments": [],
    "description": "Privacy4Cars enables the automotive ecosystem to\u00a0delete personal information from vehicles\u00a0in a fast, traceable, and cost-effective manner\u00a0to\u00a0reduce liability, meet regulatory requirements\u00a0and improve customer satisfaction",
    "document_uid": "b8bd768829",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962621",
    "title": "The rise and fall of the Gucci Goddess [video]",
    "url": "https://www.youtube.com/watch?v=Qpk2_hDNhuc",
    "score": 1,
    "timestamp": "2024-10-27T14:51:49",
    "source": "Hacker News",
    "content": "The rise and fall of the Gucci Goddess [video]",
    "comments": [],
    "description": "Dive into the story of Janet Yamanaka Mellow, a financial program manager whose \"penchant for extravagance\" led to lies, scams and lots of Gucci.MrMarket cov...",
    "document_uid": "d552c051a1",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962614",
    "title": "How digital voting works in Kenya",
    "url": "https://privacyinternational.org/explainer/5445/election-technology-kenya",
    "score": 2,
    "timestamp": "2024-10-27T14:50:34",
    "source": "Hacker News",
    "content": "Results Transmission Process A digital copy of the Form 34A is made, and sent together (\"bundled\") with a textual representation of the results for centralised provisional tallying. The physical papers are couriered in parallel as an official record. 2013 Election - the first deployment of BVR in Kenya In the 2013 Elections, a significant number of polling stations saw the kits procured for both Electronic Voter Identification and Results Transmission failing. As of 2013, only 23% of Kenya had electricity, the school buildings being used as polling stations generally were not equipped with power outlets -- with this being particularly true in rural areas. As a result, when the laptops being used for Electronic Voter Identification began to run out of battery, they were unable to be charged, forcing clerks to resort to the printed register for manual verification of voters. In addition, the mobile phones supposed to pass tallies of provisional results for centralised calculation didn't work due to forgotten PINs, low battery, and data connectivity problems, with poll workers being airlifted by helicopter to Kenya's capital, Nairobi, to hand-deliver results to the IEBC. Confounding these troubles, the IEBC's centralised tallying servers overloaded and collapsed after processing only 17,000 of the 33,000 polling station results forcing the IEBC to suspend announcing provisional results, having to wait for the physical FORM 32As to arrive into Nairobi. Further, a \"computer bug\" was blamed for counting each rejected ballot 8 times in the initial tally - artificially inflating the number of rejected ballots to more than 330,000 instead of the correct figure of circa 41,500. After 6 days, opposition candidate Uhuru Kenyatta was announced as the winner, having polled 50.07% - a minuscule majority of just 8,000 out of 12 million ballots cast. The losing incumbent Raila Odinga alleged voter fraud and petitioned Kenya's Supreme Court for the nullification of the official results, which led to a recount of ballots at 22 polling stations. After this recount, Uhuru Kenyatta was affirmed as the President Elect of Kenya. The machines procured for the 2013 election were ultimately put into storage, with 125 of the Electronic Voter ID kits going on to be stolen. The Commission stated that these devices only contained raw registration data that has not been processed for inclusion in the register of voters, and that any data stored on the devices is encrypted at rest. 2017 Having deprecated and put the kits procured for the 2013 election into long-term storage, in 2017, the Kenyan government went on to contract OT Morpho/IDEMIA - the successor company to Safran Morpho - for 45,000 MorphoTablets, boasting the following specification: An 8-inch WXGA (800*1280) display Qualcomm Snapdragon 410 processor 2GB RAM 16GB internal storage expandable via Micro SD Card A 5100mAh battery said to last upto 24hours 13MP main camera and a 2MP front camera 3.5mm headphone jack Optical fingerprint reader Contact and Contactless smart card reader capabilities Dual SIM support with 4G voice and data support Android 5.0 Lollipop Bluetooth 4.1 (BLE) Each tablet contained a Micro SD card loaded with the roll of eligible voters for a given polling booth. As described in the Voting Procedure section, electors were identified by the MorphoTablet using their fingerprint. In addition to Electronic Voter Identification, the tablets were to be used for provisional results transmission. After filling out the official results form (Form 34A), the presiding officer at each station was to key into the tablets the numeric results, together with a scan of the form - referred to as a Bundle. During testing it had been demonstrated that the send icon on the tablet was disabled until the scanned form had been added[https://nation.africa/kenya/news/politics/IEBC-tests-results-transmission/1064-4042450-lpl4hmz/index.html]. Things turned out differently on the day, however. A configuration mistake on the tablets meant that the presiding officers were able to submit the numeric results without attaching a copy of the official results form (Form 34A). In addition, many of the Form 34As which were attached were simply photographs taken with the tablet's inbuilt camera rather than legible scans of the document - and at least one Form 34A was handwritten in a school exercise book To give connectivity for results transmission, the tablets were given SIM cards for two different Mobile Network Operators from Safaricom, Telkom Kenya, and Airtel Kenya which provided a VPN. However, 11,000 - one quarter of all polling stations - were said to be outside 3G range in the 2017 polls. The IEBC announced from provisional results that the incumbent President Uhuru Kenyatta had won the election with a margin of 9%, receiving 54% of the votes cast and beating the 50% threshold for a run-off. Accusations of hacking (\"Fungua server\") The opposition leader, Railia Odinga, told a news conference that \"the 2017 general election was a fraud\" and immediately petitioned the Kenyan Supreme Court to annul the vote. Just days before the 2017 elections, the chief of IT for KIEMS - Chris Msando - was found dead, having been tortured and murdered. The opposition claimed he had been killed after refusing to surrender a password to rig the election. Mr. Odinga further claimed that between 12:37 p.m. and 4:00 p.m. on the day of the election, hackers had used Chris Msando's login credentials to load \u201can algorithm which is a formula to create a percentage gap of 11 percent between our numbers\u201d - to doctor results from polling stations in favour of the incumbent. Responding to this, the IEBC's Chair dismissed the hacking claims, stating \u201chacking was attempted but did not succeed\u201d, with the IEBC's' Chief Executive going on to tell a news briefing: \u201cI wish to confirm that our elections management system is secure, [...] There were no external or internal interferences with the system at any point before, during and after the voting.\u201d The Swahili phrase Fungua server (\"Open the servers\") became a mantra for the opposition in Kenya. In response to Mr. Odinga's Court Petition, the Kenyan Supreme Court ordered IEBC to open its servers to inspection, which it refused to",
    "comments": [],
    "description": "Many democracies, particularly younger democracies, are increasingly looking to employ technology - including biometrics - to coordinate the running of their electoral processes.",
    "document_uid": "0359578a2b",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962608",
    "title": "Is there now a generation of users who never worked with files?",
    "url": "https://blog.hyperknot.com/p/is-there-now-a-generation-of-users",
    "score": 3,
    "timestamp": "2024-10-27T14:49:07",
    "source": "Hacker News",
    "content": "MapHub's UI has remained largely consistent since I launched it eight years ago, with a prominent \"Save\" button visible in the top-right corner of the map. It turns orange when a save is needed.For years, everyone understood they had to click the \"Save\" button to preserve their changes. However, recently, I've been receiving more and more support requests along the lines of: \"I lost all my work\" or \"I've been working on a map for days, and now I can't find it.\"I explained to everyone that there's no need to worry; even if they accidentally edit or delete their maps, they can always recover previous versions or even their deleted maps by clicking on the \"Version History\" or \"Restore Deleted Maps\" buttons.But more and more users were telling me they couldn't find anything. That's when I realized that many users today simply aren't used to saving files manually.They've grown up using cloud-based editors like Google Docs, where autosave is the default. Of course, autosave is only possible if you have a robust, automatic history-tracking system built into your app, which Google Docs certainly has.The thing is, you can only offer this feature if your app's architecture is designed from the ground up to support it. Since MapHub wasn't built that way, I had to come up with a solution to remind users that they need to save their work regularly.I added a notice reminding users to save their maps. It appears three times for each new user and also when there was no save for 10 minutes.This solution has worked wonderfully. It's one of those features where, after shipping it, we didn't hear anything more. The support requests stopped overnight, and no one is losing their work anymore.You can also follow me on X: @hyperknot",
    "comments": [],
    "description": "MapHub's UI has remained largely consistent since I launched it eight years ago, with a prominent \"Save\" button visible in the top-right corner of the map.",
    "document_uid": "10aac70a34",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962607",
    "title": "Diversity buffers winegrowing regions from climate change losses \u2013 PNAS",
    "url": "https://www.pnas.org/doi/abs/10.1073/pnas.1906731117#supplementary-materials",
    "score": 1,
    "timestamp": "2024-10-27T14:49:04",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "9bf61b04fa",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962599",
    "title": "Lascaux Cave Paintings",
    "url": "https://en.wikipedia.org/wiki/Lascaux",
    "score": 1,
    "timestamp": "2024-10-27T14:48:36",
    "source": "Hacker News",
    "content": "Caves in France containing Paleolithic paintings Lascaux ( la-SKOH,[1] lah-SKOH;[2] French: Grotte de Lascaux [\u0261\u0281\u0254t d\u0259 lasko],[3] \"Lascaux Cave\") is a network of caves near the village of Montignac, in the department of Dordogne in southwestern France. Over 600 parietal wall paintings cover the interior walls and ceilings of the cave. The paintings represent primarily large animals, typical local contemporary fauna that correspond with the fossil record of the Upper Paleolithic in the area. They are the combined effort of many generations. With continued debate, the age of the paintings is now usually estimated at around 17,000 - 22,000 years (early Magdalenian).[4][5][6] Because of the outstanding prehistoric art in the cave, Lascaux was inducted into the UNESCO World Heritage List in 1979, as an element of the Prehistoric Sites and Decorated Caves of the V\u00e9z\u00e8re Valley.[7] The original caves have been closed to the public since 1963, as their condition was quickly deteriorating, but there are now a number of replicas. History since rediscovery[edit] Modern entrance to the Lascaux cave On 12 September 1940, the entrance to the Lascaux Cave was discovered on the La Rochefoucauld-Montbel lands by 18-year-old Marcel Ravidat when his dog, Robot, investigated a hole left by an uprooted tree (Ravidat would embellish the story in later retellings, saying Robot had fallen into the cave.)[8][9] Ravidat returned to the scene with three friends, Jacques Marsal, Georges Agnel, and Simon Coencas. They entered the cave through a 15-metre-deep (50-foot) shaft that they believed might be a legendary secret passage to the nearby Lascaux Manor.[9][10][11] The teenagers discovered that the cave walls were covered with depictions of animals.[12][13] Galleries that suggest continuity, context or simply represent a cavern were given names. Those include the Hall of the Bulls, the Passageway, the Shaft, the Nave, the Apse, and the Chamber of Felines. They returned along with the Abb\u00e9 Henri Breuil on 21 September 1940; Breuil would make many sketches of the cave, some of which are used as study material today due to the extreme degradation of many of the paintings. Breuil was accompanied by Denis Peyrony, curator of Les eyzies (Prehistory Museum) at Les Eyzies, Jean Bouyssonie and Dr Cheynier. The cave complex was opened to the public on 14 July 1948, and initial archaeological investigations began a year later, focusing on the Shaft. By 1955, carbon dioxide, heat, humidity, and other contaminants produced by 1,200 visitors per day had visibly damaged the paintings. As air condition deteriorated, fungi and lichen increasingly infested the walls. Consequently, the cave was closed to the public in 1963, the paintings were restored to their original state, and a monitoring system on a daily basis was introduced. Part of Lascaux IV Conservation problems in the original cave have made the creation of replicas more important. Lascaux II, an exact copy of the Great Hall of the Bulls and the Painted Gallery was displayed at the Grand Palais in Paris, before being displayed from 1983 in the cave's vicinity (about 200 m or 660 ft away from the original cave), a compromise and attempt to present an impression of the paintings' scale and composition for the public without harming the originals.[10][13] A full range of Lascaux's parietal art is presented a few kilometres from the site at the Centre of Prehistoric Art, Le Parc du Thot, where there are also live animals representing ice-age fauna.[14] The paintings for this site were duplicated with the same type of materials (such as iron oxide, charcoal, and ochre) which were believed to be used 19,000 years ago.[9][15][16][17] Other facsimiles of Lascaux have also been produced over the years. Lascaux III is a series of five exact reproductions of the cave art (the Nave and Shaft) that, since 2012, have been exhibited in various countries, allowing knowledge of Lascaux to be shared widely, far away from the original. Lascaux IV is the latest replica, in real scale, of the integrality of the cave of Lascaux. Situated on the same hill overlooking Montignac,[18] and 400 m from the original site, it is part of the International Centre for Parietal Art (Centre International de l'Art Pari\u00e9tal) that was inaugurated in December 2016. The museum, built by Sn\u00f8hetta,[19] integrates digital technology, workshops and films into adjacent display rooms. Reproduction of Lascaux artwork in Lascaux II In its sedimentary composition, the V\u00e9z\u00e8re drainage basin covers one fourth of the d\u00e9partement of the Dordogne, the northernmost region of the Black P\u00e9rigord. Before joining the Dordogne River near Limeuil, the V\u00e9z\u00e8re flows in a south-westerly direction. At its centre point, the river's course is marked by a series of meanders flanked by high limestone cliffs that determine the landscape. Upstream from this steep-sloped relief, near Montignac and in the vicinity of Lascaux, the contours of the land soften considerably; the valley floor widens, and the banks of the river lose their steepness. The Lascaux valley is located some distance from the major concentrations of decorated caves and inhabited sites, most of which were discovered further downstream.[20] In the environs of the village of Eyzies-de-Tayac Sireuil, there are no fewer than 37 decorated caves and shelters, as well as an even greater number of habitation sites from the Upper Paleolithic, located in the open, beneath a sheltering overhang, or at the entrance to one of the area's karst cavities. This is the highest concentration in Europe. Megaloceros with line of dots The cave contains nearly 6,000 figures, which can be grouped into three main categories: animals, human figures, and abstract signs. The paintings contain no images of the surrounding landscape or the vegetation of the time.[20] Most of the major images have been painted onto the walls using red, yellow, and black colours from a complex multiplicity of mineral pigments[21]: 110 [22] including iron compounds such as iron oxide (ochre),[23]: 204 hematite, and goethite,[22][24] as well as manganese-containing pigments.[22][23]: 208 Charcoal may also have been used[23]: 199 but seemingly to a sparing extent.[21] On some of the cave walls, the colour may have been applied",
    "comments": [],
    "description": "No description available.",
    "document_uid": "3f2d7323ef",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962586",
    "title": "Why I'm in the Tailwind Cult",
    "url": "https://arrowsmithlabs.com/blog/why-im-in-the-tailwind-cult",
    "score": 1,
    "timestamp": "2024-10-27T14:46:33",
    "source": "Hacker News",
    "content": "Why I'm in the Tailwind cult Learn Phoenix LiveView is the comprehensive tutorial that teaches you everything you need to build a complex, realistic, fully-featured web app with Phoenix LiveView. Click here to learn more! Recently an article titled Why Tailwind Won hit the front page of Hacker News, kicking off a heated debate about this polarizing CSS framework. Some developers can\u2019t get enough of Tailwind; to others, it\u2019s a blight upon our profession and the worst thing since the <blink> tag. Haters be damned. I love Tailwind, I\u2019m using it on every project where I have the choice, and I\u2019d be happy to never write a line of vanilla CSS again. According to one commenter this means I\u2019m in a cult, but hey, I\u2019m happy, and this Kool-Aid tastes delicious. Here\u2019s why I\u2019m letting Tailwind fill my sails. I\u2019ve been building websites for something like twenty years now, since I first learnt basic HTML at the age of roughly twelve. From an early age I understood that you should never, ever, ever write your HTML like this: <div style=\"background-color: #333; border: 1px solid #ccc;\"> <a style=\"padding: 5px 10px; color: #fff;\" href=\"/crash\">Elbow grease</a> <a style=\"padding: 5px 10px; color: #fff;\" href=\"/bang\">Striped paint</a> <a style=\"padding: 5px 10px; color: #fff;\" href=\"/wallop\">Long weight</a> </div> These so-called \u201cinline styles\u201d are verbose, ugly, repetitive, and generally read like shit. The correct approach, so I was taught, is to replace those style attributes with a meaningful \u201csemantic\u201d identifier, typically class: <div class=\"menu\"> <a class=\"menu-link\" href=\"/crash\">Elbow grease</a> <a class=\"menu-link\" href=\"/bang\">Striped paint</a> <a class=\"menu-link\" href=\"/wallop\">Long weight</a> </div> \u2026 and then to extract my styles to a tidy CSS file: .menu { background-color: #333; border: 1px solid #ccc; } .menu-link { padding: 5px 10px; color: #fff; } And thus you\u2019re on the path to enlightenment. Master the art of Cascading Style Sheets - in particular the \u201ccascading\u201d part, which refers to the hierarchical system by which style rules are inherited or overridden - and you\u2019ll be writing clean, composable, reusable, readable, maintainable stylesheets that are a pleasure to work with and a joy to behold. That\u2019s what they told me I would discover. But twenty years later, I\u2019m still searching. I don\u2019t doubt that somewhere out there, a better programmer than me is using CSS as intended to get fantastic results. More power to them, but my own CSS workflow involves fiddling around hopelessly in the Chrome inspector, darting from rule to rule trying to remember whether style X from rule Y with selector Z takes precedence over style P from rule Q with selector R or if it\u2019s overridden by rule A inherited by style B on element C, so I tweak my code but it breaks component M because I accidentally overrode rule N, and the cycle repeats, and my stylesheets keep getting bigger and shittier, and I proceed at a snail\u2019s pace while wishing I could be working on anything else. Sass helps a little, but it\u2019s like taking aspirin for a bullet wound. I\u2019ve tried, I really have - but I just can\u2019t get \u201ccascading\u201d to be anything less than agonizing. Then I found Tailwind, and saw that the answer, all along, was inline styles: <div class=\"bg-gray-800 border border-gray-300\"> <a class=\"px-2 py-1 text-white\" href=\"/crash\">Elbow grease</a> <a class=\"px-2 py-1 text-white\" href=\"/bang\">Striped paint</a> <a class=\"px-2 py-1 text-white\" href=\"/wallop\">Long weight</a> </div> Tailwind achieves the impossible: it takes inline styles and it makes them usable. It\u2019s an all-encompassing set of utility classes that directly map onto the CSS rules you already know, with some bells and whistles to make things like :hover and !important work too. I know how terrible it looks at first - I know how flagrantly it violates everything you were taught about the correct way to style your app - but when I gave it a chance, I was surprised by how quickly it overcame my objections. Unreadable? Maybe at a glance, but you quickly get used to it. The naming conventions are consistent, intuitive, and you can learn how to read (and write) them very quickly. If you can read CSS, you can read Tailwind - it\u2019s just a set of abbreviations for the language you already know, like m for margin and p for padding. Moving from regular CSS to Tailwind isn\u2019t like moving from English to Chinese - it\u2019s like moving from handwriting to touch-typing. Some accuse Tailwind of \u201cmixing presentation and content\u201d, a cardinal sin, but I say \u201cmeh\u201d. I fully agree that presentation logic should be kept separate from content, but most of my \u201ccontent\u201d isn\u2019t in my HTML anyway; it lives in the database and is output to the template using something like <%= @this %>. What\u2019s left is a bunch of HTML tags that describe how to structure, arrange and present this information to the user - and with Tailwind, the full presentational logic now sits in a single place, as a single, coherent unit that I can grok from one file. Why did I ever think it was a good idea to keep things separate? Using \u201csemantic identifiers\u201d is another of those academic ideas that I\u2019ve never seen survive contact with reality. They say your HTML tags should have meaningful names like .navbar-dropdown or .user-email-address that describe what the element is rather than what it looks like, but don\u2019t they know that \u201cnaming things\u201d is one of the two hard problems in computer science? Most of my <div>s don\u2019t have an obvious name, so my \u201csemantic\u201d labels have to get longer and more contrived - .navbar-dropdown-inner-container-upper-icon-wrapper - until, inevitably, words like \u201cleft\u201d or \u201cdark\u201d or \u201cmargin\u201d start seeping back in, and I might as well be writing Tailwind. When there is an obvious semantic identifier, it\u2019s usually already in my code anyway - e.g. as the name of the React component (<NavbarDropdown />) or the function in my Phoenix view (<.navbar_dropdown />.) So why write it again? With Tailwind I no longer have to waste mental energy inventing \u201csemantic\u201d names for tags that don\u2019t need them - I",
    "comments": [],
    "description": "A comprehensive tutorial on Phoenix 1.7 and Elixir 1.17, for developers who already know Ruby on Rails.",
    "document_uid": "9d398a687d",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962569",
    "title": "Show HN: I Built a Tech Stack Recommender",
    "url": "https://boilerplatehub.com/tech-stack-recommender",
    "score": 3,
    "timestamp": "2024-10-27T14:44:00",
    "source": "Hacker News",
    "content": "Which tech stack did it recommend to you? \nWhat do you think?",
    "comments": [],
    "description": "Discover and compare the best Express boilerplates for your next project. Save time and start building faster with our curated collection of ready-to-ship starter kits.",
    "document_uid": "6cc3cd63eb",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962559",
    "title": "Show HN: Decentralized Twitter clone built on Ethereum blockchain",
    "url": "https://github.com/Dyslex7c/Decentralized-Twitter",
    "score": 1,
    "timestamp": "2024-10-27T14:41:58",
    "source": "Hacker News",
    "content": "Dyslex7c/Decentralized-Twitter You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "Social media platform where users can post and authenticate via smart contracts, built on Ethereum using Nest.js, PostgreSQL, Solidity and Prisma - Dyslex7c/Decentralized-Twitter",
    "document_uid": "7786358890",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962538",
    "title": "Rising from Failure",
    "url": "https://www.noamlerner.com/posts/rising_from_failure/",
    "score": 1,
    "timestamp": "2024-10-27T14:37:20",
    "source": "Hacker News",
    "content": "Failures and inevitable when trying to achieve big things. This is true in many aspects in my life. Family, work, maintaining a training routine, and more. The cliche says that \u2019the road to success isn\u2019t a straight line\u2019. When working towards the goal - there will be some detours. The question is - how to navigate through those. An image of a person rising from failure, by ChatGPT. Usually, this isn\u2019t as dramatic Acknowledge the failure #The first step from rising from failure is to acknowledge that it had happened. You should acknowledge that something sub-optimal has happened. A failure, a mistake, a detour, a bug, a bad decision, something. It might be also beneficial to acknowledge the failure to others as well. It\u2019ll help with accountability. Owning a failure can build confidence in others that you\u2019ll fix it, and that you\u2019ll try to prevent it from happening again. Learn from it #At Meta, we have a framework called DERP for our post-mortem process. DERP stands for: Detection: How was the process of detecting the problem? Was there any automation put in place to identify the issue? A common follow-up for a post-mortem is to add automated detection for the issue - so that similar, future incidents could be addressed in a fast manner. Escalation: How was the process of getting the right people or resources to handle the issue. Were the oncalls responsive? Remediation: How was the process of \u2018putting a band-aid\u2019 to fix the process. To clarify, this step is about making sure the customers will not experience the outage. Under the hood, things can be ugly, as the main motivation is to \u2018keep the plane in the air\u2019. Prevention: How to make sure that this incident, and ones similar to it won\u2019t happen again. Follow-ups here can be both short and long term. It can be toggling a configuration flag to make the system more resilient to load, or it can re-architecting a component. Lower the cost of failures #As failures are bound to happen, it makes sense to build systems in a way so that they\u2019ll be fast and easy to recover from failures. An example from production systems is how fast can you push a hotfix. When you trust your push process, you can push new code, configuration or both when there\u2019s an issue. Another example is having knobs which are consumed from systems at runtime. When you have those in place, you can change a knob and the effect be applied faster than a full code or configuration push. Another type of failure (or mis-alignment) is when there\u2019s a drift between where you wanted to go and where you actually went. A way to mitigate this issue is to set smaller milestones and check-ups. It doesn\u2019t need to be a \u2018heavy process\u2019. It can be a calendar reminder where you check yourself against some higher level goal, that you still have your eyes on the prize. When doing so, you\u2019ll reduce the cost of drift - as it\u2019ll be the size of a milestone - compared to the full project. Rebuild your confidence with small, easy steps #After experiencing a failure, it may be hard to get back up to the same \u2018good state\u2019, at least its state of mind. Maybe it\u2019s me and my self-esteem, maybe it\u2019s me berating myself too much - but after taking a few breaths - I\u2019m at a better position to \u2018rebuild\u2019 myself. For example, a failure to maintain my Freeletics training routine. Because of an injury, for example. I found that it helps to get back into the training routine by starting an easier training journey. This puts the emphasis not on the results of the training sessions, but rather on the training routine. Completing the training sessions as instructed rebuilds the confidence that I can get back to the old, steady routine. Then, when I\u2019ve once again got used to training regularly, I move on to a harder training journey. At that point, I have a regular training routine that acts as foundations for achieving results in my training sessions. From the tech-side, it can be joining a new team, or starting something completely new - where I don\u2019t have enough context on. In that case, I try and find small, easy projects which can get me \u2018warmed up\u2019 to the new area, experience some competence when completing such a small project. Those can open the door to questions about why things are built the way they are, which can then produce harder projects. You\u2019ve already done well. You could do it again. #Taking a step back and looking at facts or ground assumptions helps me to regain my focus. I know for a fact that I\u2019ve already worked hard to be in \u2019that good state\u2019. I\u2019ve been there. I tell myself that if I did that before, I can probably get to that \u2018good state\u2019 again. If I have failed and regained that \u2018good state\u2019 in the past - that\u2019s even a better signal that shows that I\u2019ll be able to achieve that yet again.",
    "comments": [],
    "description": "\n      A few tips for recovering and rising from failure\n    ",
    "document_uid": "f56a209e2c",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962536",
    "title": "Ending Projects (2021)",
    "url": "https://www.mooreds.com/wordpress/archives/3477",
    "score": 1,
    "timestamp": "2024-10-27T14:37:02",
    "source": "Hacker News",
    "content": "It is okay to let things end. You don\u2019t need my permission, but I give it to you anyway. I have let projects go in the recent and far past. It\u2019s hard. Tips for making it just a bit easier: Sit on it for a while. Don\u2019t make hasty decisions. Give it a good shot. You, of course, get to define what that is, but you don\u2019t want to bounce from project to project. 6 months of effort is my rule of thumb. Listen to your gut. It knows what you really want. Make a list of the good and the bad about this project. If you can, offer to hand it off. How this works depends on the project type (an OSS project is different Allow for a transition period where the project isn\u2019t supported but isn\u2019t quite gone. Realize that changes happen and who you were when you started the project is different than who you are now. That\u2019s okay. Giving up something allows you to pursue new goals. This is a good thing. Say goodbye, either privately or publicly, whatever feels right. It\u2019s never easy to say goodbye, especially if you have poured your efforts into something for years. I recently let a project I started in 2008 fade away. It wasn\u2019t serving me any more and I had no enthusiasm for it. My life had moved on, and it took me a few years to acknowledge the truth\u2013I didn\u2019t want to maintain the project and couldn\u2019t find anyone who did. While it was very useful for some folks for some time, it was no longer useful or fun for me. It\u2019s never easy to say goodbye, but it can be the right thing to do.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "5f09fc7ff2",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962534",
    "title": "OfflinePages \u2013 View Your Notion Pages Offline with Ease on iOS",
    "url": "https://www.offlinepages.cc",
    "score": 2,
    "timestamp": "2024-10-27T14:36:55",
    "source": "Hacker News",
    "content": "Instant access to your Notion databasesYour important Notion databases are now just a tap away. Whether it\u2019s tracking project progress, managing inventories, or reviewing data on the go, our app makes sure you always have offline access.",
    "comments": [],
    "description": "OfflinePages: your Notion pages, anywhere, anytime!",
    "document_uid": "169719b280",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962531",
    "title": "Shattering Business Secrets with Social Media (2011) [video]",
    "url": "https://www.youtube.com/watch?v=HQG0iXmabsc",
    "score": 1,
    "timestamp": "2024-10-27T14:36:47",
    "source": "Hacker News",
    "content": "Shattering Business Secrets with Social Media (2011) [video]",
    "comments": [],
    "description": "Developer Evangelist for Twilio (www.twilio.com), Keith Casey (schipulcon.com/speakers/keith-casey), discusses accessing private information using various so...",
    "document_uid": "b1f2209594",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962518",
    "title": "Amnestic mild cognitive impairment assoc w higher CV impedance in older adults",
    "url": "https://journals.physiology.org/doi/full/10.1152/japplphysiol.00337.2024",
    "score": 1,
    "timestamp": "2024-10-27T14:35:15",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "9f3379da08",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962509",
    "title": "The Prophet of Cyberspace",
    "url": "https://www.filfre.net/2016/11/the-prophet-of-cyberspace/",
    "score": 3,
    "timestamp": "2024-10-27T14:34:14",
    "source": "Hacker News",
    "content": "William Gibson was born on March 17, 1948, on the coast of South Carolina. An only child, he was just six years old when his father, a middle manager for a construction company, choked on his food and died while away on one of his many business trips. Mother and son moved back to the former\u2019s childhood home, a small town in Virginia. Life there was trying for the young boy. His mother, whom he describes today as \u201cchronically anxious and depressive,\u201d never quite seemed to get over the death of her husband, and never quite knew how to relate to her son. Gibson grew up \u201cintroverted\u201d and \u201chyper-bookish,\u201d \u201cthe original can\u2019t-hit-the-baseball kid,\u201d feeling perpetually isolated from the world around him. He found refuge, like so many similar personalities, in the shinier, simpler worlds of science fiction. He dreamed of growing up to inhabit those worlds full-time by becoming a science-fiction writer in his own right. At age 15, desperate for a new start, Gibson convinced his mother to ship him off to a private school for boys in Arizona. It was by his account as bizarre a place as any of the environments that would later show up in his fiction. It was like a dumping ground for chronically damaged adolescent boys. There were just some weird stories there, from all over the country. They ranged from a 17-year-old, I think from Louisiana, who was like a total alcoholic, man, a terminal, end-of-the-stage guy who weighed about 300 pounds and could drink two quarts of vodka straight up and pretend he hadn\u2019t drunk any to this incredibly great-looking, I mean, beautiful kid from San Francisco, who was crazy because from age 10 his parents had sent him to plastic surgeons because they didn\u2019t like the way he looked. Still, the clean desert air and the forced socialization of life at the school seemed to do him good. He began to come out of his shell. Meanwhile the 1960s were starting to roll, and young William, again like so many of his peers, replaced science fiction with Beatles, Beats, and, most of all, William S. Burroughs, the writer who remains his personal literary hero to this day. William Gibson on the road, 1967 As his senior year at the boys\u2019 school was just beginning, Gibson\u2019s mother died as abruptly as had his father. Left all alone in the world, he went a little crazy. He was implicated in a drug ring at his school \u2014 he still insists today that he was innocent \u2014 and kicked out just weeks away from graduation. With no one left to go home to, he hit the road like Burroughs and his other Beat heroes, hoping to discover enlightenment through hedonism; when required like all 18-year-olds to register for the draft, he listed as his primary ambition in life the sampling of every drug ever invented. He apparently made a pretty good stab at realizing that ambition, whilst tramping around North America and, a little later, Europe for years on end, working odd jobs in communes and head shops and taking each day as it came. By necessity, he learned the unwritten rules and hierarchies of power that govern life on the street, a hard-won wisdom that would later set him apart as a writer. In 1972, he wound up married to a girl he\u2019d met on his travels and living in Vancouver, British Columbia, where he still makes his home to this day. As determined as ever to avoid a conventional workaday life, he realized that, thanks to Canada\u2019s generous student-aid program, he could actually earn more money by attending university than he could working some menial job. He therefore enrolled at the University of British Columbia as an English major. Much to his own surprise, the classes he took there and the people he met in them reawakened his childhood love of science fiction and the written word in general, and with them his desire to write. Gibson\u2019s first short story was published in 1977 in a short-lived, obscure little journal occupying some uncertain ground between fanzine and professional magazine; he earned all of $27 from the venture. Juvenilia though it may be, \u201cFragments of a Hologram Rose,\u201d a moody, plot-less bit of atmospherics about a jilted lover of the near future who relies on virtual-reality \u201cASP cassettes\u201d to sleep, already bears his unique stylistic stamp. But after writing it he published nothing else for a long while, occupying himself instead with raising his first child and living the life of a househusband while his wife, now a teacher with a Master\u2019s Degree in linguistics, supported the family. It seemed a writer needed to know so much, and he hardly knew where to start learning it all. It was punk rock and its child post-punk that finally got him going in earnest. Bands like Wire and Joy Division, who proved you didn\u2019t need to know how to play like Emerson, Lake, and Palmer to make daring, inspiring music, convinced him to apply the same lesson to his writing \u2014 to just get on with it. When he did, things happened with stunning quickness. His second story, a delightful romp called \u201cThe Gernsback Continuum,\u201d was purchased by Terry Carr, a legendary science-fiction editor and taste-maker, for the 1981 edition of his long-running Universe series of paperback short-story anthologies. With that feather in his cap, Gibson began regularly selling stories to Omni, one of the most respected of the contemporary science-fiction magazines. The first story of his that Omni published, \u201cJohnny Mnemonic,\u201d became the manifesto of a whole new science-fiction sub-genre that had Gibson as its leading light. The small network of writers, critics, and fellow travelers sometimes called themselves \u201cThe Movement,\u201d sometimes \u201cThe Mirrorshades Group.\u201d But in the end, the world would come to know them as the cyberpunks. If forced to name one thing that made cyberpunk different from what had come before, I wouldn\u2019t point to any of the exotic computer",
    "comments": [],
    "description": "No description available.",
    "document_uid": "aef0e04585",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962480",
    "title": "Mathematics for Computer Science [pdf]",
    "url": "https://courses.csail.mit.edu/6.042/spring18/mcs.pdf",
    "score": 3,
    "timestamp": "2024-10-27T14:27:10",
    "source": "Hacker News",
    "content": "%PDF-1.5 %\u00e2\u00e3\u00cf\u00d3 1 0 obj << >> endobj 2 0 obj << >> endobj 3 0 obj << /pgfprgb [/Pattern /DeviceRGB] >> endobj 4 0 obj << /D (\u00b3\u2022|\u00ab\u00e9\u00ae) /S /GoTo >> endobj 5 0 obj << /D [6 0 R /XYZ 162 664.335 null] >> endobj 7 0 obj << /Parent 8 0 R /Title 9 0 R /Last 10 0 R /Next 11 0 R /A 4 0 R /First 12 0 R /Count 347 >> endobj 9 0 obj (2 \u00aek\\b\u00ee!\u00b0) endobj 13 0 obj << /D (\u00ba\u00fai\u00fdU\\tJ\u2026\u00de[) /S /GoTo >> endobj 14 0 obj << /D [15 0 R /XYZ 162 479.277 null] >> endobj 12 0 obj << /Parent 7 0 R /Title 16 0 R /Next 17 0 R /A 13 0 R >> endobj 16 0 obj (Hd\u00eb\u2014\u00f4\u201d\u00d2Z!\u2019\u00cf\u00bd) endobj 18 0 obj << /D (\u00f8\u201c=}\\r\u00f5\u00ab\"v) /S /GoTo >> endobj 19 0 obj << /D [20 0 R /XYZ 162 340.18 null] >> endobj 17 0 obj << /Parent 7 0 R /Title 21 0 R /Next 22 0 R /Prev 12 0 R /A 18 0 R >> endobj 21 0 obj (G\u2122\u00fe\u00d0\u00e2H\u00c0%\u00f5\u21223\u00ca) endobj 23 0 obj << /D ([\u00f1l\u2013\u00d6\u00d7\u02dcU) /S /GoTo >> endobj 24 0 obj << /D [25 0 R /XYZ 162 664.335 null] >> endobj 22 0 obj << /Parent 7 0 R /Title 26 0 R /Last 27 0 R /Next 28 0 R /Prev 17 0 R /A 23 0 R /First 29 0 R /Count 35 >> endobj 26 0 obj (H\u00ec\u2022\u2018\u00ef\u00ff\ufffd;KL\u00df\ufffd\\tv\ufffd\ufffdT) endobj 30 0 obj << /D (> endobj 31 0 obj << /D [25 0 R /XYZ 162 617.768 null] >> endobj 29 0 obj << /Parent 22 0 R /Title 32 0 R /Next 33 0 R /A 30 0 R >> endobj 32 0 obj (\ufffdv\u00de\u00ee\ufffd\u0153c\u00eev\\r\u2030\u00fa$\u00e1\u00d12) endobj 34 0 obj << /D (\\t\ufffd3k\u00ea\u00c1L\u00a8w) /S /GoTo >> endobj 35 0 obj << /D [36 0 R /XYZ 162 520.703 null] >> endobj 33 0 obj << /Parent 22 0 R /Title 37 0 R /Next 38 0 R /Prev 29 0 R /A 34 0 R >> endobj 37 0 obj ('\u00c2|\u00bch\u00f7\u00d1\u00a7\u00caM\u2122) endobj 39 0 obj << /D (\u00ab\u00bf\ufffd \"\u017e\u00fe\u00cb\u0192\u00b8) /S /GoTo >> endobj 40 0 obj << /D [36 0 R /XYZ 162 221.18 null] >> endobj 38 0 obj << /Parent 22 0 R /Title 41 0 R /Next 42 0 R /Prev 33 0 R /A 39 0 R >> endobj 41 0 obj (&\u2014Q@\u00ee\u2014\u00efFS8`d\u00ac\ufffdmj\u00cd\u2039\u00c2M*) endobj 43 0 obj << /D (rm\u00f5\u00ecQn{\u00ea.w\u00f8) /S /GoTo >> endobj 44 0 obj << /D [45 0 R /XYZ 162 343.013 null] >> endobj 42 0 obj << /Parent 22 0 R /Title 46 0 R /Next 47 0 R /Prev 38 0 R /A 43 0 R >> endobj 46 0 obj (\u0152\u00bc\u00f1\u00bb\u00d3?\u0178.\u00a3\u00cb\\\\) endobj 48 0 obj << /D (\u00fctP\u2021r\u0153\u02c6\u00b4\u201dU\\n) /S /GoTo >> endobj 49 0 obj << /D [50 0 R /XYZ 162 419.881 null] >> endobj 47 0 obj << /Parent 22 0 R /Title 51 0 R /Next 52 0 R /Prev 42 0 R /A 48 0 R >> endobj 51 0 obj (\u00ect\u00a4\u00bb\u00ef! \u00b0b\u20309!\u0192\u017d\u0161\u2018\\)\u00c9\u00f4S\u2019\u00fb ) endobj 53 0 obj << /D (!?\u00aa\u00df\u00ae\u00d5GLXg) /S /GoTo >> endobj 54 0 obj << /D [55 0 R /XYZ 162 259.119 null] >> endobj 52 0 obj << /Parent 22 0 R /Title 56 0 R /Next 57 0 R /Prev 47 0 R /A 53 0 R >> endobj 56 0 obj (E\u00cev\u00af@haw\u201aF=\u2030\u00e2\u00ff'-oV\u00aa3\u00e6UoH\u00d1Ss$\u00e2h) endobj 58 0 obj << /D (}=\u0153G\u00e8\u00fe<3\u00f7p\u00ba) /S /GoTo >> endobj 59 0 obj << /D [60 0 R /XYZ 162 481.735 null] >> endobj 57 0 obj << /Parent 22 0 R /Title 61 0 R /Next 62 0 R /Prev 52 0 R /A 58 0 R >> endobj 61 0 obj (>\u00be\u00b7\u00f6\u00db\u00fa%\ufffd\u00d7\u00b2</$\u02c6) endobj 63 0 obj << /D (\u00c4\u00db\u00a8\u201aX\u2014\u00cd*) /S /GoTo >> endobj 64 0 obj << /D [65 0 R /XYZ 162 378.806 null] >> endobj 62 0 obj << /Parent 22 0 R /Title 66 0 R /Next 67 0 R /Prev 57 0 R /A 63 0 R >> endobj 66 0 obj (\u00f6s\u00a17\u00cb\u00e4; \u00cdD@\u00b6\u00d7<\u00c3\u201dD\ufffd\u00a9\u00c1/{) endobj 68 0 obj << /D (\u00d6\u00e0\\tQ\u20142Dg_j\u00c0) /S /GoTo >> endobj 69 0 obj << /D [70 0 R /XYZ 162 447.134 null] >> endobj 67 0 obj << /Parent 22 0 R /Title 71 0 R /Next 72 0 R /Prev 62 0 R /A 68 0 R >> endobj 71 0 obj (\u00d9\u00e3\u02c6\u20ac/\u00de\u00a2\u00d8\u201e\u00d6\u00cc*\u00d6~H\u2022\u00a4\u00d9\u00ab\u00b1N\u00b1~!) endobj 73 0 obj << /D (\u00a3\u00f4\\rI'\u00c6\u00bc\u00cf\u00ceW\u00cd) /S /GoTo >> endobj 74 0 obj << /D [75 0 R /XYZ 162 276.817 null] >> endobj 72 0 obj << /Parent 22 0 R /Title 76 0 R /Next 77 0 R /Prev 67 0 R /A 73 0 R >> endobj 76 0 obj (\u2026X\u00a6D\u02c6\u00c9\u00b9\u0192/\u0192JG) endobj 78 0 obj << /D (P\u00e3mTA\u00a1\u00a9\u00eb\u00f4g\u201a\u00b3) /S /GoTo >> endobj 79 0 obj << /D [80 0 R /XYZ 201.796 590.014 null] >> endobj 77 0 obj << /Parent 22 0 R /Title 81 0 R /Next 82 0 R /Prev 72 0 R /A 78 0 R >> endobj 81 0 obj (n\u00c1\u00d0\u02dc\u00ac\u00c9J\u00fc\"\u00d1) endobj 83 0 obj << /D (b\u201d\u00f6\\nPWO\u00edt) /S /GoTo >> endobj 84 0 obj << /D [85 0 R /XYZ 201.796 445.675 null] >> endobj 82 0 obj << /Parent 22 0 R /Title 86 0 R /Next 87 0 R /Prev 77 0 R /A 83 0 R >> endobj 86 0 obj (!\u00bd~^G\u00fb\u00e0\u00cci\u00b7Z) endobj 88 0 obj << /D (4\u00b7x\u00ab\u00e0\u02dc\u00ce\u0153\u00a4\u00cf\u00d3W) /S /GoTo >> endobj 89 0 obj << /D [85 0 R /XYZ 201.796 226.449 null] >> endobj 87 0 obj << /Parent 22 0 R /Title 90 0 R /Next 91 0 R /Prev 82 0 R /A 88 0 R >> endobj 90 0 obj (\u201c\u00d1\u00c8\u00d5yWz`v\u2039) endobj 92 0 obj << /D (^\u00ddQ\u00bd\u00f8\u00d7\u00efd\\t\u00a8) /S /GoTo >> endobj 93 0 obj << /D [94 0 R /XYZ 201.796 298.617 null] >> endobj 91 0 obj << /Parent 22 0 R",
    "comments": [],
    "description": "No description available.",
    "document_uid": "8ad8f6ff83",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962473",
    "title": "Scientific Truth: An Endangered Species",
    "url": "https://www.embopress.org/doi/full/10.1038/s44319-024-00293-5",
    "score": 1,
    "timestamp": "2024-10-27T14:24:58",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "ad6d63cae6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962440",
    "title": "The Active Badge Location System (1992)",
    "url": "https://dl.acm.org/doi/pdf/10.1145/128756.128759",
    "score": 1,
    "timestamp": "2024-10-27T14:20:50",
    "source": "Hacker News",
    "content": "%PDF-1.4 %\u00e2\u00e3\u00cf\u00d3 1 0 obj <>stream x\u0153\u00ed\u00c1\ufffd \u00c3 \u007f\u00ea]\u00e1 U \ufffd\u00ce\ufffd+\u015f endstream endobj 2 0 obj <>]/Intent/Perceptual/Subtype/Image/Height 1004/Filter/FlateDecode/Type/XObject/Width 1003/SMask 1 0 R/Length 50324/BitsPerComponent 8>>stream x\u0153\u00ec\u0130\u00e7\u007f\u201cw\ufffd\u00ef\u00ff\u007f\u00e2w\u00e7\u00dc=\u00bbw\u00ce\ufffdsfwg3\u203a\ufffd\u00c9\u00ec C\u00d2{\u2019\ufffdRI\u2021\u201e\u201e\u00f4\ufffd\ufffd^H! I(w\u00c0`\u00e3&[\u00eer\u2018l\u00c9E\u00b6\u0160e\u00c9r\u00af\u00e6\\\u2030\u00e7\u00c70\u0152\u00a5\u00eb\u00ba\u00f4\u00b9\u00be\u00d7\u00f5z>\u015e7w \u007fmI\u00af|\u00d7\u2013\u00ff\u00faW 3::\u00eb\u00eb\u00eb\u00ec\u00e8h\u00f6x\u00aa**\u00caKKc\u01521\u00c6s\u00c8\u00b4 \u00d62X\u2039a-\u2030\u00b50\u2013n\u00f3\u00d3\u0161\ufffd\u0161j\u00f3\u00f9\u00f6\u00e5\u00e5}\u015f\u00d1G\u00be\u00fb.c\u01521\u00c6cL\u00dbg~\u00a8Er\u203a\u00d7\u00ab\u00b3t\u00b3\u00ff\ufffd\u00af\u00b55k\u00f7n\u00f1\u201ca\u01521\u00c6c\u00cc\u00e2\u00cb\u00fa\u00f9g-\ufffd\u00d3\u00bd\u00bb\u00abk\u00d7\ufffd?\u0160\u0178c\u01521\u00c6c MKh-\u00a43\u0153\u00ee}\u00d1h\u00ce\u015e\u00bd\u00e2;c\u01521\u00c6c\u0160N\u00cbi-\u00aa3S\u00efM\ufffd\ufffd\u00e2/c\u01521\u00c6c6\u02dc\u2013\u00d6f\u00d7{yI\u2030\u00f8\u2021\u00c9c\u01521\u00c6\u02dcm\u00a6\u00b6I\u00e9~df&?;[\u00fcd\u01521\u00c6c\u00ccf\u00d32[\u2039m\u00c3~\u00cf\u00ce\ufffd\u2039\u00ff7|\u00bci\u00d3\u00d6/\u00bf\u00dc\u00b1}{^v\u00f6\ufffd\u201a\u00c6c\u01521\u00c62\u00ad\u00c6w\u00fc\u011f\u00c3\u2013\u00cd\u203a\u00b5$^|?k\u00b1ml\u00bd:p`1\u007f\u00ef\u00b6\u00af\u00bf.-.\u00ee\u00ed\u00e91\u00f6o T\u00a4\u2026\u00b1\u2013\u00c7Z$/\u00a6\u00a5\u00b5\u00e46\u00ea\u00ef\u00ad\u00a9\u00aa:\u00e3_\u00f7\u00d3\u00f7\u00df\u00fc~\u00a3\u015fF \u00c0N\u00b4T\u00d6\u201a\u00f9\u0152Q\u00ad\u2026\u00b7\u015f\u00bf\u00ab\u0130\u00e7[\u00f8o\u00d9\u00fc\u00e9\u00a7\u00cd\ufffd\u015f\u00bf \u00b07-\u203a\u00b5x^\u00b8\u00ae\u00b5\u00fc\u00d6\u00f3W=z\u00f4\u00eb/\u00beX\u00f8\u00e2=90`\u00d4G \u00d8\u203a\u00cf_\u00c5k\u00f9\u00adEx\u00da\u007f~ue\u00e5\u007fxAn\u00ae\ufffd? p -\u00a1\u00b5\ufffd^ \u00b3\u00b5O\u00efO\ufffd\u00ff\u00e4\u0192N\u00f7\u00c7\u00ee\u0130\u00bd\u00db\u00d8 p-\u00a7OW\u00daZ\u201ek)\ufffd\u00c6\u0178YRT\u00b4\u00c0\u00bb\u00cdLLL\u015fQ \u00a1\u00e5\u00f4\u00efN\u00a3\u00a5x\u00aa\u007f\u00e0\u00f8\u00f8\u00f8\u00b7\u00fa\u00d1H\u00c4\u201e p-\u00aaH\u00ee\u00f1/\u00e1[\u0161\u0161N\u00f7G\u0161\u00f4! \ufffd\u00a2\u00a5\u00f5\u00e9\u00aa[\u00f2\u201d\u015f\u00a8\u00dc\u00ac\u00ac\u00d3\u0131Q\u00a3##&\u0131\u00fb G\u00d1\u00d2\u00fat\u00d5\ufffd\u2014\ufffd\u00bd\u00f8?gvv\u00f6t\u00bf\u00f8u\u00e7?\u02dc\u00f7\u00ef \u0153F\u00ecS\u2020\u00b7\u00e4Z\u2013/\u00f2\u00e9\u011f\u00fbO\u00f7\u2018p\u00d8\u00d4\u007f? \u00e0(Z`\u0178\u00ae\u00bd\u00b5,_\u00e4Rv\u00f8\u011f)\u00ff\u201e\u00af\u00bf\u00f8\u00c2\u00d4\u007f<",
    "comments": [],
    "description": "No description available.",
    "document_uid": "9a1527389d",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962411",
    "title": "Scientists developed a novel method to detect lies",
    "url": "https://www.universal-sci.com/headlines/2020/12/17/spotting-liars-is-hard-but-our-new-method-is-effective-and-ethical",
    "score": 12,
    "timestamp": "2024-10-27T14:17:02",
    "source": "Hacker News",
    "content": "Most people lie occasionally. The lies are often trivial and essentially inconsequential \u2013 such as pretending to like a tasteless gift. But in other contexts, deception is more serious and can have harmful effects on criminal justice. From a societal perspective, such lying is better detected than ignored and tolerated.Unfortunately, it is difficult to detect lies accurately. Lie detectors, such as polygraphs, which work by measuring the level of anxiety in a subject while they answer questions, are considered \u201ctheoretically weak\u201d and of dubious reliability. This is because, as any traveller who has been questioned by customs officials knows, it\u2019s possible to be anxious without being guilty.We have developed a new approach to spot liars based on interviewing technique and psychological manipulation, with results just published in the Journal of Applied Research in Memory and Cognition.Our technique is part of a new generation of cognitive-based lie-detection methods that are being increasingly researched and developed. These approaches postulate that the mental and strategic processes adopted by truth-tellers during interviews differ significantly from those of liars. By using specific techniques, these differences can be amplified and detected.One such approach is the Asymmetric Information Management (AIM) technique. At its core, it is designed to provide suspects with a clear means to demonstrate their innocence or guilt to investigators by providing detailed information. Small details are the lifeblood of forensic investigations and can provide investigators with facts to check and witnesses to question. Importantly, longer, more detailed statements typically contain more clues to a deception than short statements.Essentially, the AIM method involves informing suspects of these facts. Specifically, interviewers make it clear to interviewees that if they provide longer, more detailed statements about the event of interest, then the investigator will be better able to detect if they are telling the truth or lying. For truth-tellers, this is good news. For liars, this is less good news.Indeed, research shows that when suspects are provided with these instructions, they behave differently depending on whether they are telling the truth or not. Truth-tellers typically seek to demonstrate their innocence and commonly provide more detailed information in response to such instructions.In contrast, liars wish to conceal their guilt. This means they are more likely to strategically withhold information in response to the AIM instructions. Their (totally correct) assumption here is that providing more information will make it easier for the investigator to detect their lie, so instead, they provide less information.",
    "comments": [
      {
        "author": "gmuslera",
        "text": "\u201cHis lips are moving\u201d is a fireproof detection method. At least for politicians and marketers.",
        "time": "2024-10-27T15:17:29"
      },
      {
        "author": "jobigoud",
        "text": "Is it robust against people that have come to believe their own lies?",
        "time": "2024-10-27T15:06:07"
      },
      {
        "author": "garciasn",
        "text": "&gt; Indeed, research shows that when suspects are provided with these instructions, they behave differently depending on whether they are telling the truth or not. Truth-tellers typically seek to demonstrate their innocence and commonly provide more detailed information in response to such instructions.<p>DiSC (or any related business personality measurement tools) will identify those who obsess over details vs those who do not (Cs vs Ds, for example).<p>I\u2019m a D; I don\u2019t provide lengthy explanations for anything. I can get my point across with fewer details and I consider this a huge win; whereas, Cs will flood conversations with details that muddy the waters.<p>So; in this case, because I\u2019m not likely to share lots of details and information, I\u2019m going to be flagged as lying, when in reality, I just don\u2019t operate that way.<p>Yet another pseudoscientific attempt at spotting liars which will be used in lieu of actual evidence and proof.",
        "time": "2024-10-27T15:01:17"
      }
    ],
    "description": "Can you spot a liar? Although almost all of us tell a lie once in a while, \nwe are remarkably bad at consistently and accurately spotting them. A team \nof researchers developed a unique method to identify liars.",
    "document_uid": "725566c80b",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962404",
    "title": "Ask HN: Looking for Collaborators on Gaia, an MMO Ecosystem Simulator",
    "url": "https://news.ycombinator.com/item?id=41962404",
    "score": 1,
    "timestamp": "2024-10-27T14:15:12",
    "source": "Hacker News",
    "content": "I\u2019m developing an idea for a massively multiplayer online game I call <i>Gaia</i> for now, where players embody individual species within a shared, dynamic ecosystem. Each player can choose a kingdom of life\u2014plant, fungus, bacterium, or animal\u2014and starts as a random mutation from an existing, competing species; players evolve their species, adapting and competing for resources in a virtual environment that mimics ecological interdependence.<p>Key concepts include:<p><pre><code>  - Unique play styles for each Kingdom: Different gameplay mechanics for plants, fungi, bacteria, and animals, reflecting their roles in an ecosystem.\n  - Nutrient economy: A \u201cstock exchange\u201d for nutrients where players decide how to share or hoard resources, impacting ecosystem balance and competition.\n  - Ecosystem simulation: Species interactions drive the game, with real-time adaptations and a leaderboard based on area controlled by each species.\n  - Ongoing evolution: If species go extinct, players respawn as a random mutation from another successful species, with unique up- and downsides.\n  - Environmental events: Natural disasters and catastrophes that occur rarely, but shake up the environment in unexpected ways, require quick changes in strategy. \n  - Educational &amp; strategic depth: The game aims to reveal the complexities of ecological balance and fragility, blending fun with insight into nature.\n</code></pre>\nCreating engaging gameplay loops will be the biggest challenge, but I&#x27;m sure the possible solution space is huge.<p>I\u2019m looking for others interested in building or brainstorming around this idea\u2014whether in game development, ecological modelling, or MMO infrastructure. This is the game I&#x27;ve wanted to play forever; if you liked the idea of Spore, but found it way too shallow, and are likewise intrigued by the idea of simulating life\u2019s interconnected web, let\u2019s talk!",
    "comments": [
      {
        "author": "hyperific",
        "text": "What are you looking for in a collaborator?",
        "time": "2024-10-27T14:36:49"
      }
    ],
    "description": "No description available.",
    "document_uid": "99dfafee8b",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962402",
    "title": "Lost for centuries, Silk Road cities are revealed by drone technology",
    "url": "https://www.nbcnews.com/news/world/lost-centuries-silk-road-cities-are-revealed-drone-technology-rcna176990",
    "score": 1,
    "timestamp": "2024-10-27T14:15:08",
    "source": "Hacker News",
    "content": "Traversed centuries ago by camel-back traders, two long-lost medieval cities that once thrived along the ancient Silk Road have been uncovered by drones sent searching for their secrets. For centuries, these abandoned cities lay hidden beneath the mountains of Central Asia. But new research, published Wednesday in the journal Nature, reveals two fortified settlements that were once perched along a key crossroad of silk trade routes.This groundbreaking research in southeastern Uzbekistan could shift our understanding of the Silk Road, a vast network of trade routes that spanned from China to the Mediterranean.On conventional maps, trade routes spanning the Eurasian continent were assumed to avoid the mountains of Central Asia. But the new research shows the Silk Road network was larger than previously predicted.Using modern drone mapping technology known as LiDAR \u2014 light detection and ranging equipment \u2014 the team of archaeologists found that the two cities, Tashbulak and Tugunbulak, were once bustling urban centers despite their isolation and elevation. A composite view of Tugunbulak using lidar technology, which uses light in the form of a pulsed laser to measure ranges.SAIElab / J.Berner / M.FrachettiThe work was led by Michael Frachetti, professor of anthropology at Washington University in St. Louis, alongside Farhod Maksudov, the director of Uzbekistan\u2019s National Center of Archaeology.Frachetti's team began conducting archaeological work at Tashbulak in 2011, with research at Tugunbulak commencing in 2018. However, the project was put on hold because of travel restrictions during the pandemic.Over time, technological advancements have revolutionized the discovery and mapping of urban centers in landscapes that are largely inaccessible because of obstacles such as dense vegetation.Thanks to this new drone-based remote sensing system the team were able to captured images revealing two large urban settlements dotted with watchtowers, fortresses, complex buildings and plazas.Frachetti and his team did not expect the technology to uncover the level of detail it revealed, however.\"We were quite surprised when the imagery was compiled, since the high-resolution reveals so much about the structure of the cities and with such clarity,\" Frachetti told NBC News in an email Thursday.Although many large urban centers have been discovered in Central Asia, the vast majority of archaeologically documented cities are in lowland riparian settings.Tugunbulak and Tashbulak are 3 miles apart and around 7,000 feet above sea level. Large urban centers above 6,000-feet elevation are extremely rare, Frachetti said in his research paper.Tim Williams, professor of silk roads archaeology at University College London in England, emphasized the significance of the findings, which reveal a more complex upland urban landscape than previously imagined.\u201cThis is a ground-breaking piece of research, which demonstrates how linking modern non-invasive survey methods, especially drone based survey, can considerably enhance our understanding of ancient landscapes and human adaption,\u201d he said in an email.Frachetti envisions the cities as being home to a wide range of communities such as craftsmen, traders, herders, political elites and soldiers.\"These were large settlements with markets which likely had bustling activity common to most urban settings of the time,\" he said.A drone captured images of the site of Tugunbulak in 2018, in what is Uzbekistan today.Michael FrachettiAccording to the radiocarbon dating, both cities rapidly declined around the first half of the 11th century, \"a time of political division among the prevailing political powers,\" Frachetti said. The research indicates the two cities produced iron or steel to sell, as well as providing fuel for Silk Road travelers, with the region being surrounded by dense juniper forests.These cities are predicted to have engaged in metal production, and may have also over-exploited the nearby forest resources beyond a point of economic sustainability, leading to its abandonment.\"We think the causes of the settlements' ultimate decline were multifaceted, and we hope our ongoing archaeological excavations will provide greater clarity in the years to come,\" Frachetti said.",
    "comments": [],
    "description": "Archeologists have uncovered stunning details of two long-lost medieval cities that once thrived along the ancient Silk Road in Central Asia.",
    "document_uid": "b16a81f24c",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962389",
    "title": "This Week in KDE Apps: Chat Changes, Editor Enhancements and Audio Advancements",
    "url": "https://blogs.kde.org/2024/10/27/this-week-in-kde-apps/",
    "score": 5,
    "timestamp": "2024-10-27T14:12:39",
    "source": "Hacker News",
    "content": "Welcome to a new issue of \"This Week in KDE Apps\"! Every week we cover as much as possible of what's happening in the world of KDE apps.This week's changes and improvements cover a wide range of applications, from audio apps (including the classic Amarok, which is making a comeback) to Kate getting improvements to its integrated Git features.In between, you have everything from new functionalities for note-taking utilities and media players, to upgrades in financial software and mobile apps.Let's dig in!Amarok A powerful music player that lets you rediscover your musicTuomas Nurmi worked on making the codebase Qt6-compatible. (Tuomas Nurmi, Link)Ark Archiving ToolJin Liu disabled the \"Compress to tar.gz/zip\" service menu items in read-only directories. (Jin Liu, 24.12.0. Link)Dolphin Manage your filesYou can now sort your videos by duration. (Somsubhra Bairi, 24.12.0. Link)Eren Karakas added more standard actions (Sort By, View Mode, Cut and Copy) to the context menu in the trash view. (Eren Karakas, 24.12.0. Link)Elisa Play music and listen to online radio stationsElisa now supports loading lyrics from .lrc files sitting alongside the song files. (Gary Wang, 24.12.0. Link)Manuel Roth fixed the bug in which the metadata for webradio http streams was not getting displayed. (Manuel Roth, 24.12.0. Link)Haruna Media playerYou now have the option to open videos in full screen mode. (Rikesh Patel, Link)Volker Krause was at the OSM Hack Weekend last week and worked on the support of MOTIS v2 API support in the public transport client library used by KDE Itinerary. He also added a map view of an entire trip to Itinerary and the KPublicTransport demo application.Read more on his blog!Kate Advanced Text EditorIn large repos, a git status update can be slow. The least we can do for the user is show that something is happening. Hence, now, if the git status is being refreshed you will see the refresh button become unclickable and start spinning. (Waqar Ahmed, 24.12.0. Link)In the project tree view, files will now show their status in git. The status is shown minimally, i.e. via a small circle displayed in front of the file name. If the file has been modified, the circle is red; if the file is staged, it's green. (Waqar Ahmed, 24.12.0. Link)We simplified the git panel by hiding the project combobox. The git panel will now show the status of the currently opened project. (Waqar Ahmed, 24.12.0. Link)We fixed the SQL plugin's SQL export being randomly ordered. (Waqar Ahmed, 24.08.3. Link)Clock Keep time and set alarmsKClock's timer now shows the remaining time instead of the elapsed time. (Zhangzhi Hu, 24.12.0. Link)KMix Sound MixerWe fixed the Audio Setup button, which didn't open the System Settings Audio page correctly. (Sergio Basto, 24.12.0. Link)KMyMoney Personal finance manager based on double-entry bookkeepingIt's once again possible to download stock quotes from yahoo.com after they changed their output format. (Ralf Habacker, Link)Reports can now be exported as PDF and XML. (Ralf Habacker, KMyMoney 5.2.0. Link 1, link 2)Photos Image GalleryWe improved the design of the properties panel. (Carl Schwan, 24.12.0. Link)Kleopatra Certificate manager and cryptography appThe name of the \"KWatchGnuPG\" utility provided by Kleopatra has been updated to \"GnuPG Log Viewer\" (Carl Schwan, 24.12.0. Link) and we gave it a new logo.KleverNotes Take and manage your notesKleverNotes' painting mode has been completely rewritten. It is now possible to add circles, rectangles, labels, and to choose the stroke size. The UI also uses a new floating toolbar. (Louis Schul, 1.2.0. Link)We improved the animation when switching pages. (Luis Schul, 1.2.0. Link)The note preview in the appearance settings was simplified to only show the important parts. (Luis Schul, 1.2.0. Link)KMail A feature-rich email applicationFix a crash in the Exchange Web Services (EWS) backend. (Louis Moureaux, 24.08.3. Link)KRDC Connect with RDP or VNC to another computerWe fixed sharing folders. (Fabio Bas, 24.08.3. Link)Merkuro Calendar Manage your tasks and events with speed and easeClaudio Cambra fixed adding and creating sub-todos (Claudio Cambra, 24.08.3. Link and Link)) and a bug that made clicking on the month view unreliable. (Claudio Cambra, 24.08.3, Link).We also added back the maps showing the location of individual events. This was disabled during the Qt6 migration and never enabled back afterwards. (Claudio Cambra, 24.08.3, Link)NeoChat Chat on MatrixSupport for libQuotient 0.9 has been backported to NeoChat 24.08. This brings, among other things, cross-signing support and support for the Matrix 1.12 API, including most importantly content repo functionality switching to authenticated media. (James Graham, 24.08.0, Link)Okular View and annotate documentsAlbert Astals fixed switching between pages in the single-page mode when using a mouse with a \"high resolution\" scroll wheel. (Albert Astals Cid, 24.12.0. Link)You can now use any image type as a signature background. (Sune Vuorela, 24.12.0. Link)We removed the last CHM support mention in Okular and on the website. CHM support was dropped when transitioning to the Qt6 version. (Albert Astals Cid, 24.12.0. Link 1, link 2)Zanshin To Do Management ApplicationFixed an issue where projects would be displayed twice when toggling on and off their data source. (David Faure, 24.08.3. Link)And all this too...Justin Zobel fixed various appstream files to use the new way of declaring the developer's name. (Justin Zobel, KRuler, Gwenview, KEuroCalc, ...)We ported various projects to use declarative QML declaration for better maintainance and performance (Carl Schwan, Koko, Francis, Kalk).... And Everything ElseThis blog only covers the tip of the iceberg! If you\u2019re hungry for more, check out Nate's blog about Plasma and be sure not to miss his This Week in Plasma series, where every Saturday he covers all the work being put into KDE's Plasma desktop environment.For a complete overview of what's going on, visit KDE's Planet, where you can find all KDE news unfiltered directly from our contributors.Get InvolvedThe KDE organization has become important in the world, and your time and contributions have helped us get there. As we grow, we're going to need your support for KDE to become sustainable.You can help KDE by becoming an active community member and getting involved. Each contributor makes a huge difference in KDE \u2014 you are not",
    "comments": [],
    "description": "\n  \n\nWelcome to a new issue of \"This Week in KDE Apps\"! Every week we cover as much as possible of what's happening in the world of KDE apps.",
    "document_uid": "2e3f55b873",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962384",
    "title": "Arduino Euclidean Gate Sequencer",
    "url": "https://diyelectromusic.com/2024/10/04/arduino-euclidean-gate-sequencer/",
    "score": 1,
    "timestamp": "2024-10-27T14:11:12",
    "source": "Hacker News",
    "content": "I\u2019ve always liked the idea of mathematics, algorithms and their application to music, so a Euclidean Sequencer has been on my list of things \u201cto do\u201d for a while. This is my take on how to build one from an Arduino. This post focuses on the code and algorithms \u2013 some more useful hardware will come along later maybe in a future post. But before I get stuck in, first let me call out two very comprehensive and well implemented and documented DIY Arduino Euclidean Sequencers that do so much more than I\u2019m thinking of doing: Warning! I strongly recommend using old or second hand equipment for your experiments. I am not responsible for any damage to expensive instruments! These are the key tutorials for the main concepts used in this project: If you are new to Arduino, see the Getting Started pages. Introduction and Background The basic idea is that splitting a number of steps in a sequence up into equal, but irregular timings, will generate interesting polyrhythms. The \u201cEuclidean\u201d part comes from the original paper that recognised a similarity between two approaches to splitting up the sequence and a range of rhythms that were in use around the world. The trick is working out how best to evenly distribute the beats across the sequence. Easy when you have 4 beats across a 16 step sequence, but not so easy for 5, 7 or 13 (for example). It turns out (see the references above) that there is a relatively simple algorithm to achieve this. In pseudo-code it looks like this: steps = 16beats = 7counter = steps-beats OR 0 if beats = 0FOR EACH step: counter += beats IF counter >= steps THEN this step is ON ELSE this step is OFF counter %= steps Basically, we keep adding \u201cbeats\u201d to a counter and every time the counter overflows the total number of steps, we log a beat to be played, and then have the counter wrap-around. Note: ideally we want the counter = steps at the start to ensure our first step is ON, but as the first thing we do is counter += beats, the counter has to be set to steps-beats to achieve this. The exception is if for some reason beats is 0, then we need all outputs to be zero, so need the counter to be less than steps (so I just set it to 0 directly). Here is how the above example plays out for 7 beats across 16 steps for each of the 16 steps: Step Counter Output 0 16->0 1 1 7 0 2 14 0 3 21->5 1 4 12 0 5 19->3 1 6 10 0 7 17->1 1 8 8 0 9 15 010 22->6 111 13 012 20->4 113 11 014 18->2 115 9 0 So the optimal sequence for playing 7 beats evenly across 16 steps of a sequencer is: 1001 0101 0010 1010OR100 10 10 100 10 10 10 This algorithm can thus be used to work out any combination of beats up to a specific number of steps. Here is some code for an Arduino that will output to the serial monitor all the sequences for a number of steps, in the form of a C language structure that can then be pasted into another Arduino sketch. #define STEPS 16void setup() { Serial.begin(9600); for (int i=0; i<=STEPS; i++) { EuclideanPattern(STEPS, i); }}void EuclideanPattern (int steps, int beats) { if (beats > steps) return; int cnt = 0; if (beats > 0) { cnt = steps - beats; } Serial.print(\" {\"); for (int i=0; i<steps; i++) { cnt += beats; if (cnt >= steps) { Serial.print(\"1\"); } else { Serial.print(\"0\"); } if (i < steps-1) { Serial.print(\",\"); } cnt = cnt % steps; } Serial.print(\"}, \\/\\/ \"); Serial.print(beats); Serial.print(\"\\n\");}void loop() {} Which gives the following output \u2013 all the Euclidean pattern combinations for a 16-step sequence. {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}, // 0 {1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}, // 1 {1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0}, // 2 {1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0}, // 3 {1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0}, // 4 {1,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0}, // 5 {1,0,0,1,0,0,1,0,1,0,0,1,0,0,1,0}, // 6 {1,0,0,1,0,1,0,1,0,0,1,0,1,0,1,0}, // 7 {1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0}, // 8 {1,0,1,0,1,0,1,0,1,1,0,1,0,1,0,1}, // 9 {1,0,1,0,1,1,0,1,1,0,1,0,1,1,0,1}, // 10 {1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1}, // 11 {1,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1}, // 12 {1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1}, // 13 {1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1}, // 14 {1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1}, // 15 {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1}, // 16 This might typically be used something like the following: #define PATTERNS (STEPS+1)const uint8_t patterns[PATTERNS][STEPS] = { ... data goes here ...};int pattern = 7; // Choose pattern between 0 and 16 inclusivefor (int step=0; step<STEPS; step++) { if (patterns[pattern][step]) { // Play } else { // Skip }} In a sequencer we\u2019d probably want to include options for adjusting the starting point in the sequence too, but this is the basics of what is to follow. Parts list Arduino Uno R3 10K potentiometer Optional: LEDs and associated (e.g. 220\u03a9 or 1K) resistors Breadboard and jumper wires The Circuit A potentiometer to control the tempo is connected to A0. There are five clock outputs on D8-D12. To illustrate the functioning of the clock patterns, each output is connected via a resistor to an LED and then onto GND, but this is optional. It just makes the output visible. The outputs will be a 5V signal, but it is not buffered in anyway. It might be possible to use the signal \u201cas is\u201d depending on what it is driving, but in order to avoid damaging the Arduino what is connected to it must not draw more than around 20mA of current. In order to avoid damaging the thing you want to connect to it \u2013 my strong recommendation is not to connect it at all. This is not a completely usable circuit, it is meant just to be proving the software. The Code The idea is to use a timer to provide a constant \u201cTICK\u201d for the system. The TEMPO (set with the pot) will determine how many TICKs are in each STEP. And the Euclidean algorithm will determine which STEPs have a GATE on and which are off. This will be supported for a number of GATE outputs allowing",
    "comments": [],
    "description": "I've always liked the idea of mathematics, algorithms and their application to music, so a Euclidean Sequencer has been on my list of things \"to do\" for a while. This is my take on how to build one from an Arduino. This post focuses on the code and algorithms - some more useful hardware will\u2026",
    "document_uid": "578344df59",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962371",
    "title": "A Cartography of Genocide",
    "url": "https://forensic-architecture.org/investigation/a-cartography-of-genocide",
    "score": 3,
    "timestamp": "2024-10-27T14:09:43",
    "source": "Hacker News",
    "content": "We're sorry but the Forensic Architecture website doesn't work properly without JavaScript enabled. Please enable it to continue.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "0fc1afaebc",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962362",
    "title": "Show HN: QuickScreen \u2013 Create job post with bespoke screening questions",
    "url": "https://quickscreen.ai",
    "score": 1,
    "timestamp": "2024-10-27T14:08:00",
    "source": "Hacker News",
    "content": "Avoid low effort applicant on your job post with automated screening questions based on uploaded CV.",
    "comments": [],
    "description": "Automate candidate screening with quickscreen.ai \u2013 generate customized screening questions, analyze responses, and get instant compatibility scores.",
    "document_uid": "609bd82aa6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962356",
    "title": "After the Deluge",
    "url": "https://www.realclearinvestigations.com/articles/2024/10/25/after_the_deluge_1067586.html",
    "score": 9,
    "timestamp": "2024-10-27T14:07:12",
    "source": "Hacker News",
    "content": "Residents of western North Carolina try to reconcile with the \u201conce in a thousand years\u201d storm that wasn\u2019t supposed to happen to them. ASHEVILLE, North Carolina \u2014 At 7:30 a.m. on Friday, Sept. 27, Chris Trusz was standing on one of the bridges spanning the Broad River in Chimney Rock. He wanted to get a photo. It had been raining steadily for 36 hours and the river was running 10 inches above normal. Trusz, who\u2019d moved to the western North Carolina mountain town 18 months earlier, wasn\u2019t worried; residents had been warned there might be a bit of flooding. He got his picture and walked up the hill to his home. \u201cNormally I have a sliver of a view of the river,\u201d he said. \u201cNow I\u2019m looking and can see the river clearly.\u201d By the time he got back to Main Street, the Broad was three times as wide and running 30 inches high. Within the hour, buildings had slid off their foundations, some taken down by the furious mud-colored current and disappearing completely. \u201cWe were watching homes wash by, all kinds of debris,\u201d said Trusz. Worse, he recalled, were the cars being carried away, some with their headlights still on. \u201cI can\u2019t unsee that,\u201d said Trusz, three weeks after Hurricane Helene took down several western North Carolina towns, paralyzed the entire region, and killed at least 123, a number that will almost certainly rise and may prove unknowable. It is one of several terrible unknowns the residents of western North Carolina now face. That they were unprepared for Helene is not on them \u2013 neither was the government nor anyone else. The \u201conce in a thousand years\u201d storm was not supposed to happen here, 500 miles from the Gulf of Mexico and 2,100 feet above sea level. There had been no local evacuation order even as the storm barreled their way. It would dump 30 inches of rain on western North Carolina and create up to 140-mile-per-hour winds. It would bring down untold thousands of trees. It would knock out the electrical grid, cell phone service, and the water supply all at once. In a matter of hours, it would obliterate the everyday security people felt, leaving survivors blinking into a new reality, wondering if they could or should rebuild lives in a place whose fragility had just been betrayed. \u201cIf you live here or own a business, where do you go when it\u2019s completely wiped out? I mean, how do you start over from that?\u201d asked Trusz, a property manager for Airbnb who\u2019d just found out the company was forbidding area rentals until June 2025 at least. Further betrayal would come, during America\u2019s overheated election season, from politicians, partisans, and conspiracists attempting to use the destruction and death caused by Helene to score political points. While mainstream news outlets suggested the area was being commandeered by armed right-wing militias, Georgia Rep. Marjorie Taylor Greene was tweeting, \u201cYes they can control the weather\u201d (no elaboration as to who \u201cthey\u201d were). Echoing a common complaint, Elon Musk claimed that \u201cFEMA is not merely failing to adequately help people in trouble, but is actively blocking citizens who try to help!\u201d Before long, Musk would instead be thanking Transportation Secretary Pete Buttigieg for \u201cexpediting approval for support flights.\u201d Below the radar were the innumerable others stoking discord, including the person who tweeted at me, \u201cWater Back on in Asheville NC. Feels like BLEACH on my skin. Very hot. Burning my face\u2026.\u201d This just after he retweeted several racist memes and the claim that the death toll in Asheville was over 8,000. Because of its unexpected and painful destruction, western North Carolina has become another symbol of America\u2019s cultural divide. One side is the politicization of everything, as human suffering was quickly transformed into a partisan cudgel swung by party operatives and media outlets who fed the public versions of events that advanced their favored narratives. On the other side was the heroic story of people and government working together as best they could in cataclysmic circumstances to aid and comfort one another. That second, hopeful story is what I found while reporting in and around Asheville last week. In a hotel with no running water, guests, some of whom had multiple trees fall on their homes, made do. While the scene could resemble a pajama party gone wrong, with people shuffling to the Porto-Sans in the driveway and choosing not to comment on the smell of body odor in the elevator, most folks showed concern for what their fellow travelers were going through. They left food and drink on a table in the lobby, next to a paper plate onto which someone had written \u201cTake what you need.\u201d They had neither the luxury nor desire to make political hay from their brethren\u2019s misery. And when the taps turned back on, the water was clear if not yet potable, and when you washed with it, it did not burn. Lobby of Extended Stay America, where patrons shared grocery items with anyone who needed them. RCI Capitalizing on the misfortune of others is reprehensible, and no one I met in western North Carolina had the leisure time or the inclination to do so. Many are still without drinking water. Commercial districts have the same ghostly quality they had during COVID, and the shoulders of many roadways are piled with what was left of downed trees, the white oaks, maples, and pines that drew 14 million visitors a year, especially in late September and October, when the region blazes red and gold. If many of those trees are now gone, what happens to the tourism economy and its $7.7 billion? What of Asheville\u2019s major creative draw, the River Arts District, where floodwaters reached 27 feet and where what studio spaces remain have been taken down to the studs? Where do people find the courage, and the resources, to start again? And what if the thousand-year storm turns out to have a different schedule? Gale's in",
    "comments": [
      {
        "author": "jmclnx",
        "text": "&gt;The \u201conce in a thousand years\u201d storm<p>Phrases like this means nothing, I am sure with in a year we will be hearing \u201conce in a million year storm&quot;.<p>Climate Change is here now, I remember predictions decades ago of storms like these occurring more and more starting in the 2020s.  For some reason 2025 sticks in my head about these predictions.  Seems again the science is correct.<p>I feel bad people had and will have to go through this over and over.  But it is not as if we were not warned.",
        "time": "2024-10-27T14:55:18"
      }
    ],
    "description": "ASHEVILLE, North Carolina \u2014 At 7:30 a.m. on Friday, Sept. 27, Chris Trusz was standing on one of the bridges spanning the Broad River in Chimney Rock. He wanted to get a photo. It had been raini",
    "document_uid": "3c89dafdb0",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962323",
    "title": "Web of Science index puts eLife 'on hold' due to its publishing model",
    "url": "https://www.science.org/content/article/web-science-index-puts-elife-hold-because-its-radical-publishing-model",
    "score": 3,
    "timestamp": "2024-10-27T14:01:42",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "f998f36cf2",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962319",
    "title": "PostgreSQL Extension Pedia, with 345 out-of-box RPM/DEB packages available",
    "url": "https://ext.pigsty.io/",
    "score": 1,
    "timestamp": "2024-10-27T14:01:24",
    "source": "Hacker News",
    "content": "Loading ...",
    "comments": [],
    "description": "Pigsty, A Battery-Included Open Source PostgreSQL Distribution",
    "document_uid": "301fdf6616",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962291",
    "title": "Security bugs engulf ShipFast, a popular indie hacker's product, in drama",
    "url": "https://www.indiehackers.com/post/tech/security-bugs-engulf-shipfast-a-popular-indie-hackers-product-in-drama-y2VgBkHu7b91rSCEEUSO",
    "score": 3,
    "timestamp": "2024-10-27T13:57:16",
    "source": "Hacker News",
    "content": "Being one of the biggest names in the space, Marc Lou is always one of the most discussed indie hackers on the X timeline. He\u2019s frequently praised\u2026 and frequently criticized. Now he's at the center of an explosive controversy that has drawn the entire indie hacking community into a debate over product quality, security practices, and the ethics of public bug reporting.And it all began with a tweet from a fellow indie hacker.Simon finds a bugOn October 18th, an indie hacker that goes by Simon quote-tweeted a post praising ShipFast\u2019s affiliate program:The tweet went super viral, pulling in four million views, and it was a precursor for the debates to come. There were people defending the merits of boilerplates and those who took the opposite position.But Simon wasn't done. Later that day, he posted about an error with ShipFast\u2019s server side validation:With 11 million views, this tweet went even more viral then the first. As you\u2019d expect, the discussion was much more lively than before, with many indie hackers feeling like Simon violated the unwritten rules around bug reporting.Here\u2019s John Rush with an accurate representation of the prevailing sentiment:However, Simon didn\u2019t feel he'd done anything wrong.And he wasn't alone in that belief:Marc, meanwhile, seemed completely unamused:Little did he know that Simon was just getting started.And another one, and another one\u2026On October 19th, Simon tweeted that he'd found a new Marc Lou-related bug.This time it was a serious security vulnerability with IndiePage, and because of its seriousness, he sent him a DM instead of posting it publicly.Later that day, he put ShipFast on serious blast:He then criticized Marc\u2019s use of SVGs instead of an icon library and how he was able to get ShipFast for free.This seemed to spur a community security audit, as other people also began to find serious security vulnerabilities:According to Simon, the reason for his crusade was simple: ShipFast is $200 and is used by a lot of people, so it deserves the highest level of security. So, if Marc isn\u2019t going to respond privately, he has no choice but to post publicly. Justified or not, this got Marc\u2019s attention, and not in a good way:Unfortunately for Marc, it also got the attention of the rest of indie hacker X.The boilerplate debateAfter Simon\u2019s numerous finds, many of the people who'd previously supported Marc Lou began to turn against him, with the consensus being that his response had been too dismissive considering the seriousness of the issues:This then became a debate over what a boilerplate should be:And how a boilerplate should be marketed:Even Pieter Levels chimed in:What does this mean for the future of indie hacking?With this tweet, Marc put an end to this saga:But the debates spurred on by the drama are sure to stay:What should be expected from a boilerplate?What is the correct way to report a bug?And, as Dagobert puts it, is the indie hacking community becoming toxic?It\u2019ll be interesting to see how these questions are answered in the coming months and years.",
    "comments": [],
    "description": "ShipFast is one of the most popular indie projects ever, but a wave of security bugs has thrown it into controversy.",
    "document_uid": "d50c2ca5ee",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962266",
    "title": "CloudTail: An OSS Tool for Long-Term Cloud Log Retention and Searchability",
    "url": "https://permiso.io/blog/introducing-cloudtail-an-open-source-tool-for-long-term-cloud-log-retention-and-searchability",
    "score": 1,
    "timestamp": "2024-10-27T13:51:47",
    "source": "Hacker News",
    "content": "Introduction In the labyrinth of modern cloud infrastructure, effective log management is a cornerstone for ensuring operational security and compliance. However, small and medium-sized enterprises often find themselves at a crossroads \u2014 balancing budget constraints against the need for comprehensive log analysis tools. Even larger organizations aren't spared, as they battle against restrictive API throttling and limited log retention policies enforced by major cloud providers such as AWS and Azure. Introducing CloudTail CloudTail is an open-source tool designed to enhance the long-term retention and searchability of cloud logs. It offers the flexibility to selectively preserve a curated subset of significant cloud events from AWS and Azure, all without the need for expensive SIEM solutions. Key Features Multi-Cloud Support: Compatible with various cloud providers including AWS and Azure. Config-Based Event Filtering Allows users to specify and retrieve detailed events using a config-based approach that interacts seamlessly with native cloud APIs. Flexible Data Storage Stores logs in their raw format locally while extracting and normalizing key properties to facilitate easier and more powerful searching capabilities. Automated and Scheduled Execution Designed to be run as a scheduled task, maintaining an execution history to ensure consistent log collection without duplicate events. How CloudTail Works CloudTail utilizes a JSON configuration file that offers a straightforward way for users to specify exactly how they want to monitor cloud events. Within this file, users can fine-tune the parameters to focus on specific events or expand their scope to encompass a broader range of activities across AWS and Azure. This flexibility allows users to tailor the monitoring to specific needs, whether they are tracking a narrow set of actions or overseeing a wide array of cloud operations. The default configuration file we provide contains a curated list of high-value events that we have identified as most important for security operations. These include key events related to user management, resource changes, and security alerts. However, users can easily modify this configuration to suit their specific needs, adding more detailed or customized event filters as required. The JSON configuration file in CloudTail is designed for precision in event monitoring across multiple cloud platforms including AWS and Azure: Data Sources Configuration: Users define which cloud environments to monitor by listing specific data sources such as AWS CloudTrail or Azure Activity Logs. Each entry can support multiple accounts or subscriptions, offering a scalable approach for organizations operating across various cloud services. Event Filtering Mechanics: Within each data source, the file allows users to determine key event attributes through lookup_Attributes. This is essential for identifying which events to capture, focusing on specific attributes like EventName to ensure only pertinent data is collected, optimizing both resource usage and security monitoring efforts. Rule-based Event Selection: The configuration supports the creation of rules that specify how events are filtered and captured. For instance, setting a rule to monitor all CreateUser events helps organizations keep tabs on new user account creations, a vital element in user management and security protocols. Local Storage and Data Normalization: Once events are retrieved, they are stored locally in their original format and also undergo a normalization process. This approach not only maintains the integrity of the original data but also simplifies later searches and analysis, boosting both the usability and investigative capabilities of the system. Usage Examples Example #1: Detecting S3 Activity Monitor Amazon S3 activities by setting EventSource as the AttributeKey with AttributeValue set to s3.amazonaws.com. This configuration hones in on S3-related events. Example #2: Tracking Deletion Events To capture all deletion-related events, utilize wildcard matching (e.g. Delete*). This is crucial for overseeing critical changes and maintaining security. Example #3: Filtering User Creation Events For monitoring specific user creation events, use EventName as the AttributeKey with AttributeValue set to CreateUser. Include CloudTrail\u2019s client-side JMESPath filtering to focus on particular users, enhancing your monitoring capabilities. Output CloudTail processes and stores the extracted event data in two SQLite databases (aws_events.db for AWS and azure_events.db for Azure), ensuring that event metadata is readily available for further analysis or reporting. In addition to storing events in SQLite databases, CloudTail allows you to export the processed events as JSON files for easier viewing and external use. Conclusion Permiso Security is continually hard at work to show our customers the most important activity across all of their identity and cloud environments, always linking the context of the originating identity (human or non-human). We hope CloudTail helps many organizations to simplify their multi-cloud log collection and retention process as a first step in ensuring they retain the highest-value events relevant to their security needs. You can access CloudTail on Github: https://github.com/Permiso-io-tools/cloudtail",
    "comments": [],
    "description": "CloudTail is an open-source tool designed to enhance the long-term retention and searchability of cloud logs. It offers the flexibility to selectively preserve a curated subset of significant cloud events from AWS and Azure, all without the need for expensive SIEM solutions.",
    "document_uid": "adb3888124",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962253",
    "title": "Liquid vs. Illiquid Careers",
    "url": "https://www.everythingisatrolley.com/p/liquid-vs-illiquid-careers",
    "score": 2,
    "timestamp": "2024-10-27T13:50:06",
    "source": "Hacker News",
    "content": "Note: This post is a preview of the types of content I'll be sharing in my newsletter, \"Optima & Outliers\" If you enjoy this piece, subscribe here.We're used to thinking about skills and experience in terms of their market value - how useful they are and how much employers or customers would pay for them. But another, more subtle dimension gets less attention: liquidity.Just like in financial markets, some human capital is easier to price and trade than others. This is not about the absolute value of your skills or experience, but how easily the market can assess and exchange them.Consider Jim - with 4 years of experience as a management consultant at McKinsey. In the job market, potential employers have a good idea of what they are buying - someone smart and conscientious enough to be hired by McKinsey; meets a reasonably high bar on commercial acumen and communication skills; and polished enough to have managed clients and moved one step higher on the well-defined McKinsey ladder.Jim's skills are highly legible to every potential employer. They can estimate his salary within narrow confidence intervals, given that McKinsey's pay structure is essentially public information. If Jim decides to switch jobs or move countries, all else equal, he can be relatively confident about his chances of landing a job that pays him his \"market value\". Much like publicly traded stocks, his legible experience and skills can be sold quickly at the prevailing market rate.On the flip side, picture someone who's spent two years at an AI startup, followed by a stint running operations for a non-profit in Asia, and is now working on a political campaign. How comfortable would you be guessing this person's potential earnings or, more fundamentally, the value they could generate?If I were hiring for a high-leverage, entrepreneurial position requiring an exceptional young candidate, I'd be more inclined to interview this person over most McKinsey consultants. However, I'd evaluate them more rigorously than someone with the McKinsey stamp, and might even limit my search to candidates within my social or professional network.When human capital is built through non-linear or less legible paths, the lack of legibility increases variance from the employer's perspective. This isn't necessarily negative, but it does increase the value of additional information. If the cost of obtaining this information doesn't justify the potential upside, candidates with less legible backgrounds may be passed over in screening processes.In essence, linear, legible paths represent liquid investments in human capital - easily valued and traded. Non-linear, \"customized\" paths, on the other hand, are illiquid investments. They could be accruing significant value or none at all, but they're hard to mark to market. Even with an impressive skill set, you can't easily cash out by simply applying to a few publicly posted jobs.So, what makes some career paths and skill sets more liquid than others? Dimensions that drive liquidityIn financial markets, liquidity is defined as the efficiency or ease with which an asset can be converted into cash at a given price or within a given price range.In the context of human capital, liquidity is a function of two factors: the overall demand for your skills/experience, and the ease with which others can verify that you possess these skills at the level you claim.Several dimensions drive the liquidity of human capital:Industry growth and activity: Skills that can be deployed in high-growth industries or sectors with high levels of activity tend to be more liquid. The increased \"trading volume\" of human capital in these areas enhances liquidity.Skill specificity: Counterintuitively, more specific skills often enjoy higher liquidity. A carpenter's abilities are well-defined and easily understood, whereas \"project management\" can vary widely based on context. This clarity makes it easier for a carpenter to market their skills, even in a new location.Testability: Skills that can be easily and quickly assessed tend to be more liquid. For instance, a software developer's competence can often be gauged through a brief coding test. In contrast, evaluating leadership or managerial skills typically requires more time and varied assessment methods, resulting in lower liquidity.Context dependency: If your value primarily stems from familiarity with a specific organization or geographically limited social capital, your skills may be less liquid. However, this can sometimes correlate with skill specificity, so the net effect on liquidity isn't always straightforward.Institutional brand: Larger, more established brands often provide greater liquidity to their employees' human capital. This is due to their well-known screening processes and the larger sample size of previous employees, allowing potential employers to better estimate the value typically generated by individuals from these institutions.The Appeal of Liquid Paths is the Case for Illiquid OnesLegible (liquid) paths offer non-monetary compensation in the form of psychological comfort and optionality. With each additional year of experience, your career capital becomes highly visible - salary increases, promotions, or clear market rates for your level. You can probably move to a different country (or even industry) and have your skills valued easily.There's another reason these liquid paths are in high demand. Sure, competing for a McKinsey job isn't easy, but everything from recruiting to promotions is structured and streamlined. Job seekers and employers are cushioned from uncertainty and ambiguity in a way that only becomes obvious when you consider the counterfactual. Moreover, one can also more reliably predict future income and gain psychological security (or at least perceived security) around one\u2019s career trajectory.Given these hidden benefits, you'd expect these paths to be crowded. That\u2019s one argument in favor of pursuing non linear, illiquid paths instead. You should expect \u201calpha\u201d in these paths precisely because lots of smart, conscientious people are terrified of uncertainty. Maybe you can afford to be less of those things if you're willing to be brave?In financial markets, illiquid assets often command a premium - investors accept lower liquidity for the prospect of higher returns. I can't prove empirically that this is replicated in human capital markets but that's a reasonable prior.Costs and Constraints of Hierachies and StandardizationAnother source of alpha comes from avoiding the",
    "comments": [],
    "description": "Note: This post is a preview of the types of content I'll be sharing in my newsletter, \"Optima & Outliers\" If you enjoy this piece, subscribe here.",
    "document_uid": "c7f49e1c17",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962246",
    "title": "Gitlab as a Terraform state back end (2022)",
    "url": "https://balaskas.gr/blog/2022/11/11/gitlab-as-a-terraform-state-backend/",
    "score": 1,
    "timestamp": "2024-10-27T13:49:37",
    "source": "Hacker News",
    "content": "Using Terraform for personal projects, is a good way to create your lab in a reproducible manner. Wherever your lab is, either in the \u201ccloud\u201d aka other\u2019s people computers or in a self-hosted environment, you can run your Infrastructure as code (IaC) instead of performing manual tasks each time. My preferable way is to use QEMU/KVM (Kernel Virtual Machine) on my libvirt (self-hosted) lab. You can quickly build a k8s cluster or test a few virtual machines with different software, without paying extra money to cloud providers. Terraform uses a state file to store your entire infra in json format. This file will be the source of truth for your infrastructure. Any changes you make in the code, terraform will figure out what needs to add/destroy and run only what have changed. Working in a single repository, terraform will create a local state file on your working directory. This is fast and reliable when working alone. When working with a team (either in an opensource project/service or it is something work related) you need to share the state with others. Eitherwise the result will be catastrophic as each person will have no idea of the infrastructure state of the service. In this blog post, I will try to explain how to use GitLab to store the terraform state into a remote repository by using the tf backend: http which is REST. Greate a new private GitLab Project We need the Project ID which is under the project name in the top. Create a new api token Verify that your Project has the ability to store terraform state files You are ready to clone the git repository to your system. Backend Reading the documentation in the below links seems that the only thing we need to do, is to expand our terraform project with this: terraform { backend \"http\" { } } Doing that, we inform our IaC that our terraform backend should be a remote address. Took me a while to figure this out, but after re-reading all the necessary documentation materials the idea is to declare your backend on gitlab and to do this, we need to initialize the http backend. The only Required configuration setting is the remote address and should be something like this: terraform { backend \"http\" { address = \"https://gitlab.com/api/v4/projects/<PROJECT_ID>/terraform/state/<STATE_NAME>\" } } Where PROJECT_ID and STATE_NAME are relative to your project. In this article, we go with GITLAB_PROJECT_ID=\"40961586\" GITLAB_TF_STATE_NAME=\"tf_state\" Terraform does not allow to use variables in the backend http, so the preferable way is to export them to our session. and we -of course- need the address: TF_HTTP_ADDRESS=\"https://gitlab.com/api/v4/projects/${GITLAB_PROJECT_ID}/terraform/state/${GITLAB_TF_STATE_NAME}\" For convience reasons, I will create a file named: terraform.config outside of this git repo cat > ../terraform.config <<EOF export -p GITLAB_PROJECT_ID=\"40961586\" export -p GITLAB_TF_STATE_NAME=\"tf_state\" export -p GITLAB_URL=\"https://gitlab.com/api/v4/projects\" # Address export -p TF_HTTP_ADDRESS=\"${GITLAB_URL}/${GITLAB_PROJECT_ID}/terraform/state/${GITLAB_TF_STATE_NAME}\" EOF source ../terraform.config this should do the trick. Authentication In order to authenticate via tf against GitLab to store the tf remote state, we need to also set two additional variables: # Authentication TF_HTTP_USERNAME=\"api\" TF_HTTP_PASSWORD=\"<TOKEN>\" put them in the above terraform.config file. Pretty much we are done! Initialize Terraform source ../terraform.config terraform init Initializing the backend... Successfully configured the backend \"http\"! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Finding latest version of hashicorp/http... - Finding latest version of hashicorp/random... - Finding latest version of hashicorp/template... - Finding dmacvicar/libvirt versions matching \">= 0.7.0\"... - Installing hashicorp/random v3.4.3... - Installed hashicorp/random v3.4.3 (signed by HashiCorp) - Installing hashicorp/template v2.2.0... - Installed hashicorp/template v2.2.0 (signed by HashiCorp) - Installing dmacvicar/libvirt v0.7.0... - Installed dmacvicar/libvirt v0.7.0 (unauthenticated) - Installing hashicorp/http v3.2.1... - Installed hashicorp/http v3.2.1 (signed by HashiCorp) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. ... Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Remote state by running terraform plan we can now see the remote terraform state in the gitlab Opening Actions \u2013> Copy terraform init command we can see the below configuration: export GITLAB_ACCESS_TOKEN=<YOUR-ACCESS-TOKEN> terraform init -backend-config=\"address=https://gitlab.com/api/v4/projects/40961586/terraform/state/tf_state\" -backend-config=\"lock_address=https://gitlab.com/api/v4/projects/40961586/terraform/state/tf_state/lock\" -backend-config=\"unlock_address=https://gitlab.com/api/v4/projects/40961586/terraform/state/tf_state/lock\" -backend-config=\"username=api\" -backend-config=\"password=$GITLAB_ACCESS_TOKEN\" -backend-config=\"lock_method=POST\" -backend-config=\"unlock_method=DELETE\" -backend-config=\"retry_wait_min=5\" Update terraform backend configuration I dislike running a \u201clong\u201d terraform init command, so we will put these settings to our tf code. Separating the static changes from the dynamic, our Backend http config can become something like this: terraform { backend \"http\" { lock_method = \"POST\" unlock_method = \"DELETE\" retry_wait_min = 5 } } but we need to update our terraform.config once more, to include all the variables of the http backend configuration for locking and unlocking the state. # Lock export -p TF_HTTP_LOCK_ADDRESS=\"${TF_HTTP_ADDRESS}/lock\" # Unlock export -p TF_HTTP_UNLOCK_ADDRESS=\"${TF_HTTP_ADDRESS}/lock\" Terraform Config So here is our entire terraform config file # GitLab export -p GITLAB_URL=\"https://gitlab.com/api/v4/projects\" export -p GITLAB_PROJECT_ID=\"<>\" export -p GITLAB_TF_STATE_NAME=\"tf_state\" # Terraform # Address export -p TF_HTTP_ADDRESS=\"${GITLAB_URL}/${GITLAB_PROJECT_ID}/terraform/state/${GITLAB_TF_STATE_NAME}\" # Lock export -p TF_HTTP_LOCK_ADDRESS=\"${TF_HTTP_ADDRESS}/lock\" # Unlock export -p TF_HTTP_UNLOCK_ADDRESS=\"${TF_HTTP_ADDRESS}/lock\" # Authentication export -p TF_HTTP_USERNAME=\"api\" export -p TF_HTTP_PASSWORD=\"<>\" And pretty much that\u2019s it! Other Colleagues So in order our team mates/colleagues want to make changes to this specific gitlab repo (or even extended to include a pipeline) they need Git clone the repo Edit the terraform.config Initialize terraform (terraform init) And terraform will use the remote state file.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "12cabd566a",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962242",
    "title": "The New Home Kitchen Business",
    "url": "https://www.nytimes.com/2024/10/25/headway/meet-the-new-home-kitchen-business.html",
    "score": 1,
    "timestamp": "2024-10-27T13:48:54",
    "source": "Hacker News",
    "content": "Please enable JS and disable any ad blocker",
    "comments": [
      {
        "author": "mooreds",
        "text": "<a href=\"https:&#x2F;&#x2F;archive.is&#x2F;g3u28\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.is&#x2F;g3u28</a>",
        "time": "2024-10-27T13:49:18"
      }
    ],
    "description": "No description available.",
    "document_uid": "adcb57cbce",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962216",
    "title": "Ask HN: Lean for Machine Learning?",
    "url": "https://news.ycombinator.com/item?id=41962216",
    "score": 1,
    "timestamp": "2024-10-27T13:46:06",
    "source": "Hacker News",
    "content": "JAX kinda excels at tracing python functions into jaxpr then into XLA because of the pure function assumption JAX has made.<p>How about using Lean, the dependently typed language. I know this might hinder speed of coding, but what if? Not just for deep learning but ML.<p>What if we write graph networks and graph updates through Lean or so on. Could there be any upsides with using Lean for ML? (The downsides are kinda obvious, so let\u2019s maybe talk about possible up sides)<p>I heard some research group is trying to add type theory into deep learning through category theory, but i\u2019m unsure of how that\u2019s going and if this question might be related to that.<p>Do you think there is any benefit in using Lean for ML research?",
    "comments": [],
    "description": "No description available.",
    "document_uid": "60c5f8516f",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962205",
    "title": "You-get: Dumb downloader that scrapes the web",
    "url": "https://github.com/soimort/you-get",
    "score": 16,
    "timestamp": "2024-10-27T13:45:02",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [
      {
        "author": "politelemon",
        "text": "It seems they do not want you to report an issue without an accompanying fix for it.<p>&gt; If you would like to report a problem you find when using you-get, please open a Pull Request, which should include [snip]<p>Can&#x27;t say I&#x27;ve encountered this before.",
        "time": "2024-10-27T15:01:09"
      },
      {
        "author": "KTibow",
        "text": "Can someone explain why this is better than yt-dlp",
        "time": "2024-10-27T15:07:27"
      }
    ],
    "description": ":arrow_double_down: Dumb downloader that scrapes the web - soimort/you-get",
    "document_uid": "e27f716bc9",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962192",
    "title": "Rumble in the Jungle 50 years on: how Ali danced rings around apartheid",
    "url": "https://www.theguardian.com/sport/2024/oct/27/rumble-in-the-jungle-50-years-on-how-ali-danced-rings-around-apartheid",
    "score": 2,
    "timestamp": "2024-10-27T13:42:40",
    "source": "Hacker News",
    "content": "Fifty years ago, in a corner of white South Africa, Muhammad Ali already seemed a miracle-maker. Deep in our strictly regimented and divided country, Ali danced rings around apartheid. I had first heard about the inspirational boxer from a black man, Cassius, who sold bottles of beer from the illegal shebeen he and his friends ran across the road from our house.Cassius and his crew kept their illicit stash hidden in the drains outside the corner shop owned by an irritable Greek man. Whenever my football was booted over the garden wall, Cassius chased after it. After a dazzling display of slightly drunken footwork he would return the ball with a cackle. One day, while showcasing his trickery, he sang a strange song: \u201cAli, Ali, float like a butterfly, sting like a bee, Ali, Ali, Muhammad Ali.\u201dMuhammad Ali receives advice from trainer Bundini Brown at his training camp in the summer of 1974. Photograph: APCassius flicked rangy left jabs into the winter sunshine as his huge feet danced. He wore a pair of battered brown sandals that had split at the seams. They fluttered over the tar while the soles flapped in a jitterbug of their own. He pretended to be outraged when I asked who he was singing about: \u201cYou mean the baasie [Afrikaans for little boss] don\u2019t know?\u201d When I shook my head he became serious: \u201cAli is the heavyweight champion of the world.\u201dA thrill surged through me. Cassius told me how he was nicknamed after Ali \u2013 who had been born as Cassius Clay. I struggled to understand how one man could have two names. Cassius explained that the master boxer was a black American who dreamed up those happy bee and butterfly lines.Years later, in 1974, when I had just turned 13, I learned that Ali had been stripped of his world title in 1967 when he refused to fight in the Vietnam war. But he had become even more of a mythical figure to me because Ali entranced our frightening Afrikaans teacher with the same spell he cast over Cassius.When we summoned the courage to ask him why he liked Ali so much, while suspecting he was a staunch racist, the teacher softened. He spoke of the beauty and brilliance of Ali in the ring. Rather than being \u201cone of our blacks\u201d, Ali resembled the king of the world.On 30 October 1974, Ali finally had a chance to regain the title when he faced George Foreman. We were agog that the fight would take place not too far from us, in Zaire [now the Democratic Republic of Congo].The Rumble in the Jungle was promoted by Don King who underlined his ingenuity by taking the bout \u201cback to Africa\u201d. Zaire\u2019s dictatorial President, Mobutu Sese Seko, agreed to pay the boxers an unprecedented $10m each.Although the rest of Africa felt as far removed from our privileged suburb near Johannesburg as it did in Hertfordshire or New Hampshire, King brought the continent into our classrooms. Other kinder teachers confessed their fondness for Ali and favoured him over Foreman. Ali was also hailed by the black cleaners and gardeners who serviced the school and our homes. And the shebeen corner \u2013 from the Greek shop owner to the biggest drinkers \u2013 still belonged to him. Only Ali could forge such an alliance.skip past newsletter promotionafter newsletter promotionNo heavyweight was bigger or more threatening than Foreman, who had become world champion when demolishing Joe Frazier in two rounds. Frazier was so good he had beaten Ali in the Fight of the Century in 1971 \u2013 but he was blown away by Foreman who had a 40-0 record with 38 stoppages. Big George rained down bludgeoning punches, bringing sorrow to every fighter he faced. Only the 32-year-old Ali remained.\u201cForeman by knockout,\u201d I predicted mournfully. Maybe I made that pessimistic forecast because I feared so much for Ali. A quick knockout would save him from permanent damage. But Bennie da Silva, my friend\u2019s dad and the only real boxing expert we knew, backed Ali. He was a stocky Portuguese man who made us laugh while flooring us with his ring knowledge.He promised that Ali would dance the night away until Foreman was so dizzy he wouldn\u2019t know what hit him. Ali would rumba through the rumble and be crowned world champion again.Muhammad Ali stands back as the referee, Zack Clayton, calls the count over George Foreman in the eighth round. Photograph: APTelevision was still banned in South Africa as the government regarded it as a tool of communist propaganda. So we could not watch the fight in the early hours of a spring morning. But we listened to games of English football on the BBC World Service every Saturday afternoon that meant our schoolyard was packed with staunch followers of Arsenal and Liverpool, Leeds and Manchester United.Muhammad Ali is cheered by a crowd as he drives in downtown Kinshasa for a sightseeing trip ahead of his fight against George Foreman. Photograph: APMy dad helped me tune into the BBC radio broadcast and, in bed, I trembled as Ali went into battle with the ogre. But I was stunned as, in the crackle and hiss of the wireless, Ali did not do the rumba. He simply refused to dance.He not only stood still but, in an act of supposed madness, leaned against the ropes and allowed Foreman to hit him. Ali took every ruinous blow and still he stood, waving his man in and doing the \u201crope-a-dope\u201d trick we would try to copy at school.George Foreman walks toward his corner after his two-round win over the heavyweight champion, Joe Frazier. Photograph: APThen, as if he was as exultant as Cassius on the corner or Mr da Silva boxing in the kitchen, Ali began to pick off Foreman. Through the static and electrifying commentary it sounded like Ali was weaving a new kind of black magic.Abruptly, near the end of the eighth round, it was all over. A cry reverberated from the tiny speaker,",
    "comments": [],
    "description": "King of the world was revered across South Africa\u2019s racial divide when we heard the words \u2018Foreman is down!\u2019",
    "document_uid": "bb70cf0e09",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962189",
    "title": "Human Embryology and the Holy Quran: An Overview",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3068791/",
    "score": 1,
    "timestamp": "2024-10-27T13:42:32",
    "source": "Hacker News",
    "content": "403 Forbidden",
    "comments": [],
    "description": "No description available.",
    "document_uid": "7d77ee7eac",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962186",
    "title": "Single-pass online statistics algorithms (2013)",
    "url": "https://www.numericalexpert.com/articles/single_pass_stat/",
    "score": 1,
    "timestamp": "2024-10-27T13:42:02",
    "source": "Hacker News",
    "content": "Single-Pass Online Statistics Algorithms Nikolai Shokhirev March 13, 2013 1. Introduction The \"textbook\" two-pass algorithm for the centered moments (variance, skewness, kurtosis, covariance) is obviously inefficient. There were suggested several alternative algorithms [1 - 16]. Unfortunately I could not find suitable one-pass windowed algorithms similar to the mowing average [17]. Therefore I decided to derive the equations and implement them myself. 2. Problem statement We assume that there is a stream of data: $x_{1},\\, x_{2},\\, x_{3},\\,\\ldots\\,,\\, x_{k}$ . Here $x_{k}$ is the most recent value. There are two variants of statistics: (i) a cumulative statistics of all $k$ values, and (ii) the statistics of $n$ the most recent values. The latter is also called a moving or a windowed statistics. Note that the cumulative statistics is also a windowed with $n=k$. 3. The first moment, mean value 3.1. Definition The first moment (moving average, mean value) of the length $n$ is defined as \\begin{equation} m_{k}(n)=\\frac{1}{n}\\sum_{i=k-n+1}^{k}x_{i}=\\frac{1}{n}S_{k}(n)\\label{eq:avg} \\end{equation} Below for $k=n$ we use the notation $X_{k}(k)=X_{k}$. 3.2. Cumulative calculations When all incoming values are incorporated into calculations ($n=k$), the above equation (\\ref{eq:avg}) reduces to: \\begin{equation} m_{k}=m_{k}(k)=\\frac{1}{k}\\sum_{i=1}^{k}x_{i}=\\frac{1}{k}S_{k}\\label{eq:avgc} \\end{equation} From Eq. \\ref{eq:avgc} we get the following recurrence: \\begin{eqnarray} m_{k} & = & \\frac{1}{k}S_{k}=\\frac{1}{k}\\left(x_{k}+S_{k-1}\\right)=\\frac{x_{k}}{k}+\\frac{k-1}{k}m_{k-1}=m_{k-1}+\\frac{x_{k}-m_{k-1}}{k}\\nonumber \\\\ & = & m_{k-1}+\\frac{\\delta_{k}}{k}\\label{eq:recc} \\end{eqnarray} where \\begin{equation} \\delta_{k}=x_{k}-m_{k-1}\\label{eq:dk} \\end{equation} 3.3. Windowed (constant-length, moving) calculations In the case when only the last $n$ values are used, the recurrence is \\begin{eqnarray} m_{k}(n) & = & \\frac{1}{n}S_{k}(n)=\\frac{1}{n}\\left(x_{k}+S_{k-1}(n)-x_{k-n}\\right)=\\frac{x_{k}}{k}+\\frac{k-1}{k}m_{k-1}=m_{k-1}+\\frac{x_{k}-x_{k-n}}{n}\\nonumber \\\\ & = & m_{k-1}(n)+\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\label{eq:recw} \\end{eqnarray} Here \\begin{equation} \\delta_{k-n}=x_{k-n}-m_{k-1}\\label{eq:dkn} \\end{equation} Note that (\\ref{eq:recw}) reduces to (\\ref{eq:recc}) if we set \\begin{eqnarray} \\delta_{k-n} & = & 0\\nonumber \\\\ n & = & k\\label{eq:w2c} \\end{eqnarray} Instead of (\\ref{eq:w2c}) we can also formally define $\\delta_{0}=0$. 4. Central moments 4.1. Definitions Similar to (\\ref{eq:avg}), the p-th order central moments are defined as \\begin{equation} M_{p,k}(n)=\\frac{1}{n}\\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k}(n)\\right]^{p}=\\frac{1}{n}S_{p,k}(n)\\label{eq:avgp} \\end{equation} 4.2. Windowed variance ($p=2$) For the case of $p=2$ Eq. \\ref{eq:avgp} reduces to \\begin{equation} M_{2,k}(n)=\\frac{1}{n}\\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k}(n)\\right]^{2}=\\frac{1}{n}S_{p,k}(n)\\label{eq:avg2} \\end{equation} It can be rewritten using (\\ref{eq:recw}): \\begin{eqnarray} S_{2,k}(n) & = & \\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k}(n)\\right]^{2}=\\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k-1}(n)-\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right]^{2}\\nonumber \\\\ & = & \\sum_{i=k-n+1}^{k}\\left\\{ \\left[x_{i}-m_{k-1}(n)\\right]^{2}-2\\left[x_{i}-m_{k-1}(n)\\right]\\frac{\\delta_{k}-\\delta_{k-n}}{n}+\\left(\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right)^{2}\\right\\} \\label{eq:S2w} \\end{eqnarray} or \\begin{eqnarray} S_{2,k}(n) & = & S_{2,k-1}(n)+\\delta_{k}^{2}-\\delta_{k-n}^{2}-2(\\delta_{k}-\\delta_{k-n})[m_{k}(n)-m_{k-1}(n)]+\\frac{(\\delta_{k}-\\delta_{k-n})^{2}}{n}\\nonumber \\\\ & = & S_{2,k-1}(n)+\\delta_{k}^{2}-\\delta_{k-n}^{2}-\\frac{(\\delta_{k}-\\delta_{k-n})^{2}}{n}\\label{eq:S2wrec} \\end{eqnarray} This gives the recurrence formula for the windowed online variance calculations. 4.3. Cumulative variance calculations In order to get the cumulative formula, we can formally apply the conditions (\\ref{eq:w2c}): \\begin{equation} S_{2,k}=S_{2,k-1}+\\delta_{k}^{2}-\\frac{\\delta_{k}^{2}}{k}=S_{2,k-1}+\\frac{k-1}{k}\\delta_{k}^{2}\\label{eq:rec2c} \\end{equation} This can be also rewritten as \\[ S_{2,k}=S_{2,k-1}+(x_{k}-m_{k})(x_{k}-m_{k-1}) \\] 5. Higher central sums 5.1. Windowed calculations for $p=3$ Similar to (\\ref{eq:S2w}): \\begin{eqnarray} S_{3,k}(n) & = & \\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k}(n)\\right]^{3}=\\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k-1}(n)-\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right]^{3}\\nonumber \\\\ & = & \\sum_{i=k-n+1}^{k}\\left\\{ \\begin{array}{c} \\left[x_{i}-m_{k-1}(n)\\right]^{3}-3\\left[x_{i}-m_{k-1}(n)\\right]^{2}\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\\\ +3\\left[x_{i}-m_{k-1}(n)\\right]\\left(\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right)^{2}-\\left(\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right)^{3} \\end{array}\\right\\} \\label{eq:S3w} \\end{eqnarray} or \\begin{equation} S_{3,k}(n)=S_{3,k-1}(n)+\\delta_{k}^{3}-\\delta_{k-n}^{3}-\\frac{3}{n}(\\delta_{k}-\\delta_{k-n})[S_{2,k-1}(n)+\\delta_{k}^{2}-\\delta_{k-n}^{2}]+\\frac{2}{n^{2}}(\\delta_{k}-\\delta_{k-n})^{3}\\label{eq:S3wrec} \\end{equation} 5.2. Cumulative calculations for $p=3$ Using the same trick and applying the condition (\\ref{eq:w2c}), we get: \\begin{equation} S_{3,k}=S_{3,k-1}+\\delta_{k}^{3}-\\frac{3}{k}\\delta_{k}[S_{2,k-1}+\\delta_{k}^{2}]+\\frac{2}{k^{2}}\\delta_{k}^{3} \\end{equation} or \\begin{equation} S_{3,k}=S_{3,k-1}-\\frac{3}{k}S_{2,k-1}\\delta_{k}+\\frac{(k-1)(k-2)}{k^{2}}\\delta_{k}^{3} \\end{equation} 5.3. Windowed calculations for $p=4$ Similar to (\\ref{eq:S2w}) and (\\ref{eq:S3w}): \\begin{eqnarray} S_{4,k}(n) & = & \\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k}(n)\\right]^{4}=\\sum_{i=k-n+1}^{k}\\left[x_{i}-m_{k-1}(n)-\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right]^{4}\\nonumber \\\\ & = & \\sum_{i=k-n+1}^{k}\\left\\{ \\begin{array}{c} \\left[x_{i}-m_{k-1}(n)\\right]^{3}-4\\left[x_{i}-m_{k-1}(n)\\right]^{3}\\frac{\\delta_{k}-\\delta_{k-n}}{n}+\\left(\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right)^{4}\\\\ +6\\left[x_{i}-m_{k-1}(n)\\right]^{2}\\left(\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right)^{2}-4\\left[x_{i}-m_{k-1}(n)\\right]\\left(\\frac{\\delta_{k}-\\delta_{k-n}}{n}\\right)^{3} \\end{array}\\right\\} \\end{eqnarray} or \\begin{eqnarray} S_{4,k}(n) & = & S_{4,k-1}(n)+\\delta_{k}^{4}-\\delta_{k-n}^{4}-\\frac{4}{n}(\\delta_{k}-\\delta_{k-n})[S_{3,k-1}(n)+\\delta_{k}^{3}-\\delta_{k-n}^{3}]\\nonumber \\\\ & + & \\frac{6}{n^{2}}(\\delta_{k}-\\delta_{k-n})^{2}[S_{2,k-1}(n)+\\delta_{k}^{2}-\\delta_{k-n}^{2}]-\\frac{3}{n^{3}}(\\delta_{k}-\\delta_{k-n})^{4} \\end{eqnarray} 5.4. Cumulative calculations for $p=4$ Using the same trick and applying the condition (\\ref{eq:w2c}), we get: \\begin{equation} S_{4,k}=S_{4,k-1}+\\delta_{k}^{4}-\\frac{4}{k}\\delta_{k}[S_{3,k-1}+\\delta_{k}^{3}]+\\frac{6}{k^{2}}\\delta_{k}^{2}[S_{2,k-1}+\\delta_{k}^{2}]-\\frac{3}{k^{3}}\\delta_{k}^{4} \\end{equation} or \\begin{equation} S_{4,k}=S_{4,k-1}-\\frac{4}{k}S_{3,k-1}\\delta_{k}+\\frac{6}{k^{2}}S_{2,k-1}\\delta_{k}^{2}+\\frac{(k-1)(k^{2}-3k+3)}{k^{2}}\\delta_{k}^{4} \\end{equation} 6. Covariance The above approach can be generalized for the calculation of the covariance matrix elements. 6.1. Definition The covariance of the components $x_{\\mu}$ and $x_{\\nu}$ is defined as \\begin{equation} c_{\\mu\\nu,k}(n)=\\frac{1}{n}\\sum_{i=k-n+1}^{k}\\left[x_{\\mu,i}-m_{\\mu,k}(n)\\right]\\left[x_{\\nu,i}-m_{\\nu,k}(n)\\right] = \\frac{1}{n}C_{\\mu\\nu,k}(n)\\label{eq:covw} \\end{equation} Here $m_{\\mu,k}(n)$ is the mean value for the $\\mu$-th component. It obeys the generalized equation (\\ref{eq:recc}): \\begin{equation} m_{\\mu,k}(n)=m_{\\mu,k-1}(n)+\\frac{\\delta_{\\mu,k}-\\delta_{\\mu,k-n}}{n}\\label{eq:mmunuk} \\end{equation} with Eqs. (\\ref{eq:dk}) and (\\ref{eq:dkn}) modified accordingly. 6.2. Windowed calculations Using the recurrence (\\ref{eq:mmunuk}) for each component we have \\begin{equation} C_{\\mu\\nu,k}(n)=\\sum_{i=k-n+1}^{k}\\left[x_{\\mu,i}-m_{\\mu,k-1}(n)-\\frac{\\delta_{\\mu,k}-\\delta_{\\mu,k-n}}{n}\\right]\\left[x_{\\nu,i}-m_{\\nu,k-1}(n)-\\frac{\\delta_{\\nu,k}-\\delta_{\\nu,k-n}}{n}\\right] \\end{equation} The above equation can be rewritten as \\begin{equation} C_{\\mu\\nu,k}(n)=\\sum_{i=k-n+1}^{k}\\left\\{ \\begin{array}{c} \\left[x_{\\mu,i}-m_{\\mu,k-1}(n)\\right]\\left[x_{\\nu,i}-m_{\\nu,k-1}(n)\\right]-\\left[x_{\\mu,i}-m_{\\mu,k-1}(n)\\right]\\frac{\\delta_{\\nu,k}-\\delta_{\\nu,k-n}}{n}\\\\ \\frac{\\delta_{\\mu,k}-\\delta_{\\mu,k-n}}{n}\\left[x_{\\nu,i}-m_{\\nu,k-1}(n)\\right]+\\frac{\\delta_{\\mu,k}-\\delta_{\\mu,k-n}}{n}\\,\\frac{\\delta_{\\nu,k}-\\delta_{\\nu,k-n}}{n} \\end{array}\\right\\} \\end{equation} or \\begin{equation} C_{\\mu\\nu,k}(n)=C_{\\mu\\nu,k-1}(n)+\\delta_{\\mu,k}\\delta_{\\nu,k}-\\delta_{\\mu,k-n}\\delta_{\\nu,k-n}-\\frac{(\\delta_{\\mu,k}-\\delta_{\\mu,k-n})(\\delta_{\\nu,k}-\\delta_{\\nu,k-n})}{n}\\label{eq:Cmunuk} \\end{equation} This gives the recurrence formula for the windowed online covariance calculations. 6.3. Cumulative calculations Applying the conditions (\\ref{eq:w2c}) to each component, we get: \\begin{eqnarray} C_{\\mu\\nu,k} & = & C_{\\mu\\nu,k-1}+\\delta_{\\mu,k}\\delta_{\\nu,k}-\\frac{\\delta_{\\mu,k}\\delta_{\\nu,k}}{k}\\nonumber \\\\ & = & C_{\\mu\\nu,k-1}+\\delta_{\\mu,k}\\delta_{\\nu,k}-\\frac{k-1}{k}\\delta_{\\mu,k}\\delta_{\\nu,k} \\end{eqnarray} This gives the recurrence formula for the cumulative online covariance calculations. Note that for $\\mu=\\nu$ the above equations reduce to the variance formulas. 7. Implementation My Python implementation can be found on GitHub: https://github.com/dr-nikolai/online_stat. It also includes the Jupyter notebook with a test/demo. Note that this code is not fully optimized. It is primarily an illustration to this article. Acknowledgement I am grateful to Marko Draisma who pointed to a typo in Eq. 18. Suggested citation for this article: Nikolai Shokhirev, 2013. Single-Pass Online Statistical Algorithms, http://www.numericalexpert.com/articles/single_pass_stat References Chan, T. F.; Golub, G. H. and LeVeque, R. J., Updating formulae and a pairwise algorithm for computing sample variances, COMPSTAT 1982 5th Symposium held at Toulouse 1982, 1982, 30-412 Chan, T. F.; Golub, G. H. and LeVeque, R. J., Algorithms for computing the sample variance: Analysis and recommendations The American Statistician, Taylor & Francis Group, 1983, 37, 242-247 Datar, M.; Gionis, A.; Indyk, P. and Motwani, R., Maintaining stream statistics over sliding windows, SIAM Journal on Computing, SIAM, 2002, 31, 1794-1813 Zivot, E. and Wang, J., Rolling Analysis of Time Series, Modeling Financial Time Series with S-Plus, Springer, 2003, 299-346 P\u00e9bay, P., Formulas for robust, one-pass parallel computation of covariances and arbitrary-order statistical moments. Sandia National Laboratories, 2008 Alexander, C., Moving Average Models for Volatility and Correlation, and Covariance Matrices, in Handbook of Finance, John Wiley & Sons, Inc., 2008, 2-14 Finch, T., Incremental calculation of weighted mean and variance, University of Cambridge, 2009, 4, 11-5 Bennett, J.; Grout, R.; P\u00e9bay, P.; Roe, D. and Thompson, D., Numerically stable, single-pass, parallel statistics algorithms, Cluster Computing and Workshops, 2009. CLUSTER'09. IEEE International Conference on, 2009, 1-8 Choi, M. and Sweetman, B., Efficient calculation of statistical moments for structural health monitoring. Structural Health Monitoring, SAGE Publications, 2010, 9, 13-24 McCrary, S., Implementing Algorithms to Measure Common Statistics, Available at SSRN 2695198, 2015 Wikipedia, Algorithms for calculating variance, 2016. Cook, J. D., Accurately computing running variance, 2008. Cook, J. D., Computing skewness and kurtosis in one pass, 2008. TurnerSR, Single-pass, parallel statistics algorithms for mean, variance, and standard deviation, 2014. Jenks, G., RunStats: Computing Statistics and Regression in One Pass, 2015. Babcock, B.; Datar, M.; Motwani, R. and O'Callaghan, L. Maintaining variance and k-medians",
    "comments": [],
    "description": "Single-Pass Online Statistics Algorithms",
    "document_uid": "136743ff0c",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962171",
    "title": "What's My Vote Worth?",
    "url": "https://whats-my-vote-worth.oliver-ernst.com",
    "score": 1,
    "timestamp": "2024-10-27T13:39:43",
    "source": "Hacker News",
    "content": "Biggest vote fraction*: 0.00 - Smallest vote fraction*: 0.00 -",
    "comments": [
      {
        "author": "beardyw",
        "text": "Is there a particular reason why it&#x27;s not just one vote per person?",
        "time": "2024-10-27T14:39:39"
      },
      {
        "author": "laptopdev",
        "text": "Purple is not on the chart. Suspicious.",
        "time": "2024-10-27T14:29:07"
      },
      {
        "author": "okish",
        "text": "Your state\u2019s voting power in the electoral college",
        "time": "2024-10-27T13:39:43"
      }
    ],
    "description": "Your state's voting power in the electoral college.",
    "document_uid": "8a0faadb87",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962168",
    "title": "Chrome Extension to Automate AWS Dialogs",
    "url": "https://github.com/kongdata/clickfirm",
    "score": 1,
    "timestamp": "2024-10-27T13:39:31",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [
      {
        "author": "steve_scorn",
        "text": "First time open sourcer and first time HN poster.<p>I&#x27;ve written a little (very little) Chrome Extension to speed up my own workflow when using AWS answering all the warning dialogs. I wanted to share it incase anyone else found it useful.<p>The gif on the readme shows it all - there&#x27;s nothing more.<p>Obviously sacrifices safeguards for speed, so if you don&#x27;t like that don&#x27;t use it :).",
        "time": "2024-10-27T13:39:31"
      }
    ],
    "description": "Automate those annoying AWS Console \"type to confirm\" inputs - kongdata/clickfirm",
    "document_uid": "35d232d440",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962167",
    "title": "LinkedIn Shoots for TikTok's Virality",
    "url": "https://english.elpais.com/technology/2024-10-27/linkedin-shoots-for-tiktoks-virality.html",
    "score": 1,
    "timestamp": "2024-10-27T13:39:11",
    "source": "Hacker News",
    "content": "The planet\u2019s biggest job bulletin board has pinned up a sign: content creators welcome. If a few years ago, LinkedIn saw itself as a place where professionals could showcase their resumes and promote their personal brand, that landscape has begun to change. Following in the steps of TikTok and Instagram, LinkedIn is turning into a social network where users can help their content go viral, upload short videos and even share memes. All, of course, in the context of their profession and without dramatically shifting the formal tone that has characterized the platform since it was founded in 2002 in the United States. LinkedIn, which has belonged to Microsoft since 2016, reports that in the last year alone, \u201ccreator mode\u201d accounts have risen from 13 to 16 million worldwide.\u201cLinkedIn is no longer the buttoned-up network for job seeking; people and brands are now leading with more human messages to introduce themselves,\u201d says Beatriz Torres, who has led a lengthy career as an online publicist. She explains that the pandemic and working from home have compelled users to share the more emotional side of their jobs. That has opened the doors to new creators, who are finding communities through posts related to corporate wellbeing, personal finances, mediation or simply, advice on getting work and increasing sales.Such is the case of Mar\u00eda Begue, 28, who until 2022 was just another LinkedIn user. Her role on the social network began to change when she decided to post daily recommendations meant to help her contacts to build their personal brand. With messages like \u201cthe resume is a thing of the past\u201d and \u201cwrite something on the internet every day,\u201d she began to build a community that today includes more than 90,000 followers. \u201cNow I can make a living from this without needing to work another job,\u201d she says. Her earnings come from selling personalized advice and access to pre-recorded courses that she offers through her LinkedIn account. She estimates that, depending on the month, a creator can earn between $539 to $18,313.This phenomenon, according to experts, is in part the consequence of the arrival of a younger public to the platform. \u201cThe generation that grew up using Instagram and TikTok is entering the labor market and naturally engages in content creation,\u201d says Brendan Gahan, a specialist in marketing through LinkedIn and the director of an agency of influencers on the social network. Gahan says that the platform is facilitating this approach with a series of tools like lives, short vertical videos, newsletters and podcasts, which have already achieved a fair amount of online success.Mar\u00eda Begue, LinkedIn content creator.LinkedIn Spain\u2019s Virginia Collera, who is in charge of its editorial section, breaks down the success its video format is seeing, to the tune of a 45% growth in views in 2024. She says that the platform\u2019s team is inviting content creators to trainings so that they can take better advantage of the network\u2019s tools, and that these trainings even extend to the communications teams of large Spanish companies. \u201cWe also want CEOs to make better moves on this platform,\u201d she says. She believes this is one of the factors that is attracting new audiences: \u201cThe social network is increasingly being used by decision-makers at companies, which is an invitation to users to make themselves known.\u201dJorge Branger, 28, is the founder of an agency that specializes in Linkedin influencers. He agrees that large companies are becoming more interested in their presence on the platform. \u201cInvestment funds and firms like Oracle and Nvidia don\u2019t want to be on TikTok because that\u2019s not where their target audience is, but on LinkedIn, they\u2019re found the ideal territory for connecting with other companies,\u201d he says. Branger and Gahan agree that the money that brands are open to paying for marketing campaigns on the platform is well above that of other networks like Instagram. \u201cIf in 2020, a company was offering $1,077 per post, today the number could be double,\u201d says Branger, who has worked with brands like Santander and Vodafone.\u201cLately, the B2B (company to company) model is moving more money worldwide, and LinkedIn is the platform that represents that,\u201d says Branger, who has one of the top five most popular accounts in Spain, with close to 250,000 followers. That factor is what is driving content creators to make the leap to the social network, he argues.\u201cLinkedIn was getting old and users were leaving,\u201d opines Torres, who thinks that these changes arrived at just the right time. The platform has seen an increase in its user base, reaching the one billion mark in January, a number that only Meta platforms and TikTok had previously achieved. In Spain, it has around 18 million accounts, according to the company, which says that the number of posts has also shot up by 41% between 2021 and 2023.Change to the algorithmGahan says that the platform has also modified its algorithm to favor content creators, in contrast to companies like TikTok, which reward \u201clikes\u201d and views over more genuine forms of interaction (like shares and comments.) That strategy has been employed in the hopes of affording more visibility to those who are creating content specifically for LinkedIn. He says that while on TikTok, there are 40,000 accounts with more than a million followers, on LinkedIn that number stands at 80, worldwide.Begue, who recently participated in a LinkedIn training for content creators, says that the rise of influencers on the platform is providing more work opportunities to copywriters, who write posts and newsletters from the accounts of others. In her case, she does her own copy. She admits that at first, she could spend several hours preparing her daily posts, but as time has passed, she\u2019s gotten the system down, and dedicates on average 90 minutes to each publication.In addition to recent trends, the network did already have its own influencers, the so-called \u201ctop voices\u201d who were already benefiting from high visibility on LinkedIn. Santiago \u00cd\u00f1iguez was among the first to receive that distinction in Spain. In his",
    "comments": [],
    "description": "The workplace social network is shifting its focus to content creators, who are pouring in, thanks to the presence of large companies and the opportunities the platform offers to gain industry influence ",
    "document_uid": "a0c251aa3a",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962150",
    "title": "Game AI Course",
    "url": "https://www.alanzucconi.com/courses/game-ai/",
    "score": 1,
    "timestamp": "2024-10-27T13:37:06",
    "source": "Hacker News",
    "content": "Course Description \u201cGame AI\u201d is an online course designed for game developers and programmers who are keen to learn how Artificial Intelligence is applied in videogames. The course focuses on classical AI, covering popular techniques such as Behaviour Trees, Pathfinding, Goal-Oriented Action Planning, and Evolutionary Computation. The aim of this course is not just to teach how to use those techniques, but to understand how and why they work in the first place. Many online tutorials, in fact, can get you started quickly, but without a solid theoretical foundation, they fail to teach transferrable skills and to develop a solid understanding of Game AI at a professional level. VIDEO While Neural Networks are also covered in details, this course does not include Generative AI (such as Midjourney and ChatGPT). Classical AI techniques are, in fact, the dominant way to automate decision-making in modern videogames, as they offer full control to the developers. Course Content The course counts overs 20 hours of content, divided between 12 chapters, focusing on as many topics and techniques. \u201cGame AI\u201d is designed like a buffet: each chapter is self-contained, so that you can skip the ones that are not relevant to you, and focus on what you really need. \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Lecture \ud83d\udd22 Maths \ud83d\udda5\ufe0f Coding \ud83d\udce6 Unity package \ud83d\udcc4 Documentation \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Defining Artificial Intelligence \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb PONG: Reactive & Predictive AI \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Pac-Man: Intelligence vs Personality \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb History of Game AI \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb AI Before Computers \ud83d\udd04 03 \u25cf Finite State Machines \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Pac-Man\u2019s Ghost AI \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Extensions \ud83d\udda5\ufe0f Implementation \ud83d\udce6 Finite State Machine Library \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Structure \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Extensions \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Games \ud83d\udda5\ufe0f Implementation \ud83d\udce6 Behaviour Tree Library \ud83d\udcc4 Behaviour Tree Library Documentation \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Path to Pathfinding \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Breath-First Search \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Depth-First Search \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Dijkstra\u2019s Algorithm \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb A* \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb From Pathfinding to AI \ud83d\udda5\ufe0f Implementation \ud83d\udce6 Pathfinding Library \ud83d\udcc4 Pathfinding Library Documentation \ud83d\ude97 06 \u25cf Steering Behaviours \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udd22 Linear Algebra \ud83d\udd22 Kinematics \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Seek & Flee \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Arrival & Leave \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Other Steering Behaviours \ud83d\udce6 Steering Behaviours Library \ud83d\udc26 07 \u25cf Emergent Behaviours \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Flocking Behaviours \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Collision Avoidance \ud83d\udda5\ufe0f Particle Life \ud83d\udda5\ufe0f Cellular Automata \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Utility Systems \ud83d\udd22 Expected Utility \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Action Selection \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Utility Curves \ud83d\udda5\ufe0f Chess AI \ud83d\udda5\ufe0f Tetris AI \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb From Pathfinding to Planning \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Goal-Oriented Action Planning \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb STRIPS: Stanford Research Institute Problem Solver \ud83d\udda5\ufe0f Implementation \ud83d\udce6 GOAP Library \ud83d\udcc4 GOAP Library Documentation \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Minimax \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Alpha-Beta Pruning \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Multi-Armed Banding Problem \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Monte Carlo Tree Search \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Go AI \ud83d\udda5\ufe0f Implementation \ud83d\udce6 Monte Carlo Tree Search Library \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Content Warning \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Evolution by Natural Selection \ud83d\udda5\ufe0f Evolutionary Algorithms \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Genetic Algorithms \ud83d\udce6 Evolution Library \ud83d\udcc4 Evolution Library Documentation \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Introduction \ud83d\udd22 Regression Analysis \ud83d\udd22 Linear Regression \ud83d\udd22 Non-Linear Regression \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Universal Approximation Theorem \ud83d\udd22 Backpropagation \ud83d\udce6 Neural Network Library \ud83d\udcc4 Neural Network Library Documentation All students also have access to a large number of C# libraries, designed to perform most of the AI techniques covered in the course. They are fully compatible with Unity, and can be used for free in your games and projects. Course Prerequisites This course is designed for game developers and programmers, but there are no formal prerequisites. A basic knowledge of coding is recommended, but all the programming bits are self-contained and can be skipped. Likewise, the most mathematical parts are optional, but available to the students who feel ready for an extra challenge. All resources for the course are developed in C# and Unity, but you do not need to be proficient in either. Moreover, most the theoretical content is language-agnostic, so that you can apply your knowledge to any other game engine or programming language. Assignments The course includes two practical assignments, which challenge students to create AIs for popular games, such as Snake and Tetris. \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Snake Assignment \ud83d\udcdd Assignment Brief \ud83d\udce6 Assignment package \ud83e\uddf1 CW2 \u25cf Tetris Assignment \ud83d\udcdd Assignment Brief \ud83d\udce6 Assignment package Although all assignments are done in Unity and C#, you do not need to be proficient in either, thanks to the use of very expressive APIs.",
    "comments": [],
    "description": "The ultimate course on Classical AI in modern videogames, covering everything you need to know, from Behaviour Trees to Neural Networks.",
    "document_uid": "3b3bf513d5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962146",
    "title": "GPM J1839\u221210",
    "url": "https://en.wikipedia.org/wiki/GPM_J1839%E2%88%9210",
    "score": 3,
    "timestamp": "2024-10-27T13:36:23",
    "source": "Hacker News",
    "content": "From Wikipedia, the free encyclopedia Neutron star in the Scutum constellation GPM J1839\u221210[1] is a potentially unique[2] ultra-long period magnetar[3][4] located about 15,000 light-years away from Earth in the Scutum constellation, in the Milky Way. It was discovered by a team of scientists at Curtin University using the Murchison Widefield Array.[5][6] Its unusual characteristics violate current theory and prompted a search of other radio telescope archives, including the Giant Metrewave Radio Telescope and the Very Large Array, which revealed evidence of the object dating back to 1988.[5] The signature of the object went unnoticed because scientists did not know to look for its unusual behavior.[5] The current understanding of neutron stars is that below a certain rate of rotation, called \"the death line\", they cease emissions. Uniquely, not only does GPM J1839\u221210 have an extremely slow rotation of approximately twenty-two minutes, it emits bursts of radio waves lasting up to five minutes, for which there is currently no generally accepted explanation.[5][4][6][7][8]",
    "comments": [],
    "description": "No description available.",
    "document_uid": "d3ebf64b12",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962100",
    "title": "The twins who created their own language",
    "url": "https://www.bbc.com/future/article/20241025-do-twins-have-a-secret-language",
    "score": 1,
    "timestamp": "2024-10-27T13:31:03",
    "source": "Hacker News",
    "content": "'It's a unique language spoken by two people': The twins who created their own languageMatthew and Michael Youlden/ Superpolyglotbros(Credit: Matthew and Michael Youlden/ Superpolyglotbros)Up to 50% of twins develop their own communication pattern with one another. Most lose it over time, but for the Youlden twins it has become a normal way of communicating.Twins Matthew and Michael Youlden speak 25 languages each. The 26th is Umeri, which they don't include in their tally. If you've not heard of Umeri, there's good reason for that. Michael and Matthew are the only two people who speak, read and write it, having created it themselves as children.The brothers insist Umeri isn't an intentionally secret language.\"Umeri isn't ever reduced to a language used to keep things private,\" they say in an email. \"It definitely has a very sentimental value to us, as it reflects the deep bond we share as identical twins.\"An estimated 30-50% of twins develop a shared language or particular communication pattern that is only comprehensible to them, known as cryptophasia. The term translates directly from Greek as secret speech.Nancy Segal, director of the Twin Studies Center at California State University, believes there are now better and more nuanced words for the phenomenon, and prefers to use \"private speech\". In her book Twin Mythconceptions, Segal also uses the phrase \"shared verbal understanding\" to refer to speech used within the pair.\"Based on available studies, it is safe to say that about 40% of twin toddlers engage in some form of 'twin-speak',\" writes Segal. \"But that figure does not convey just how complex twins' language development turns out to be.\"Umeri is now written using the Latin alphabet, though the Youlden twins tried to design their own alphabet for the languageRoy Johannink from the Netherlands is father to teenage twins Merle and Stijn. Thirteen years ago, when they were babies, he took a video of them babbling to one another and shared it on YouTube. To date, their conversation has had over 30 million views. Johannink happened to have his camera on hand at the moment the two first began to verbally interact with each other.\"I was a little surprised that they saw each other,\" remembers Johannink. \"They thought: 'Hey, I'm not alone in this moment. There's another one of me! It's us against the world.'\"Segal explains that like Merle and Stijn (who went on to lose their shared language when they learnt Dutch), most twins outgrow their private words as they gain more exposure to other people beyond the home.But for the Youlden twins, this wasn't the case. They didn't outgrow their language. Quite the contrary, they enriched and perfected it over the years.Matthew and Michael Youlden/ SuperpolyglotbrosTwins Matthew and Michael Youlden developed their own language as children, which they speak to this day (Credit: Matthew and Michael Youlden/ Superpolyglotbros)Born and raised in Manchester in the UK, the Youlden twins grew up surrounded by different ethnicities and cultures, fostering a love of languages.Memories of when Umeri first began are hazy, but the brothers remember their grandfather being confused when as pre-schoolers, the two would share a joke between themselves he would not understand.Then came their first family holiday abroad, at the age of eight. They were headed to Spain and decided they were going to learn Spanish, convinced that if they didn't, they'd struggle to order ice cream. Armed with a dictionary and with little understanding of how the grammar worked, they began to translate phrases word for word from English into Spanish. Later they took on Italian, and then turned their attention to learning Scandinavian languages. Pooling together various grammatical elements of all the languages they had studied, the brothers realised Umeri could actually become a fully-fledged language itself.This chimes with Segal's observations. According to her, in general, \"twins do not invent a new language, they tend to produce atypical forms of the language they are exposed to. Even though it's unintelligible, they still direct it to other people\".The Youlden twins began to standardise and codify Umeri. At one point, they even tried to design their own alphabet but realised (when they got their first computer) it would be of little use considering there was no Umeri font. Umeri is now written using the Latin alphabet.Shared languagePreserving a language spoken by few people comes with its own challenges, however.\"Twins have this shared language, that at some point they stop using, as if they feel ashamed of it,\" says Matthew. \"This is also not something unique to twin languages.\"Anyone speaking a minority language \u2013 meaning a language not shared by much of the rest of society \u2013 may grow shy of speaking it, \"especially if you are raised with a minority language where you are maybe ostracised or looked at funnily at school,\" he says. \"We thankfully never had that [reaction from others].\" On the contrary, in the Youlden home, their parents never saw the development of Umeri between the brothers as a negative thing.LET'S TALKLet's Talk is a BBC series exploring the wonder and mystery of languages.When the brothers would swerve off to converse in their own language when with extended family, the response tended to be \"they're off doing the language thing again\", recalls Matthew.Karen Thorpe is a specialist in child development, education and care research at the Queensland Brain Institute at the University of Queensland. She has in previous roles extensively studied language development in twins.\"For me, it's about a very close relationship,\" she says. \"Rather than seeing it as something strange and unusual, private language is really about a beautiful thing that humans do when they're very, very close to one another. But is that exclusive to twins? I don't think so. I think it's exclusive to very special, close relationships.\"She also regards it as a normal development feature. As she put it in a 2010 research paper: \"It is simply that young children who are just beginning to speak tend to understand each other rather better than do their parents or other adults.\"For others, such as the Youldens, the",
    "comments": [],
    "description": "Up to 50% of twins develop their own communication pattern with one another. Most lose it over time, but for the Youlden twins it has become a normal way of communicating.",
    "document_uid": "c95147bb70",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962093",
    "title": "The British Sleep Society Position Statement on Daylight Saving Time in the UK",
    "url": "https://onlinelibrary.wiley.com/doi/10.1111/jsr.14352",
    "score": 4,
    "timestamp": "2024-10-27T13:30:32",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "4ad1729b95",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962059",
    "title": "Worth Doing Poorly; How I Failed to Run an Event (and What I Learned)",
    "url": "https://bendauphinee.com/writing/2024/10/26/worth-doing-poorly-how-i-failed-to-run-an-event-and-what-i-learned/",
    "score": 3,
    "timestamp": "2024-10-27T13:26:20",
    "source": "Hacker News",
    "content": "30 min reading time; ~6300 words Anything worth doing is worth doing poorly\u2026 because doing it poorly is better than not doing it. \u2014 redheadhatchet This quote is one I read on Tumblr a few years ago, immediately printed out, and taped to my wall (imperfectly of course, as you can see from the image to the right). It\u2019s based on a quote by G. K. Chesterton, a philosopher from the early 20th century, who said \u201cAnything worth doing is worth doing badly.\u201d Honestly, I like the updated version better. Finding the quote finally put words to a driving force in my life. For me, the core message is about consistency and simply starting. To tackle important goals, you need the willingness to jump in and figure things out as you go. Take risks according to your tolerance of course, but don\u2019t be afraid to start. I feel this has helped me grow, and I\u2019ve learned to trust myself more. It\u2019s part of the reason that I jumped into running this event as well. Last October, on a long drive home, I had the idea to start a Renaissance Faire style event. The idea felt exciting, and a little overwhelming at first, but having this mindset anchored me, making the risk seem worth taking. As an engineer and first-time event planner, I knew the learning curve would be steep but rewarding. After thinking it over, I seized on an idea I thought workable in the budget and time I had available, a Viking-themed feast. And thus began an almost year-long journey of trying to pull an event out of thin air. Attention: This post is going to be a long one. If you want to get to the lessons, here\u2019s a link to jump you directly to them. So How To Get Started? First, I contacted a friend of mine, a chef, and sat down with him over supper to talk about the idea. Since the whole idea at the time was centered around a Viking themed feast in the woods, there were going to be a lot of logistics to pull off, the core of it being cooking for a potential audience of 120 people, and being able to serve it at a remote location. I was lucky, because he was on board immediately, and with his credentials and experience, I was confident in his ability to pull off the best food we could within those constraints. Plus, his enthusiasm for the idea was immediately apparent, because it gave him a new creative outlet (the answer was very much \u201cVikings?! Absolutely yes!\u201d). So, with that buy in, I started thinking very seriously about the details of the event. We worked out that we could likely accommodate 120 guests, with a rough budget of $45 per seat in wholesale food costs, between him and another chef. This idea that was very hand-wave suddenly crystalized as something that was potentially within reach. During our conversation, it became apparent to me that I also needed to have something to make the event more than just eating themed meals in the woods. I contacted another friend, who runs a local roleplay group, and asked him if his group might be interested in playing the part of Vikings in an event like this, to which I got another yes. The First Draft Sitting down with a spreadsheet (and Google, and reddit), I started plotting out the logistics I thought I would need for the event, including some very low quality cost estimates. The first draft of this idea was: Costumed actors who would be mixed into the guest tables, providing the Viking atmosphere, telling stories, and otherwise being Vikings. A musician to provide appropriate music through supper (and potentially after). A Viking feast, with a menu of dishes that either were period accurate or paid homage to food (and drink) of that era and culture. Axe throwing, because what Viking event doesn\u2019t have axes! With some rapidly developing research and planning, this budget changed a lot very early, and along with the idea, started filling out. It\u2019s amazing how much goes into even a simple event logistically. By mid-Nov, I had developed a backstory for the event, filled out a lot of the initial costs for the logistics, and the plan was starting to take shape. Draft budget, Nov 18, 2023 A rough drawing of the potential event site. We had several volunteers (including a seamstress), and were starting to have regular meetings to work out various ideas for the event itself. I own several acres of land, so I was trying to work out the logistics for getting that many people down a dirt road to it. As well, since the land was raw, I was now scouting for a site to clear and hold the event (you can see more details of the surveying work I did for this in my article about surveying my land). It was becoming apparent as I worked through planning that there needed to be more to the event. Food, music, but what else? While I was trying to work out the event details, the tentative budget was getting more detailed, and large. Oh and also by the end of December, I owned 42 web domains related to the event. I won\u2019t reveal the best one that I got yet, but needless to say, I was very surprised it was still available! Goal Setting By this point, my expectations for what I wanted the result to be were finally set. My goal wasn\u2019t profit but rather learning\u2014success meant understanding what worked and what didn\u2019t. While I had plenty of support from some of my friends, and some of the small business people I talked with, some people later told me they couldn\u2019t understand my goal, and even one that very specifically said that there was no way they could be motivated to volunteer to help with the event because my goal was completely counter to how they",
    "comments": [],
    "description": "No description available.",
    "document_uid": "08f5e86970",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962056",
    "title": "A chain of Mars exploration catastrophes",
    "url": "https://english.elpais.com/science-tech/2024-10-27/a-chain-of-mars-exploration-catastrophes-when-nasa-confused-the-metric-with-the-imperial-system.html",
    "score": 1,
    "timestamp": "2024-10-27T13:25:37",
    "source": "Hacker News",
    "content": "The 1990s were a disastrous decade for Mars exploration projects. Out of seven attempts, only two were successful. And the apparent ease of Viking 1 and 2 \u2014 a pair of NASA landers and orbiters \u2014 hid a more difficult reality: landing on the red planet was much more difficult than it seemed. In the first 50 years of exploring Mars, almost half of the vehicles sent there crashed or stopped working.In September 1992, NASA launched its Mars Observer, a platform that was meant to continue and expand the studies carried out by the Viking orbiters. It was a newly-designed vehicle, the first of a class intended to make planetary visits not only to Mars, but also \u2014 in the future and with the necessary adaptations \u2014 to Venus, or even Mercury.It was built from a standard structure typical of conventional communications satellites. It seemed like a good idea from the point of view of taking advantage of already proven designs, but in the end, it wasn\u2019t. Some of its components \u2014 which had worked well for weeks around the Earth \u2014 wouldn\u2019t be able to withstand the rigors of a months-long trip to much colder environments.When the Mars Observer was only a couple of days away from reaching its target, it was given the order to pressurize its tanks in preparation for braking. It\u2019s unclear exactly what happened: suspicion points to a slight leak of oxidizer (nitrogen tetroxide) from a valve. Even though it was a small amount over the course of an 11-month-long flight, the corrosive liquid would have built up in the pipes. Hence, when a second set of valves opened, it could have come into contact with the fuel, causing an explosion. This is just one of several hypotheses, but the sudden failure of communications didn\u2019t allow for a definitive conclusion to be reached.The Mars Global Surveyor (MGS) \u2014 another probe launched four years later \u2014 had better luck. Following the failure of the Observer, the idea of using a type of spacecraft that would be of use to all missions had already been abandoned. This was a new design, specifically built for operating on Mars: the scientific instruments it carried were almost identical to those that had been lost in the previous attempt. The relative positions of Earth and Mars made the journey a long one: 11 months. The trip culminated with the vessel\u2019s entry into a heavily-elongated orbit, whose altitude would be reduced until it became circular at a level of only 142 miles above Mars. The adjustment took another year-and-a-half because, for the first time, solar panels (rather than chemical engines) were used as air brakes. There were also two steerable fins, strong enough to withstand repeated friction from the upper layers of the atmosphere.In the end, the Global Surveyor was placed in a sun-synchronous orbit, calculated in such a way that it passed over the same terrain feature at the same solar time. The lighting conditions were similar and the shadows \u2014 always identical \u2014 made it easy to detect changes in the landscape.Although projected with a useful life of only two years, the MGS received repeated mission extensions (that is, budget allocations), which kept it active for almost 10 years. This was longer than any other spacecraft sent to Mars until then. During that time, it obtained a quarter-of-a-million images, as well as detailed altimetric coverage of the planet. This information would prove to be of great help in preparing for future operations taken by the mobile robots that were about to be completed.By November 1996, the Soviet Union had ceased to exist. Mars 8 was intended to be the first deep-space probe to fly the Russian tricolor flag and was tasked with an ambitious scientific program. In addition to a platform equipped with video cameras and remote sensors, it carried two landing capsules and two penetrators. The latter were six-foot-long darts that would be launched from orbit to embed themselves in the ground of Mars. Just before impact, they would split into two sections: the nose cone would be buried deep, while the aft section \u2014 connected by a series of cables \u2014 would remain on the surface. The bow contained a seismometer, thermal meters and mineralogy analyzers, all of them capable of surviving a crash of up to 186 miles per hour; their measurements would reach the orbital vehicle through the transmitter installed in the rear segment of the probe.In the end, however, this plan couldn\u2019t be put into action. The rocket\u2019s final stage failed and the probe \u2014 after a day spent trapped in an incorrect orbit around the Earth \u2014 disintegrated upon re-entering the atmosphere.The series of failures continued, this time under the Japanese flag. Nozomi was a small vehicle, whose instruments had been co-designed by Japan and four foreign space agencies. Its objective \u2014 like that of its predecessors \u2014 was also to orbit around Mars so as to analyze its terrain, atmosphere and neighboring interplanetary environment.The most novel thing was Nozomi\u2019s trajectory. To reach escape velocity without much fuel expenditure, it was made to pass close to the Moon twice and once near the Earth itself, accelerating more thanks to its gravitational pull. The maneuver took six months of navigation, but it was successful. In late-December of 1998 \u2014 with a little help from its engine \u2014 it set off for Mars.Unfortunately, another poorly-closed valve would cause a loss of valuable fuel needed for the final course adjustments. The Japanese technicians were forced to recalculate the trajectory to accelerate the deep space probe without wasting propellant. It passed in front of the Earth twice more, but each maneuver meant going around the Sun again. This resulted in four more years of travel. An odyssey.A solar flare ended up damaging the communications equipment and the heater control. The remaining hydrazine in the supply lines froze. Only by carefully managing the ship\u2019s orientation to take advantage of the Sun\u2019s heat could the technicians \u2014 monitoring the spacecraft",
    "comments": [],
    "description": "In this excerpt from his book, journalist Rafael Clemente recounts the amazing history of space exploration  ",
    "document_uid": "d54169bec5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962051",
    "title": "WASM Debugging with Emscripten and VSCode",
    "url": "https://floooh.github.io/2023/11/11/emscripten-ide.html",
    "score": 1,
    "timestamp": "2024-10-27T13:25:06",
    "source": "Hacker News",
    "content": "TL;DR: glueing together VSCode, Cmake and the Emscripten SDK to enable an IDE-like workflow (including debugging). 09-Oct-2024: updated This is written from the perspective of a UNIX-like OS (macOS or Linux), but should also work on Windows with some minor tweaks. Prerequisites First make sure that the following tools are in the path: You\u2019ll also need VSCode and Chrome installed. On macOS I\u2019d recommend using Homebrew and on Windows Scoop to install those. On Linux of course, your system\u2019s standard package manager. Emscripten Hello World Let\u2019s start from scratch. On the command line: mkdir hello cd hello git init Add a .gitignore file: .gitignore Install the Emscripten SDK, we\u2019ll do so in a way that it doesn\u2019t leave a trace on your system when deleted so don\u2019t worry. Still inside the hello directory: git clone --depth=1 https://github.com/emscripten-core/emsdk cd emsdk ./emsdk install latest ./emsdk activate --embedded latest cd .. Don\u2019t forget the ./emsdk activate --embedded latest step! (happens to me all the time) \u2026let\u2019s check if that worked. Create a hello.c source file in the hello project directory: hello.c #include <stdio.h> int main() { printf(\"Hello World!\\n\"); return 0; } \u2026compile that into a .wasm/.js pair runnable with node.js: emsdk/upstream/emscripten/emcc hello.c -o hello.js \u2026there should be a hello.js and hello.wasm file now: ls emsdk hello.c hello.js hello.wasm \u2026run the hello.js file via node.js (depending on the emsdk version the path may differ): emsdk/node/18.20.3_64bit/bin/node hello.js \u2026you should see a Hello World! printed to the terminal. Delete the compiler output, we don\u2019t need that anymore: CMake + Emscripten Let\u2019s bake the build process into a cmake file. Create a CMakeLists.txt file in the hello project directory: CMakeLists.txt cmake_minimum_required(VERSION 3.21) project(hello) add_executable(hello hello.c) if (CMAKE_SYSTEM_NAME STREQUAL Emscripten) set(CMAKE_EXECUTABLE_SUFFIX .js) endif() \u2026and since this is a cross-compilation scenario, let\u2019s also create a CMakeUserPresets.json file. This simplifies calling cmake with the right arguments for cross-compilation, and will help us later when integrating with VSCode: CMakeUserPresets.json { \"version\": 3, \"cmakeMinimumRequired\": { \"major\": 3, \"minor\": 21, \"patch\": 0 }, \"configurePresets\": [ { \"name\": \"default\", \"displayName\": \"Emscripten\", \"binaryDir\": \"build\", \"generator\": \"Ninja Multi-Config\", \"toolchainFile\": \"emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake\" } ], \"buildPresets\": [ { \"name\": \"Debug\", \"configurePreset\": \"default\", \"configuration\": \"Debug\" }, { \"name\": \"Release\", \"configurePreset\": \"default\", \"configuration\": \"Release\" } ] } \u2026let\u2019s configure and build with cmake: cmake --preset default -B build cmake --build build --preset Debug \u2026and run with node.js: emsdk/node/18.20.3_64bit/bin/node build/Debug/hello.js \u2026this should again print Hello World!. VSCode + CMake + Emscripten Let\u2019s integrate what we have so far with VSCode! You\u2019ll need the following VSCode extensions: \u2026with those installed, start VSCode from within the hello project directory: You should see something like this, pay attention to the status bar at the bottom (underlined in red), these items are used to control the cmake build config and target: (NOTE 09-Oct-2024: the underlined items in the bottom bar have moved into the CMake Tools sidepanel in recent versions). Clicking those allows you to select a Configure- and Build-Preset, and a build target. Change those that it looks like this: Here we also encounter the first wart, the CMake Tools extension isn\u2019t able to communicate the correct Emscripten sysroot include paths over to the C/C++ extension. You\u2019ll see an error squiggle under the stdio.h include path: I haven\u2019t found a solution to this problem, it looks like a bug in the CMake Tools extension. Annoying for sure, but not a showstopper, because only Intellisense is affected, building should work fine. You can test that by pressing F7, or run the palette command CMake: Build. You should see something like this in the VSCode Output panel: [main] Building folder: hello [build] Starting build [proc] Executing command: /opt/homebrew/bin/cmake --build /Users/floh/scratch/hello/build --config Debug --target hello [build] [1/2] Building C object CMakeFiles/hello.dir/Debug/hello.c.o [build] [2/2] Linking C executable Debug/hello.js [driver] Build completed: 00:00:00.361 [build] Build finished with exit code Debugging \u2026next lets make debugging work! Create a launch.json file in the .vscode subdirectory: .vscode/launch.json { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Launch\", \"type\": \"node\", \"request\": \"launch\", \"program\": \"build/Debug/${command:cmake.launchTargetFilename}\", } ] } Pressing F5 should now work. You should see a Hello World! in the VSCode Debug Panel. But when trying to debug there\u2019s the next wart. Try to set a breakpoint in the C source code: Now hit F5. We\u2019d expect that the execution stops at the breakpoint, but that doesn\u2019t happen. This is a known issue in the DWARF debugging extension. From the documentation: Breakpoints in WebAssembly code are resolved asynchronously, so breakpoints hit early on in a program\u2019s lifecycle may be missed. There are plans to fix this in the future. If you\u2019re debugging in a browser, you can refresh the page for your breakpoint to be hit. If you\u2019re in Node.js, you can add an artificial delay, or set another breakpoint, after your WebAssembly module is loaded but before your desired breakpoint is hit. Hopefully this problem will be fixed soon-ish, since it\u2019s currently the most annoying. One workaround is to first set a breakpoint in the Javascript launch file at a point where the WASM blob has been loaded. Load the file build/Debug/hello.js into the editor, search the function callMain, and set a breakpoint there: Press F5 and execution should stop at that breakpoint. Now press F5 again and execution should stop in the C code\u2019s main() function (assuming that breakpoint is still set): Yay. This is how debugging works for a Node.js Emscripten application. Moving into the web browser Let\u2019s extend our hello.c to render something in WebGL2. Clone the sokol headers into the hello project directory and copy some headers up into the project: git clone --depth=1 https://github.com/floooh/sokol cp sokol/sokol_gfx.h sokol/sokol_app.h sokol/sokol_log.h sokol/sokol_glue.h . \u2026delete the sokol directory since we don\u2019t need it anymore: Replace the hello.c file with the following code which just clears the canvas with a dynamically changing color: hello.c #define SOKOL_IMPL #define SOKOL_GLES3 #include \"sokol_gfx.h\" #include \"sokol_app.h\" #include \"sokol_log.h\" #include \"sokol_glue.h\" static sg_pass_action pass_action; static void init(void) { sg_setup(&(sg_desc){ .environment = sglue_environment(), .logger.func = slog_func, }); pass_action = (sg_pass_action) { .colors[0] = { .load_action = SG_LOADACTION_CLEAR, .clear_value = { 1.0f,",
    "comments": [],
    "description": "TL;DR: glueing together VSCode, Cmake and the Emscripten SDK to enable an IDE-like workflow (including debugging).",
    "document_uid": "4ee4010a20",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962044",
    "title": "'Americans just work harder' than Europeans, says CEO of Norway's $1.6T oil fund",
    "url": "https://fortune.com/europe/article/how-many-hours-work-week-year-american-workers-ethic-norges-bank/",
    "score": 6,
    "timestamp": "2024-10-27T13:24:22",
    "source": "Hacker News",
    "content": "Norway\u2019s \u201ctrillion-dollar man\u201d believes America\u2019s attitude toward failure is helping propel the nation ahead of its European counterparts\u2014where workers may have a better work-life balance but aren\u2019t as ambitious. Norway\u2019s \u201ctrillion-dollar man\u201d believes America\u2019s attitude toward failure is helping propel the nation ahead of its European counterparts\u2014where workers may have a better work-life balance but aren\u2019t as ambitious. Nicolai Tangen leads Nordic behemoth Norges Bank Investment Management, which governs the revenue earned by Norway\u2019s oil and gas resources, with the aim of ensuring its benefits are distributed fairly between current and future Norwegian generations. Under Tangen\u2019s leadership since 2020, and over the past decade, the $1.6 trillion fund has invested more and more heavily in the U.S. instead of its closer neighbors in Europe\u2014and it\u2019s no coincidence. America\u2019s performance, particularly in innovation, is \u201cworrisome\u201d in contrast to Europe, Tangen told the Financial Times. Part of comes down to mindset, Tangen added, and how accepting each continent is of mistakes and risk: \u201cYou go bust in America, you get another chance. In Europe, you\u2019re dead,\u201d he said. But it goes deeper than that\u2014there\u2019s a difference in the \u201cgeneral level of ambition,\u201d he added. \u201cWe are not very ambitious. I should be careful about talking about work-life balance, but the Americans just work harder,\u201d Tangen continued. How many hours do Americans work each week? Data suggests that Tangen is right\u2014but only by a fine margin. According to the European Union, in 2022 the average workweek of people between the ages of 22 and 65 was 37.5 hours. The longest working weeks recorded were in Greece\u201441 hours a week\u2014and Poland\u201440.4 hours. By contrast, the Netherlands had the shortest working week of 33.2 hours, followed by Germany at 35.3 hours. Meanwhile, data from the International Labour Organization, last updated in January, showed the average hours workers clocked in the U.S. was 38 hours a week. However, of those employees, 13% worked 49 hours or more per week, which outstripped the majority of European nations. Moreover, countries like the U.K. have a statutory requirement entitling staff to 28 paid days of leave a year\u2014if you\u2019re a full-time employee. In the U.S. it is not a legal requirement for staff to be given any paid time off; however, according to the Bureau of Labor Statistics, the average employee who is in their first year of service takes eight PTO days. Are CEOs paid too much? Despite admiring the work ethic of staffers in the U.S., Tangen has made it clear he doesn\u2019t agree with the extreme pay packages handed to execs. Last year he told Fortune that CEOs who earn more than, say, $20 million a year, are \u201cenriching themselves on our behalf.\u201d \u201cIt\u2019s like daylight robbery,\u201d he added. Norges Bank\u2019s investment strategy has certainly leaned into the U.S. trend: After all, America is home to the Magnificent Seven, which have provided a backbone to the stock market boom and, according to analysts, will continue to do so. And the backing of an institution like Norges Bank Investment Management will encourage other investors to jump on board. The group is one of the most powerful financial vehicles on the planet: It is the world\u2019s largest single owner of global stock markets, controlling 1.5% of shares in the world\u2019s listed companies. Investing in the United States The organization also owns swaths of high-end property, including a 25% stake in London\u2019s Regent Street and an approximately 50% holding in offices in New York\u2019s Times Square and on Washington\u2019s Pennsylvania Avenue. Investments in the U.S. now represent 46.9% of Norges Bank\u2019s portfolio, whereas a decade ago the U.S. represented just under 30%. Going back a further 10 years, in 2003, the organization\u2019s investment in America made up just 26.3% of all investments. Conversely, in 2003, 59.5% of Norges Bank\u2019s portfolio was invested in European countries, a figure that, by 2023, had fallen to 28.7%. The 2024 election looms over business Of course, like many American investors, Tangen is closely watching the 2024 presidential elections, which could rock the investing boat. The CEO, who as a public servant earns less than $1 million a year, said there were people within the organization who were concerned about the upcoming race, but added: \u201cI probably shouldn\u2019t say too much about that. We just invest in America in great companies for the long term. It won\u2019t have any implications for how we allocate our capital. We have nearly half the assets in America; we will stay invested in America.\u201d Per the FT, Magnificent Seven stocks make up 12% of Norges Bank\u2019s equity holdings, with Tangen adding, there\u2019s \u201can argument for the big getting bigger, [and] the winner taking it all.\u201d There is, of course, a common thread among all the Magnificent Seven businesses\u2014and it\u2019s the current favorite phrase of Wall Street: artificial intelligence. Again, this is an area where, Tangen said, Europe was making life difficult for itself. Tech CEOs are frustrated, he said, by the amount of red tape in Europe compared with the U.S. Admittedly, even those who are leading the way with AI in the U.S. are asking for guardrails\u2014just ask OpenAI\u2019s Sam Altman and Tesla CEO Elon Musk. \u201cI\u2019m not saying it\u2019s good, but in America you have a lot of AI and no regulation; in Europe you have no AI and a lot of regulation. It\u2019s interesting,\u201d Tangen added.",
    "comments": [
      {
        "author": "nabla9",
        "text": "Mexicans work harder than Americans. People in Turkey and Greece work harder than North Europeans. The general rule of thumb is that people in poorer countries work harder, more hours, and less breaks.<p>The US is not poor, but there are poor country incentives. No automatic free healthcare, no free high-quality education for children. Large number of elderly people must work without pensions. You have to work harder and have less free time to get the same income safety compared to  Europeans.",
        "time": "2024-10-27T13:44:52"
      },
      {
        "author": "RadiozRadioz",
        "text": "I&#x27;d work harder too if I&#x27;d lose my healthcare after being fired. Americans also do more work in a year because they have hardly any vacation days; technically they&#x27;ve worked more hours.",
        "time": "2024-10-27T13:54:35"
      },
      {
        "author": "bdangubic",
        "text": "except that every European I know (and I know a lot of them) that lives &amp; works in the US works harder than any US employee at the same place of employment\u2026",
        "time": "2024-10-27T13:27:43"
      },
      {
        "author": "wmstack",
        "text": "Americans just work harder\u2019 than Europeans, says CEO of Norway\u2019s $1.6 trillion oil fund, because they have a higher \u2018general level of ambition\u2019",
        "time": "2024-10-27T13:24:22"
      },
      {
        "author": "WhereIsTheTruth",
        "text": "Tech contributors per capita (github):<p>European countries rank very high (USA only 14th)<p><a href=\"https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S0040162522000105\" rel=\"nofollow\">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S004016252...</a><p>How many employees does it take to run chess.com vs lichess.org ? who work harder? who is more efficient? That kind of business is bad for him, yet is great of value, perhaps there is a disconnect somewhere?",
        "time": "2024-10-27T14:09:58"
      }
    ],
    "description": "Norges Bank Investment Management, a $1.6 trillion oil revenue fund, is increasingly backing U.S. stocks like Tesla, Apple, Amazon, and more.",
    "document_uid": "96c1d36850",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962036",
    "title": "Decomposing the Dark Matter of Sparse Autoencoders",
    "url": "https://arxiv.org/abs/2410.14670",
    "score": 3,
    "timestamp": "2024-10-27T13:22:44",
    "source": "Hacker News",
    "content": "Happy Open Access Week from arXiv! Open access is only possible with YOUR support. Give to arXiv this week to help keep science open for all.",
    "comments": [],
    "description": "Abstract page for arXiv paper 2410.14670: Decomposing The Dark Matter of Sparse Autoencoders",
    "document_uid": "03c9cd45d9",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962033",
    "title": "Unlocking the Potential of Brain-Computer Interfaces",
    "url": "https://www.wsj.com/tech/ai/precision-neuroscience-brain-computer-technology-b4713d04",
    "score": 1,
    "timestamp": "2024-10-27T13:22:29",
    "source": "Hacker News",
    "content": "Skip to Main ContentSkip to Search",
    "comments": [
      {
        "author": "bookofjoe",
        "text": "<a href=\"https:&#x2F;&#x2F;archive.ph&#x2F;uEw08\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.ph&#x2F;uEw08</a>",
        "time": "2024-10-27T13:23:00"
      }
    ],
    "description": "We can\u2019t find the page you are looking for.",
    "document_uid": "fe61acda0a",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962016",
    "title": "Westinghouse's eVinci micro nuclear reactor for data centers",
    "url": "https://www.tomshardware.com/pc-components/power-supplies/westinghouses-evinci-micro-nuclear-reactor-for-data-centers-delivers-5-megawatts-of-power-for-eight-years-without-refueling-microreactors-could-power-next-gen-ai-data-centers",
    "score": 4,
    "timestamp": "2024-10-27T13:20:10",
    "source": "Hacker News",
    "content": "The rise of AI has led to an explosion of new data center build-outs, but power consumption has become one of the key bottlenecks to further expansion. That's fueled intense interest in small nuclear reactors that can be built right into newer data centers, such as the new eVinci microreactor from Westinghouse, which is designed to output five megawatts of power and work 24/7 for over eight years without refueling.Westinghouse Electric Company, one of the leaders in nuclear power plant construction, has submitted its Preliminary Safety Design Report (PSDR) for the eVinci Microreactor to the National Reactor Innovation Center (NRIC) at the U.S. Department of Energy (DOE). This is a crucial step required at the Idaho National Laboratory (INL) so that the eVinci can be deployed for testing at the NRIC\u2019s Demonstration of Microreactor Experiments (DOME) facility. Westinghouse also says that it\u2019s the first company to do so, making it one of the pioneers in microreactor technology.Some see development in microreactor technology as a crucial strategy to keep our data-driven society moving forward. Many key leaders, including Mark Zuckerberg, say that power supply will constrain AI growth, especially as data centers require more power-hungry GPUs just to process all the data we generate. One report even says that a single modern AI GPU could consume up to 3.7MWh annually \u2014 or about the power needs of the average American home for four months.Aside from Westinghouse, several major AI firms are looking at nuclear reactors to supply their power requirements. This includes Oracle, which has already secured the permits to build three small modular reactors (SMR), and Microsoft, which, at first, was also looking at SMRs for its data centers, but recently just inked a deal to restart the Three Mile Island reactor.The eVinci microreactor is a unique design in such a way that it arrives at the location as a single unit with few moving parts. You could say it is like a very large battery. This makes it easy to transport and deploy, requiring only a single trailer (and likely some armed escorts) to carry the entire reactor from Westinghouse\u2019s manufacturing facility to where it is required.Another way the Westinghouse simplified the design of the eVinci was in its use of heat pipes to transfer thermal energy from the nuclear core. This is similar to how some high-performance laptops cool CPUs and GPUs \u2014 by using a sealed pipe and using natural convection to move the coolant within.This microreactor has a target output of five megawatts and is designed to work 24/7 for over eight years. Once the nuclear fuel inside it is spent, Westinghouse will recover it as a complete unit and could even swap it with another one, minimizing the disruption to the facility that uses it. Again, there are parallels with the concept of a battery.Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.Nuclear energy is a great solution for all this new demand for clean power. However, there are also risks involved, which, hopefully, have already been addressed by newer technologies like this. Nevertheless, Westinghouse\u2019s eVinci is a promising development, as it will make it easier for data centers to set up shop in faraway places that will not disturb population centers. With that, we could get the computing power we require from data centers without adding stress to our ageing electric grid.",
    "comments": [],
    "description": "Westinghouse is taking one step forward to getting approval for its eVinci microreactor.",
    "document_uid": "84f8067aa1",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961986",
    "title": "Visualizing the Nucleus",
    "url": "https://www.youtube.com/watch?v=ach9JLGs2Yc",
    "score": 1,
    "timestamp": "2024-10-27T13:15:20",
    "source": "Hacker News",
    "content": "Visualizing the Nucleus",
    "comments": [],
    "description": "Physicists Rolf Ent from Jefferson Lab, Newport News, VA, and Richard Milner from MIT, together with animator James LaPlante from Sputnik Animation, Portland...",
    "document_uid": "38147d9ba5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961961",
    "title": "Ask HN: What does a fatal attack on the internet look like?",
    "url": "https://news.ycombinator.com/item?id=41961961",
    "score": 2,
    "timestamp": "2024-10-27T13:12:50",
    "source": "Hacker News",
    "content": "I&#x27;ve always been worried about the fragility of infrastructure in the scenario of a real war. I&#x27;m under the impression that it wouldn&#x27;t be too hard for e.g. Russia to take down the European power grid with a cyber attack. In the last couple of years Russia has been spending effort to disconnect from the global internet, and thus the following question naturally arises: &quot;What does a fatal attack on the global internet look like and how feasible is it&quot;?",
    "comments": [
      {
        "author": "beardyw",
        "text": "The internet was conceived at the hight of the cold war to be resilient, enabling local networks to function if disconnected. I am sure that in our race to make it more sophisticated that idea has been compromised, but I think basic operation could be re-established reasonably quickly.",
        "time": "2024-10-27T14:04:55"
      },
      {
        "author": "RadiozRadioz",
        "text": "I imagine it would look similar to the effects of the CrowdStrike incident earlier this year. The feasibility of an attack of that nature is definitely high; it is commonplace for software to download updates unattended, an attacker need only compromise one trusted company to spread their code across the globe.",
        "time": "2024-10-27T13:46:13"
      },
      {
        "author": "belter",
        "text": "It&#x27;s very feasible. It&#x27;s spelled BGP but you would need a distributed team to pull it off. Since I am pro internet wont provide here the details. Same way why a physicist should not go into the full subtle details required to construct a nuclear device :-)<p>In other words, if you need to ask, then you are not the person who should know.",
        "time": "2024-10-27T13:30:28"
      },
      {
        "author": "protomolecule",
        "text": "&gt;In the last couple of years Russia has been spending effort to disconnect from the global internet<p>Kremlin has been spending effort to make sure Russian economy survives being disconnected from the Internet by the West (just like with SWIFT) and also to selectively block sources of western propaganda (plus genuine criticism of the state of affairs in Russia by patriotic Russians).",
        "time": "2024-10-27T13:40:05"
      }
    ],
    "description": "No description available.",
    "document_uid": "efdffe6813",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961956",
    "title": "Germany's Mittelstand Is Collapsing",
    "url": "https://unherd.com/newsroom/the-backbone-of-german-industry-is-collapsing/",
    "score": 6,
    "timestamp": "2024-10-27T13:11:45",
    "source": "Hacker News",
    "content": "403 Forbidden nginx",
    "comments": [],
    "description": "No description available.",
    "document_uid": "15b52410cf",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961939",
    "title": "Capturing carbon from the air just got easier",
    "url": "https://news.berkeley.edu/2024/10/23/capturing-carbon-from-the-air-just-got-easier/",
    "score": 2,
    "timestamp": "2024-10-27T13:09:53",
    "source": "Hacker News",
    "content": "403 Forbidden nginx",
    "comments": [],
    "description": "No description available.",
    "document_uid": "b372f2ed58",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961920",
    "title": "Peer Review: Predicting the Future",
    "url": "https://onlinelibrary.wiley.com/doi/10.1111/opo.13408",
    "score": 1,
    "timestamp": "2024-10-27T13:06:30",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "dec91b6a31",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961902",
    "title": "Demystifying Distributed Checkpointing",
    "url": "https://expertofobsolescence.substack.com/p/demystifying-distributed-checkpointing",
    "score": 1,
    "timestamp": "2024-10-27T13:03:31",
    "source": "Hacker News",
    "content": "Recently, a good friend suggested that given my background and interest in infrastructure, I should explore ML training infra. I had the impression that this area is very academic and something I\u2019d struggle to grasp without a refresher on linear algebra and ML frameworks. While that\u2019s all true, a specific topic that caught my attention is distributed checkpointing. After a bit of Googling, it turns out that aside from academic papers and a couple blogposts from ML researchers, there aren\u2019t that many writings on this topic written by, or tailored to, infrastructure and system engineers. So here I am, kicking off my Substack newsletter to discuss distributed checkpointing in LLM training workflows from the lens of an infrastructure engineer.Checkpointing is a familiar mechanism for folks that have worked with stateful systems \u2014 the idea of storing a snapshot of the current state of a system, so if the system stops or crashes, it can be restored to that state. From database crash recovery, to save files in video games, to web-based tools with auto-save and version history features like Figma, Notion and Google Docs, checkpointing is a well-applied mechanism.What does checkpointing have to do with ML? An LLM training workflow is a massive data pipeline that takes weeks or even months to complete. Since it\u2019s also an active field of research, it tends to be an iterative process that combines automated tasks with hands-on tweaks as researchers continuously experiment and fine-tune model performance. And when issues like overfitting or training instability are detected, the model may be rolled back to a previous checkpoint, discarding work completed after that point. As a result, periodic checkpointing is essential to minimizing waste of compute resources.A typical GPU instance consists of both CPUs and (typically up to 8) GPUs \u2014 CPUs handle general processing tasks, while GPUs focus on highly parallelizable matrix operations. While each GPU has dedicated memory (VRAM), the machine is typically equipped with significantly more main memory (RAM) accessible to the CPUs.A common approach to performing a checkpoint consists of three high-level steps:Copying current training state from VRAM to RAM (known as Device-To-Host copy, or D2H copy).Serializing state into compact byte objects.Dumping serialized objects to disk or remote persistent storage.Why not just directly serialize and store GPU state to disk? It turns out that transferring data from VRAM to RAM is relatively performant, leveraging high-bandwidth PCIe lanes to maximize throughput and minimize latency. In addition, not all hardware supports directly writing GPU state to disk; and even if they did, having GPU tied up to disk- or network-bound operations is suboptimal for resource utilization.It\u2019s worth noting that step 2 and 3 are often executed asynchronously, allowing GPUs to resume training tasks immediately after the state is offloaded to main memory (see PyTorch\u2019s async_save, for example).State-of-the-art LLM training workflows are distributed over hundreds or even thousands of machines. ML engineers have coined the term \u201c3D parallelism\u201d, representing the 3 dimensions used to shard the training workflow across machines:Data Parallelism (DP): shard the input data into batches, with each batch of data trained on the entire model.Pipeline Parallelism (PP): shard the model into sequential layers, making it possible to train with very large models that cannot fit on a single machine.Tensor Parallelism (TP): shard each layer of the model into separate compute units, allowing more parallel execution within a layer.A combination of these sharding mechanisms are used to improve training efficiency.A natural question is, could we just have each node periodically perform its checkpoint independently, in a way that\u2019s no different from single-node checkpointing? This is known as uncoordinated checkpoint; and if these nodes never have to communicate with one another, this approach would be just fine. However, LLM training requires frequent communication and synchronization across nodes (known as collective communication). For example:Sharding input data into batches means we need to later aggregate the results across nodes (i.e. averaging the gradients)Sharding the model into layers means we need synchronize data relayed from one layer to anotherSharding a single layer means each tensor shard must exchange its results with other shards to ensure computation is complete.Having a consistent global training state for checkpointing is critical to the correctness and effectiveness of the training process, so we need mechanisms to synchronize state across shards.Barrier is a classic synchronization technique in distributed systems. For example, distributed processing engine like Flink implemented a variation of the Chandy-Lamport algorithm known as asynchronous barrier snapshotting. The core idea is to periodically insert a barrier event into each parallel stream, and when a downstream task joining the parallel streams receives a barrier event from one of them, it waits for all other barriers to arrive. At that point it can safely take a consistent snapshot and then resume to processing.In LLM training workflow, barriers are used as a synchronization technique when nodes don\u2019t need to exchange data. However, LLM training often requires data exchange due to 3D parallelism discussed earlier. This is where an operation called AllReduce plays a critical role in synchronization. The gist is that each node gathers necessary data from other nodes, performs an aggregation, and then broadcasts the aggregated result back to all other nodes.Broadcasting here is inherently a synchronization mechanism, as each node waits for results from all other nodes, which is why checkpointing is often performed after an AllReduce operation.So does this mean distributed checkpointing is a solved problem? It turns out that as the LLM training state grows with the size of the model, checkpointing becomes increasingly challenging due to memory constraints, network congestions, limited bandwidth of storage I/O, rising failure rates, and increasingly wasted compute; all in ways that are familiar to engineers working on traditional large-scale infrastructure. Plus, reducing time spent performing checkpoint/restore is an important optimization to accelerate training iterations, much like reducing build and CI times to enhance developer productivity.Let\u2019s dive into a few active areas of research.Today\u2019s LLM training workflows primarily rely on full snapshots for checkpointing. So it\u2019s natural to wonder: can we take an incremental snapshot",
    "comments": [],
    "description": "Improving efficiency of LLM training workflow",
    "document_uid": "b4eb43dd17",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961898",
    "title": "Better Java Builds with the Mill Build Tool [video]",
    "url": "https://www.youtube.com/watch?v=Dry6wMRN6MI",
    "score": 1,
    "timestamp": "2024-10-27T13:03:02",
    "source": "Hacker News",
    "content": "Better Java Builds with the Mill Build Tool [video]",
    "comments": [],
    "description": "Mill is an open-source JVM build tool that attempts to improve upon Maven and Gradle. With automatic caching, extensive parallelism, and easy type-checked ex...",
    "document_uid": "dea4e4afd1",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961874",
    "title": "Turn any website into a real Mac app with Fluid",
    "url": "https://fluidapp.com/",
    "score": 1,
    "timestamp": "2024-10-27T12:58:52",
    "source": "Hacker News",
    "content": "Web applications like Gmail, Facebook, Campfire and Pandora are becoming more and more like desktop applications every day. Running each of these web apps in a separate tab in your browser can be a real pain. Fluid lets you create a Real Mac App (or \"Fluid App\") out of any website or web application, effectively turning your favorite web apps into OS X desktop apps. Creating a Fluid App out of your favorite website is simple. Enter the website's URL, provide a name, and optionally choose an icon. Click \"Create\", and within seconds your chosen website has a permanent home on your Mac as a real Mac application that appears in your Dock. Fluid is free. You can download Fluid for free and create as many Fluid Apps as you like. However, for $5, you can purchase a Fluid License which unlocks a few extra features: Pin Fluid Apps to the Mac OS X Status Bar. (Fluid App Menu \u2192 Pin to Status Bar\u2026) Use Userscripts or Userstyles in your Fluid Apps. (Window \u2192 Userscripts) Use Full Screen mode in your Fluid Apps. (View \u2192 Enter Full Screen)",
    "comments": [],
    "description": "No description available.",
    "document_uid": "b5132c8c80",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961866",
    "title": "A Chopin Waltz Unearthed After Nearly 200 Years",
    "url": "https://www.nytimes.com/2024/10/27/arts/music/chopin-waltz-discovery.html",
    "score": 71,
    "timestamp": "2024-10-27T12:57:04",
    "source": "Hacker News",
    "content": "Please enable JS and disable any ad blocker",
    "comments": [
      {
        "author": "lioeters",
        "text": "Rhymes with the &quot;new&quot; Mozart composition rediscovered last month.<p>&gt; While compiling the K\u00f6chel catalogue&#x27;s newest edition \u2013 an authoritative list of all of Mozart&#x27;s documented musical works \u2013 classical music researchers rediscovered the manuscript of the previously unknown piece from the Carl Ferdinand Becker collection in Leipzig&#x27;s music library.<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ganz_kleine_Nachtmusik#Rediscovery\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ganz_kleine_Nachtmusik#Redisco...</a> (Sept 2024)",
        "time": "2024-10-27T15:26:30"
      },
      {
        "author": "Jabbles",
        "text": "I wonder if music experts could have identified it as a work by Chopin just by the sound? Obviously it&#x27;s a bit late now, but it would have been an interesting experiment - to ask 100 &quot;professors of music&quot; to guess which composer out of [1] wrote this newly discovered piece.<p>[1] <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Category:19th-century_classical_composers\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Category:19th-century_classica...</a>",
        "time": "2024-10-27T13:45:45"
      },
      {
        "author": "pvg",
        "text": "<a href=\"https:&#x2F;&#x2F;archive.is&#x2F;tjk6Q\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.is&#x2F;tjk6Q</a>",
        "time": "2024-10-27T13:02:18"
      },
      {
        "author": "pama",
        "text": "Is there a link to the complete music score somewhere?",
        "time": "2024-10-27T13:34:31"
      },
      {
        "author": "stevage",
        "text": "Seems pretty plausible to me. But as usual, there&#x27;s a reason this piece wasnt published - it&#x27;s not great.",
        "time": "2024-10-27T13:43:54"
      },
      {
        "author": "mathgeek",
        "text": "Sounds like a great excuse to remaster&#x2F;remake Endless Sonata.",
        "time": "2024-10-27T14:05:00"
      },
      {
        "author": "andrewstuart",
        "text": "I always assume these\u201dlong lost works\u201d are fakes.<p>How do they know they aren\u2019t?",
        "time": "2024-10-27T14:11:42"
      },
      {
        "author": "benjamaan",
        "text": "[flagged]",
        "time": "2024-10-27T14:05:09"
      }
    ],
    "description": "No description available.",
    "document_uid": "2a7ecb9dfb",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961863",
    "title": "OpenZFS deduplication is good now and you shouldn't use it",
    "url": "https://despairlabs.com/blog/posts/2024-10-27-openzfs-dedup-is-good-dont-use-it/",
    "score": 3,
    "timestamp": "2024-10-27T12:56:36",
    "source": "Hacker News",
    "content": "27 October, 2024 OpenZFS deduplication is good now and you shouldn't use it OpenZFS 2.3.0 will be released any day now, and it includes the new \u201cFast Dedup\u201d feature. My team at Klara spent many months in 2023 and 2024 working on it, and we reckon its pretty good, a huge step up from the old dedup as well as being a solid base for further improvements. I\u2019ve been watching various forums and mailing lists since it was announced, and the thing I kept seeing was people saying something like \u201cit has the same problems as the old dedup; needs too much memory, nukes your performance\u201d. While that was true (ish), and is now significantly less true, the real problem is that this just repeating the same old non-information that they probably heard from someone else repeating it. I don\u2019t blame anyone really; it is true that dedup has been extremely challenging to get the best out of, it\u2019s very difficult to find good information about using it well, and \u201cdon\u2019t use it\u201d was and remains almost certainly the right answer. But, with this being the first time in almost two decades that dedup has been worth even considering, I want to get some fresh information out there about the what dedup is, how it worked traditionally and why it was usually bad, what we changed with fast dedup, and why it\u2019s still probably not the thing you want. Table of contents What even is dedup? \ud83d\udd17 Dedup can be easily described in a sentence. When OpenZFS prepares to write some data to disk, if that data is already on disk, don\u2019t do the write but instead, add a reference to the existing copy. The challenge is all in how you determine whether or not the data is already on disk, and knowing where on disk it is. The reason it\u2019s challenging is that that information has to be stored and retrieved, which is additional IO that we didn\u2019t have to do before, and that IO can add surprising amounts of overhead! This stored information is the \u201cdedup table\u201d. Conceptually, it\u2019s hashtable, with the data checksum as the \u201ckey\u201d and the on-disk location and refcount as the \u201cvalue\u201d. It\u2019s stored in the pool as part of the pool metadata, that is, it\u2019s considered \u201cstructural\u201d pool data, not user data. How does dedup work? \ud83d\udd17 When dedup is enabled, the \u201cwrite\u201d IO path is modified. As normal, a data block is prepared by the DMU and handed to the SPA to be written to disk. Encryption and compression are performed as normal and then the checksum is calculated. Without dedup, the metaslab allocator is called is called to request space on the pool to store the block, and the locations (DVAs) are returned and copied into the block pointer. When dedup is enabled, OpenZFS instead looks up the checksum in the dedup table. If it doesn\u2019t find it, it calls out to the metaslab allocator as normal, gets fresh DVAs, fills the block and lets the IO through to be written to disks as normal, and then creates a new dedup table entry with the checksum, DVAs and the refcount set to 1. On the other hand, if it does find it copies the DVAs from the the value into the block pointer and returns the writing IO as \u201ccompleted\u201d. and then increment the refcount. Blocks allocated with dedup enabled have a special D flag set on the block pointer. This is to assist when it comes time to free the block. The \u201cfree\u201d IO path is similarly modified to check for the D flag. If it exists, the same dedup table lookup happens, and the refcount is decremented. If the refcount is non-zero, the IO is returned as \u201ccompleted\u201d, but if it reaches zero, then the last \u201ccopy\u201d of the block is being freed, so the dedup table entry is deleted and the metaslab allocator is called to deallocate the space. So all this is working, in that OpenZFS is avoiding writing multiple copies of the same data. The downside is that every single write and free operation requires a lookup and a then a write to the dedup table, regardless of whether or not the write or free proper was actually done by the pool. It should be clear then that any dedup system worth using needs to save more in \u201ctrue\u201d space and IO than it spends on the overhead of managing the table. And this is the fundamental issue with traditional dedup: these overheads are so outrageous that you are unlikely to ever get them back except on rare and specific workloads. Why is traditional dedup so bad? \ud83d\udd17 All of the detail of dedup is in how the table is stored, and how it interacts with the IO pipeline. There\u2019s three main categories of problem with the traditional setup: the construction and storage of the dedup table itself the overheads required to accumulate and stage changes to the dedup table the problem of \u201cunique\u201d entries in the table The dedup table \ud83d\udd17 Traditional dedup implemented the dedup table in probably the simplest way that might work: it just used hooked up the standard OpenZFS on-disk hashtable object and called it a day. This object type is a \u201cZAP\u201d, and it\u2019s used throughout OpenZFS for file directories, property lists and internal housekeeping. It\u2019s an entirely reasonable choice. It\u2019s also really not well suited to an application like dedup. A ZAP is a fairly complicated structure, and I\u2019m not going to get into it here. For our purposes, it\u2019s enough to know that each data block in a ZAP object is an array of fixed-size \u201cchunks\u201d, with a single key/value consuming as many chunks as are needed to hold the key, the data, and and a header describing how the chunks are being used. A dedup entry has a 40-byte key. The value part can be up to 256 bytes, however this is compressed before storing it, so",
    "comments": [
      {
        "author": "bhouston",
        "text": "Nice work!<p>I found setup worked well on my ZFS device.  The main issue with ZFS that I ran into was slow writes even with caches when dealing with a lot of disks.  It just felt like I wasn\u2019t making the most of my hardware even with bulk not modifying writes.<p>I guess it may be the result of needing to do random writes to update directory structures or similar?<p>I had an array of 10 8GB drives and large writes would get &lt;100MB&#x2F;s even on 10GBE and the bottleneck wasn\u2019t cpu or memory either.",
        "time": "2024-10-27T13:20:29"
      },
      {
        "author": "nabla9",
        "text": "Everybody talks about OpenZFS block level dedup. The real gem is to use file level deduplication in copy-on-write transactional filesystems like ZFS.<p><pre><code>   cp --reflink=auto\n</code></pre>\nThe commands above perform a lightweight copy (zfs clone in file level), where the data blocks are copied only when modified.",
        "time": "2024-10-27T13:19:43"
      }
    ],
    "description": "OpenZFS 2.3.0 will be released any day now, and it includes the new \u201cFast Dedup\u201d feature. My team at Klara spent many months in 2023 and 2024 working on it, and we reckon its pretty good, a huge step up from the old dedup as well as being a solid base for further improvements.",
    "document_uid": "28d75f0ae5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961860",
    "title": "Show HN: UTMultiply \u2013 easy VM cloning in UTM",
    "url": "https://github.com/lambda-m/UTMultiply",
    "score": 1,
    "timestamp": "2024-10-27T12:55:07",
    "source": "Hacker News",
    "content": "Scratching my own itch in setting up lab environments on my laptop. I have some ideas on extending this to become a little more robust, support more distros, clean up etc. but for now it helps me quickly spin up a few machines to test other software.",
    "comments": [],
    "description": "Easy UTM Cloning. Contribute to lambda-m/UTMultiply development by creating an account on GitHub.",
    "document_uid": "b264c1a1eb",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961846",
    "title": "Norlha, the Luxury Yak Wool Brand Made by Nomads on the Tibetan Plateau",
    "url": "https://www.anothermag.com/fashion-beauty/12229/norlha-is-the-tibetan-label-changing-what-luxury-fashion-looks-like",
    "score": 3,
    "timestamp": "2024-10-27T12:52:32",
    "source": "Hacker News",
    "content": "With ethical and sustainable practices at the forefront, Kim and Dechen Yeshi\u2019s label Norlha makes use of Tibet\u2019s precious yak wool and ancient techniques to make beautiful hand-crafted clothingFebruary 06, 2020 Who is it? Norlha, the Tibetan luxury brand working with nomads who skilfully hand-weave from yak wool Why do I want it? The brand places the utmost importance on ethical and sustainable practices, preserving tradition, nurturing a local economy and producing timeless garments in the process Where can I find it? At Norlha\u2019s online store and various worldwide stockists Who is it? 12 years ago, in a small valley hidden in the Amdo region of the Tibetan Plateau, a mother, Kim Yeshi \u2013 an American anthropologist married to a Tibetan academic \u2013 and her daughter, Dechen, founded Norlha. Today, their yak khullu atelier \u2013 yak khullu wool comes from the soft under-down of a yak \u2013 produces two seasonal collections a year in a nomad settlement comprising 230 families, 6,000 yaks and 20,000 sheep. \u201cI had this idea to do something with yak wool,\u201d explains Kim of the label\u2019s origins. \u201cI had a friend in Kathmandu who worked with cashmere, and we were always talking about the animals that had precious wools in the region. One was the camel, one was the yak, and I came to realise people knew very little about the yak; it wasn\u2019t something out on the market. The idea was to fulfil the potential of this precious fibre, something that hadn\u2019t been done before.\u201d Kim encouraged Dechen to pursue this journey into the unknown \u2013 her daughter was then a Connecticut College graduate interested in directing documentaries \u2013 eventually settling in Ritoma village where Norlha began its life. \u201cI didn\u2019t have a background in textiles and was much more interested in filmmaking,\u201d says Dechen. \u201cI started to talk to young people here and realised that they were very open to having an alternative source of employment and, more than anything, wanted to be more in touch with the modern world. What Kim had told me about starting a business with yak wool, plus the Tibetan\u2019s interest in doing something bigger, suddenly clicked together. Filmmaking became a bridge to ask people questions, and through their answers I realised there was a future for us too.\u201d NorlhaPhotography by Nikki McClarron Norlha is now home to 120 employees, all former nomads who have spun, woven and felted for generations. \u201cEverything and everyone works in the same courtyard, there\u2019s a mutual sense of responsibility and family,\u201d Dechen explains. Norlha fibres are hand-spun using the finest yak khullu, but unlike other animals that are combed or shaved, the wool of the yak can only be caught as molt when it sheds in late spring. A complex process, one winter scarf could be made up of an entire year\u2019s worth of wool from seven different yaks. The soft wool yarns are then dyed, using a palette of colours obtained by natural pigments made locally by the craftsmen of the workshop. The resulting clothing \u2013 timeless Chuba shirts and dresses, oversized scarfs, seamless vests, felt trousers and shirt jackets \u2013 have a precious, home-spun quality. Earthy tones are matched with rich red and indigo, and encapsulate the brand\u2019s strong bond to the natural world and traditions around them, the tundra of winter, grasslands of summer and its many other seasons of colour. NorlhaPhotography by Nikki McClarron Why do I want it? Norlha hopes to redefine the way we see luxury, making it synonymous not just with high-quality materials and careful craftsmanship, but also an ethical conscience and passionate workforce, with Tibetan culture at the fore (the name itself, in the Tibetan language, means \u2018wealth of the Gods\u2019 and is used by local nomads in reference to their yaks, which are among the most important resources of Central Asia). \u201cWe don\u2019t want people buying from us just out of sympathy, because then they won\u2019t come back and it\u2019s not sustainable,\u201d says Kim. \u201cWe see this sustainability in the same way with Norlha\u2019s workers. Our artisans need to have a purpose and income to see their worth in the business, to encourage them to stand on their own two feet,\u201d she adds. Employees have learnt not only to spin and weave, but become linked to the world within a familiar environment, one that moves ahead without compromising core cultural values. \u201cOur mission is to attract the younger generation as they are the future,\u201d says Dechen. Norlha was first founded in the midst of a changing Tibet: the growing availability of the internet in the country, and the introduction of the Tibetan language on iPhones, has help opened to the rest of the world. \u201cThis is when we realised it was possible to be a brand,\u201d Dechen says. \u201cWe want people to understand Tibetan culture in a different way through our products. We want to tell our story, for the local community as much as for the global community, bringing everyone together as one.\u201d Where can I find it? At Norlha\u2019s workshop and online store, plus various worldwide stockists, found here.",
    "comments": [],
    "description": "With ethical and sustainable practices at the forefront, Kim and Dechen Yeshi\u2019s label Norlha makes use of Tibet\u2019s precious yak wool and ancient techniques to make beautiful hand-crafted clothing",
    "document_uid": "21b1c97963",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961844",
    "title": "A Useful metric for finding Great Art",
    "url": "https://medium.com/luminasticity/a-useful-metric-for-finding-great-art-3a88b727ca4c",
    "score": 2,
    "timestamp": "2024-10-27T12:52:20",
    "source": "Hacker News",
    "content": "So now the scene has been set, two songs that are not to my taste \u2014 and I hope given my examples that you will think hey those are not to IG Agent 13\u2019s taste.The Beatles \u2014 YesterdayThe Beatles \u2014 SomethingThese are songs that are often described as great, by a group that is often described as great. But as I was thinking to myself lately about the song Yesterday, prompted an article about the songwriting skills of Mr. McCartney, I think they might actually be great! (The article made me think about how it was not to my taste, but I like a great deal, which led to me thinking about Something which is also not to my taste and which I like)Now before we go on and just in case you think he likes them just because they are the Beatles, here\u2019s another song not to my taste which I absolutely hate, definitely more than I hate Your Song, which I suppose you would think yeah that song would not be to Agent 19's taste:The Beatles \u2014 MichelleGod I hate that song. Play the damn Helter Skelter thing again McCartney!So I could go on about why I think these songs are great, but that is beside the point (of this article at least, although very far from beside the point about art), or I could make the rude point that the movie Yesterday would be a perfect example of a movie that should be to my taste but I only ever think about to get annoyed at how bad it was which I guess I just did. (no offense to the actors who I quite liked)The point is that with a sufficient understanding of your own taste and what you like that transcends your taste you have a likely indicator of greatness, and it may repay further thinking later as to what greatness in the work attracted you despite yourself. So, think about it.",
    "comments": [],
    "description": "As always when we use the word Art we do not mean just the graphic or plastic arts, but also musical and literary art forms. In the previous critical article To Speak Meaningfully About Art we said\u2026",
    "document_uid": "a033a57049",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961787",
    "title": "Rio Terminal \u2013 Vulkan, Retro Shaders and Rust [video]",
    "url": "https://www.youtube.com/watch?v=zERXjdfuSBw",
    "score": 1,
    "timestamp": "2024-10-27T12:42:17",
    "source": "Hacker News",
    "content": "Rio Terminal \u2013 Vulkan, Retro Shaders and Rust [video]",
    "comments": [],
    "description": "Rio Terminal features 3d acceleration, tabs, split panes, shortcuts customization, and many cool shaders! Githubhttps://github.com/raphamorim/rioOfficial Sit...",
    "document_uid": "a5e45e2447",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961762",
    "title": "Show HN: Time Tracking and Analytics for freelance developers",
    "url": "https://www.chrono.ly/",
    "score": 1,
    "timestamp": "2024-10-27T12:38:06",
    "source": "Hacker News",
    "content": "My friend and I were sick of the existing options having bad ui and slow responsiveness so we made this dashboard and our own VSCode extension to go with it.",
    "comments": [],
    "description": "Time tracking and analytics for developers",
    "document_uid": "97321d33a2",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961761",
    "title": "The Great Exhibition unveils the only office roller coaster in Stockholm",
    "url": "https://www.creativeboom.com/news/worlds-first-office-roller-coaster-launches-in-stockholm/",
    "score": 2,
    "timestamp": "2024-10-27T12:37:57",
    "source": "Hacker News",
    "content": "The Great Exhibition, a newly rebranded creative studio in Stockholm, has introduced The Frontal Lobe, the world's only indoor office roller coaster. The 60-metre track winds through the company's light-filled workspace in Liljeholmen, threading through areas such as the kitchen and storage spaces, creating an unexpected experience that's designed to make a lasting impression. Named after the brain's frontal lobe \u2013 responsible for memory, decision-making, and creativity \u2013 the roller coaster reflects the studio's shift towards \"exploring creativity through unexpected experiences and physical installations,\" as they put it. \"I know absolutely nothing about algorithms or data, but I know a lot about living \u2013 and living is about experiencing things and collecting strong memories,\" says Petter Kukacka, founder and creative director of The Great Exhibition. \"That's the idea we're trying to work with, both in the roller coaster and in all of our projects.\" Previously known as PJADAD, the studio has spent 15 years working with some of Sweden's largest brands. Now, the team is focusing on projects that move away from the \"predictable patterns of technology-driven content\", instead aiming to create experiences that resonate emotionally. \"Our goal is to foster creativity that feels real, breaking from routine and leaving a genuine mark,\" Kukacka adds. The roller coaster, which took over a year to design and build, features three metres of elevation and is constructed from four tons of red-lacquered steel. While some questioned the practicality of having a roller coaster in the office, Kukacka reflects, \"In the end, everyone agreed that the benefits outweighed the challenges. Building this roller coaster taught us that almost anything is possible if you have a clear idea and stick to it.\" Per Cromwell, who led the production, explains the project's deeper meaning: \"This roller coaster is about proving that when you commit to an idea, no matter how unconventional, it can become something remarkable \u2013 something that transcends the everyday and creates strong memories. It's also a response to the pressures the creative industry is facing. The technological revolution is killing creativity, and we want to bring back what feels real \u2013 something an algorithm could never propose.\" Even better, The Frontal Lobe will be open to the public starting today, 25 October, at L\u00f6vholmsv\u00e4gen 18, Liljeholmen, Stockholm. Has this inspired a new addition to your own office?",
    "comments": [],
    "description": "A creative studio might grab attention in many ways, but some solutions might be more thrilling than others. To announce its new name and brand identi...",
    "document_uid": "571e37c515",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961746",
    "title": "The Internet Archive Breach",
    "url": "https://rxx.fi/internet-archive-databreach/",
    "score": 3,
    "timestamp": "2024-10-27T12:35:58",
    "source": "Hacker News",
    "content": "The Internet Archive, home to the Wayback Machine, is one of the most widely used digital archiving services. Recently, however, it became the target of a massive data breach impacting millions of users and raising serious concerns about online security practices and the implications for personal data safety. Initial Attack and Security Weaknesses The breach\u2019s initial signs appeared in May during a Distributed Denial of Service (DDoS) attack. Taking advantage of the holiday weekend, hackers targeted the platform when its employees were likely unavailable. Although the service was restored, further vulnerabilities soon emerged. By October 9, the situation had escalated, with users receiving a concerning JavaScript alert hinting at the platform\u2019s inadequate security measures. This breach led to over 31 million users\u2019 information being leaked, exposing personal data such as email addresses and even GitLab authentication tokens. Key Details of the Data Breach The database stolen from the Internet Archive included email addresses, password timestamps, and Bcrypt-hashed passwords, totaling about 6.4 GB. The breach timeline suggests that hackers had access to the system for weeks, gathering data without detection. The incident also revealed the Archive\u2019s failure to promptly reset compromised credentials, as hackers were still able to access the Zenes email support platform even after the breach was discovered. This oversight allowed the hackers to control user communications, further compromising user privacy and the organization\u2019s credibility. How Hackers Exploited the System The breach was later traced back to a configuration file left exposed on one of the Archive\u2019s development servers since at least December 2022. This file contained authentication tokens, enabling hackers to download source code, access database management systems, and manipulate website elements, including user alerts. They allegedly also exfiltrated around 7 terabytes of additional data, though the exact content of this data has not been publicly verified. Broader Implications and Speculation on Motives Many speculate that the Archive\u2019s status as a \u201csacred\u201d internet resource, valued for preserving content often removed elsewhere, made it a controversial target. There are theories linking the breach to copyright infringement suits the Archive has faced from media companies, though no definitive cause has been established. The platform\u2019s role in preserving valuable open-source intelligence data and investigative resources makes it an unlikely target for most hackers, adding to the complexity of the breach\u2019s motives. What Users Should Know Going Forward This breach raises several lessons for both users and organizations regarding data security: Password Management: Users are encouraged to use unique, strong passwords for every account. Password managers can help automate this process, reducing the risk posed by password leaks. Data Privacy Awareness: The incident emphasizes the importance of controlling personal data shared online, as the Archive reportedly requires users to submit identification for certain requests. Users should be cautious about sharing sensitive information, even with trusted organizations. Security Protocols: Organizations should ensure all servers and development files are secure, particularly those that may hold sensitive information or authentication tokens. Regular audits and immediate resets of compromised credentials are essential to preventing prolonged unauthorized access. The Internet Archive breach is a reminder of the challenges in maintaining secure digital infrastructures, even for reputable and well-respected organizations. As hackers become increasingly sophisticated, both users and organizations need to stay informed and vigilant about evolving security practices to protect their data. Conclusion The Internet Archive\u2019s recent breach is a wake-up call to prioritize data security, not just for users but also for organizations managing vast amounts of sensitive data. While the Archive may recover, the incident serves as a reminder of the potential risks associated with digital archives and the essential measures needed to prevent similar occurrences in the future.",
    "comments": [
      {
        "author": "nymanromeo",
        "text": "[flagged]",
        "time": "2024-10-27T12:35:58"
      }
    ],
    "description": "The Internet Archive, home to the Wayback Machine, is one of the most widely used digital archiving services. Recently, however, it became the target of a massive data breach impacting millions of users and raising serious concerns about online security practices and the implications for personal data safety.",
    "document_uid": "ad243e2f3b",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961723",
    "title": "Exceptional One-Minute Short Films",
    "url": "https://www.musicbed.com/articles/filmmaking/cinematography/6-exceptional-one-minute-short-films/",
    "score": 2,
    "timestamp": "2024-10-27T12:32:19",
    "source": "Hacker News",
    "content": "Movie magic happens in an instant. It\u2019s an alchemy of elements that, for whatever reason, creates something much more valuable than the pieces themselves. And like alchemy, the formula has been sought for generations. So far, no luck. But if there\u2019s one thing we know movie magic does not require, it\u2019s a lot of time. When we were gearing up for the Filmsupply Challenge, we started searching for some great films under one minute. The films below are some that really stood out to us, both for their brevity and their impact. They\u2019re 1/120th the length of the average feature film, and yet many of them deliver the same emotional punch, the same sense of wonder, the same movie magic as their much longer counterparts. Super short films are like ants. Compact, but able to carry 5,000 times their body weight. They are an education in concision, implication, distillation, and the raw power of moving images. They follow their own rules, which is why it\u2019s nearly impossible to find a formula for them. Often, they can be understood only in retrospect, as they play themselves over and over again in your head. VIDEO WHEN YOU SAY YOU\u2019RE A SWIMMER Like a swimmer, this film by Chris Shimojima creates \u23af and is carried by \u23af its own momentum. It tumbles out in monologue and imagery, giving the piece the same relentless force as the sport it represents. By the end, the narrator is nearly breathless, and so are we. It may seem like the film is telling us about swimming. In reality, it\u2019s throwing us into the deep end. In just one minute, When You Say You\u2019re a Swimmer conveys all the pain, panic, and glory of swimming itself. VIDEO LATE It doesn\u2019t take long to establish characters an audience cares about. Late uses its minute with poise. It takes its time. And for a second in the middle, it even seems to drag. But these extra moments with the characters are essential for its final punch to land. When it does, we feel a huge emotional burden for two people we\u2019ve barely spent 60 seconds with. Without even realizing it, we bonded with them. As their world sinks, so does ours. MEMORIES OF THE MASTER BREWER Memories of the Master Brewer is an ad, but it\u2019s so beautifully executed that it elevates itself into the realm of cinema. A journey into a brewer\u2019s past told through the characteristics of beer, the film moves through time, space, and metaphor at the perfect pace (not plodding or breakneck), giving us a sense of history and significance in less time than it takes to down a cold one. BUTTONS This hypnotic little film perfectly captures the quirky and colorful world of button making. Like buttons themselves, the beauty of this film is in its simplicity. It relies on composition, colors, music, and spot-on sound design to create its charm. Somehow, even without any characters, story, or dialogue, Giant Ant has managed to create a one-minute film we can watch over and over again, and still find something new to appreciate. VIDEO BLACK HOLE While brilliantly executed, Black Hole is truly a concept-driven piece. It\u2019s the idea that sticks with you more than the film does. In that way, its one-minute run time works to its advantage. Just long enough to plant an idea in your mind, but not so long that it exhausts its possibilities. It makes you want more but refuses to indulge. And like all good films, it twists just enough at the end to provide a bit of closure. VIDEO WILDEBEEST WILDEBEEST cheats its one-minute constraint by allowing the story to loop in its viewers\u2019 minds. The ending has a \u201csong that never ends\u201d quality, implying that perhaps this argument between wildebeests will continue down the line until the end of time. The film is essentially a gag, executed with no cuts, no music, and just two graphics to indicate dialogue. A brilliant little machine of a film.",
    "comments": [],
    "description": "Movie magic happens in an instant. It\u2019s an alchemy of elements that, for whatever reason, creates something much more valuable than the pieces themselves. And like alchemy, the formula has been sought for generations. So far, no luck. But if there\u2019s one thing we know movie magic does not require, it\u2019s a lot of time.",
    "document_uid": "b675b586e5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961716",
    "title": "Bridging Search and Recommendation in Generative Retrieval",
    "url": "https://dl.acm.org/doi/10.1145/3640457.3688123",
    "score": 1,
    "timestamp": "2024-10-27T12:30:34",
    "source": "Hacker News",
    "content": "Publication HistoryPublished: 08 October 2024",
    "comments": [],
    "description": "No description available.",
    "document_uid": "c4054fcfa5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961677",
    "title": "A New Volcanic Era",
    "url": "https://www.cnn.com/interactive/2024/10/climate/solutions/iceland-volcanos/index.html",
    "score": 2,
    "timestamp": "2024-10-27T12:22:39",
    "source": "Hacker News",
    "content": "KraflaGrindavikReykjavikICELAND50 miles50 kmFor Klara Halld\u00f3rsd\u00f3ttir, the volcano that has roiled southwest Iceland for the last year is both awe-inspiring and devastating. She has hiked to a crater to marvel at the fiery eruption and the river of cooled lava stretching down toward Grindavik, the town she has lived in her whole life.But this volcano also forced her from her home.Last November, she was at the beach letting her dogs run wild when the ground started moving and did not stop. It was a swarm of earthquakes, the warning signs of an imminent eruption.Her family packed swiftly and joined a line of cars rolling out of town. It was \u201clike a terrible movie,\u201d she said. No one knew whether they would be able to come back. Nearly a year on, only a handful of the 3,600 residents have returned.Vikings roamed the last time the volcanoes on Iceland\u2019s Reykjanes Peninsula raged. Now, eight centuries later, this slice of land close to the capital city Reykjavik is one of the more densely populated parts of the country.Icelanders like Klara have a complex relationship with volcanoes. They are both a force of destruction \u2014 lava has already consumed homes and carved up roads in Grindavik \u2014 and a source of abundant clean energy, powering people\u2019s lives.The country itself was born of volcanic activity.Earthquakes since 2020:4.53.55 magnitudeApproximate location of fissureTectonic plate boundaryVolcanic eruptions, since 1960Tectonic plates are huge pieces of the planet\u2019s outer shell constantly ripping apart and crashing together, causing earthquakes and volcanoes.Iceland, one of the most volcanically active places on the planet, was created from lava, steam and heat.It sits astride the Mid-Atlantic Ridge, a huge spine of mostly underwater mountains that separates two plates. Magma pushes up as the plates pull apart, creating new land in a violent, searing process.Since December 2023, the Reykjanes Peninsula in southwest Iceland has been rocked by a series of earthquakes and eruptions.Data as of June 17, 2024 Source: National Oceanic and Atmospheric Administration, United States Geological Survey, Icelandic Meteorological Office, Polar Geospatial CenterThe first eruption arrived in December. A fissure more than two miles long sliced the ground open, sending fountains of lava spewing hundreds of feet into the air. Five more eruptions have followed, engulfing homes on the outskirts of town, threatening a vital power station and turning Grindavik into a ghost town. The lava has also flowed close to the turquoise pools of the Blue Lagoon geothermal spa, one of Iceland\u2019s most famous tourist attractions, forcing several evacuations and closures over the past year.Klara Halld\u00f3rsd\u00f3ttir visits the Grindavik home she was forced to flee, as lava erupts from a crater just a few miles away, in April 2024. Iceland is used to volcanoes. It experiences roughly one eruption every five years, though most are in uninhabited areas. Some are even described as \u201ctourist volcanoes,\u201d relatively accessible and typically non-disruptive. These new eruptions are not that; they are violent, dangerous and could last centuries. They could also hold the key to a new future.As this new volcanic era upends lives in the southwest, hundreds of miles northeast at a volcanic caldera called Krafla, there is an audacious plan underway to drill directly into a magma chamber.Firefighters driving near the Sundhnukagigar fissure, north of Grindavik, in April 2024KraflaGrindavikReykjavikICELAND50 miles50 km\u2018The potential is limitless\u2019In 2009, Bjarni P\u00e1lsson was an engineer with Iceland\u2019s national power company, Landsvirkjun, running a deep drilling geothermal project at Krafla. They were trying to sink a borehole nearly 3 miles into the ground, but the drill kept getting stuck. \u201cAgain and again, at exactly the same depth,\u201d he said.When they were eventually able to free it, they found glass chips \u2014 cooled, crystallized, molten rock. It was proof of what they had stumbled upon: a magma chamber. P\u00e1lsson was shocked. These reservoirs of super-hot molten rock exist everywhere there are volcanoes, but are very hard to find and usually much deeper. The team rushed to control and cool the borehole, pumping in around 1 million tons of cold water before closing it. Fifteen years later, P\u00e1lsson is standing in the exact same spot, his high-viz jacket the only splash of color in Krafla\u2019s stark, white landscape. Armed with new technology and know-how, he is going back in.The ambition of the geothermal experts and volcanologists that comprise the Krafla Magma Testbed is to convert the immense heat and pressure into a new \u201climitless\u201d form of supercharged geothermal energy \u2014 a tantalizing prospect as the world struggles to end its relationship with planet-heating fossil fuels. \"This has never been done before,\" said Hjalti P\u00e1ll Ing\u00f3lfsson, director of the Geothermal Research Cluster, which developed the project. He compares its scale and ambition to the James Webb Space Telescope, which is giving humans an unprecedented view of the universe. If they succeed, the implications could reverberate around the world, Ing\u00f3lfsson said. There are an estimated 800 million people living within roughly 60 miles of an active volcano.\u201cWe are looking into the sky, we are spending trillions and trillions of dollars to understand planets far away,\u201d he said. \u201cBut we do not spend nearly as much on understanding our own.\u201dGeothermal expert Bjarni P\u00e1lsson was at Krafla in 2009 when they discovered the magma chamber. Fifteen years on, he\u2019s getting ready to drill into it again \u2014 this time with an ambitious purpose. Krafla is a unique natural laboratory for studying volcanoes. Not only is it one of the hottest geothermal fields in the world, it is also very accessible. There is a power plant, high speed internet and a paved road \u2014 all on top of a volcano.Drilling into magma that is around 1,800 Fahrenheit (nearly 1,000 degrees Celsius) won\u2019t be easy. But as humans heat the planet at record speed with fossil fuel pollution, there is increasing pressure to perform moonshot feats of engineering to save us from ourselves.If all goes to plan, the first borehole will be completed in 2027 and will mark the first time anyone has ever implanted sensors directly into a magma chamber.",
    "comments": [],
    "description": "They drilled into a volcano. What they found could help power the world.",
    "document_uid": "9350620c94",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961669",
    "title": "Why Debian won't distribute AI models any time soon [audio]",
    "url": "https://deepdive.opensource.org/podcast/why-debian-wont-distribute-ai-models-any-time-soon/",
    "score": 2,
    "timestamp": "2024-10-27T12:21:48",
    "source": "Hacker News",
    "content": "Why Debian won\u2019t distribute AI models any time soon Deep Dive: AI Why Debian won't distribute AI models any time soon Play Episode Pause Episode Mute/Unmute Episode Rewind 10 Seconds 1x Fast Forward 30 seconds 00:00 / Subscribe Share Welcome to a brand new episode of Deep Dive: AI! For today\u2019s conversation, we are joined by Mo Zhou, a PhD student at Johns Hopkins University and an official Debian developer since 2018. Tune in as Mo speaks to the evolving role of artificial intelligence driven by big data and hardware capacity and shares some key insights into what sets AlphaGo apart from previous algorithms, making applications integral, and the necessity of releasing training data along with any free software. You\u2019ll also learn about validation data and the difference powerful hardware makes, as well as why Debian is so strict about their practice of offering free software. Finally, Mo shares his predictions for the free software community (and what he would like to see happen in an ideal world) before sharing his own plans for the future, which include a strong element of research. If you\u2019re looking to learn about the uphill climb for open source artificial intelligence, plus so much more, you won\u2019t want to miss this episode! Full transcript. Key points from this episode: Background on today\u2019s guest, Mo Zhou: PhD student and Debian developer. His recent Machine Learning Policy proposal at Debian. Defining artificial intelligence and its evolution, driven by big data and hardware capacity. Why the recent advancements in deep learning would be impossible without hardware. Where AlphaGo differs from past algorithms. The role of data, training code, and inference code in making an application integral. Why you have to release training data with any free software. The financial and time expense of classifying images. What you need access to in order to modify an existing model. The validation data set collected by the research community. Predicting the process of retraining. What you can gain from powerful hardware. Why Debian is so strict in the practice of free software. Problems that occur when big companies charge for their ecosystems. What Zhou is expecting from the future of the free software community. Which licensing schemes are most popular and why. An ideal future for Open Source AI. Zhou\u2019s plans for the future and why they include research. Links mentioned in today\u2019s episode: Credits Special thanks to volunteer producer, Nicole Martinelli. Music by Jason Shaw, Audionautix. This podcast is sponsored by GitHub, DataStax and Google. No sponsor had any right or opportunity to approve or disapprove the content of this podcast. The views expressed in this podcast are the personal views of the speakers and are not the views of their employers, the organizations they are affiliated with, their clients or their customers. The information provided is not legal advice. No sponsor had any right or opportunity to approve or disapprove the content of this podcast.",
    "comments": [],
    "description": "Welcome to a brand new episode of Deep Dive: AI! For today\u2019s conversation, we are joined by Mo Zhou, a PhD student at Johns Hopkins University and an official Debian developer since 2018. Tune in as Mo speaks to the evolving role of artificial intelligence driven by big data and hardware capacity and shares some [\u2026]",
    "document_uid": "3614585ef1",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961654",
    "title": "Is this thing on? Using OpenBMC and ACPI power states for reliable server boot",
    "url": "https://blog.cloudflare.com/how-we-use-openbmc-and-acpi-power-states-to-monitor-the-state-of-our-servers/",
    "score": 3,
    "timestamp": "2024-10-27T12:18:59",
    "source": "Hacker News",
    "content": "At Cloudflare, we provide a range of services through our global network of servers, located in 330 cities worldwide. When you interact with our long-standing application services, or newer services like Workers AI, you\u2019re in contact with one of our fleet of thousands of servers which support those services.These servers which provide Cloudflare services are managed by a Baseboard Management Controller (BMC). The BMC is a special purpose processor \u2014 different from the Central Processing Unit (CPU) of a server \u2014 whose sole purpose is ensuring a smooth operation of the server.Regardless of the server vendor, each server has this BMC. The BMC runs independently of the CPU and has its own embedded operating system, usually referred to as firmware. At Cloudflare, we customize and deploy a server-specific version of the BMC firmware. The BMC firmware we deploy at Cloudflare is based on the Linux Foundation Project for BMCs, OpenBMC. OpenBMC is an open-sourced firmware stack designed to work across a variety of systems including enterprise, telco, and cloud-scale data centers. The open-source nature of OpenBMC gives us greater flexibility and ownership of this critical server subsystem, instead of the closed nature of proprietary firmware. This gives us transparency (which is important to us as a security company) and allows us faster time to develop custom features/fixes for the BMC firmware that we run on our entire fleet.In this blog post, we are going to describe how we customized and extended the OpenBMC firmware to better monitor our servers\u2019 boot-up processes to start more reliably and allow better diagnostics in the event that an issue happens during server boot-up. Server systems consist of multiple complex subsystems that include the processors, memory, storage, networking, power supply, cooling, etc. When booting up the host of a server system, the power state of each subsystem of the server is changed in an asynchronous manner. This is done so that subsystems can initialize simultaneously, thereby improving the efficiency of the boot process. Though started asynchronously, these subsystems may interact with each other at different points of the boot sequence and rely on handshake/synchronization to exchange information. For example, during boot-up, the UEFI (Universal Extensible Firmware Interface), often referred to as the BIOS, configures the motherboard in a phase known as the Platform Initialization (PI) phase, during which the UEFI collects information from subsystems such as the CPUs, memory, etc. to initialize the motherboard with the right settings. Figure 1: Server Boot ProcessWhen the power state of the subsystems, handshakes, and synchronization are not properly managed, there may be race conditions that would result in failures during the boot process of the host. Cloudflare experienced some of these boot-related failures while rolling out open source firmware (OpenBMC) to the Baseboard Management Controllers (BMCs) of our servers. Baseboard Management Controller (BMC) as a manager of the host A BMC is a specialized microprocessor that is attached to the board of a host (server) to assist with remote management capabilities of the host. Servers usually sit in data centers and are often far away from the administrators, and this creates a challenge to maintain them at scale. This is where a BMC comes in, as the BMC serves as the interface that gives administrators the ability to securely and remotely access the servers and carry out management functions. The BMC does this by exposing various interfaces, including Intelligent Platform Management Interface (IPMI) and Redfish, for distributed management. In addition, the BMC receives data from various sensors/devices (e.g. temperature, power supply) connected to the server, and also the operating parameters of the server, such as the operating system state, and publishes the values on its IPMI and Redfish interfaces. Figure 2: Block diagram of BMC in a server system.At Cloudflare, we use the OpenBMC project for our Baseboard Management Controller (BMC).Below are examples of management functions carried out on a server through the BMC. The interactions in the examples are done over ipmitool, a command line utility for interacting with systems that support IPMI. # Check the sensor readings of a server remotely (i.e. over a network) $ ipmitool <some authentication> <bmc ip> sdr PSU0_CURRENT_IN | 0.47 Amps | ok PSU0_CURRENT_OUT | 6 Amps | ok PSU0_FAN_0 | 6962 RPM | ok SYS_FAN | 13034 RPM | ok SYS_FAN1 | 11172 RPM | ok SYS_FAN2 | 11760 RPM | ok CPU_CORE_VR_POUT | 9.03 Watts | ok CPU_POWER | 76.95 Watts | ok CPU_SOC_VR_POUT | 12.98 Watts | ok DIMM_1_VR_POUT | 29.03 Watts | ok DIMM_2_VR_POUT | 27.97 Watts | ok CPU_CORE_MOSFET | 40 degrees C | ok CPU_TEMP | 50 degrees C | ok DIMM_MOSFET_1 | 36 degrees C | ok DIMM_MOSFET_2 | 39 degrees C | ok DIMM_TEMP_A1 | 34 degrees C | ok DIMM_TEMP_B1 | 33 degrees C | ok \u2026 # check the power status of a server remotely (i.e. over a network) ipmitool <some authentication> <bmc ip> power status Chassis Power is off # power on the server ipmitool <some authentication> <bmc ip> power on Chassis Power Control: On Switching to OpenBMC firmware for our BMCs gives us more control over the software that powers our infrastructure. This has given us more flexibility, customizations, and an overall better uniform experience for managing our servers. Since OpenBMC is open source, we also leverage community fixes while upstreaming some of our own. Some of the advantages we have experienced with OpenBMC include a faster turnaround time to fixing issues, optimizations around thermal cooling, increased power efficiency and supporting AI inference.While developing Cloudflare\u2019s OpenBMC firmware, however, we ran into a number of boot problems.Host not booting: When we send a request over IPMI for a host to power on (as in the example above, power on the server), ipmitool would indicate the power status of the host as ON, but we would not see any power going into the CPU nor any activity on the CPU. While ipmitool was correct about the power going into the chassis as ON, we had no information about the power state",
    "comments": [],
    "description": "Cloudflare\u2019s global fleet benefits from being managed by open source firmware for the Baseboard Management Controller (BMC), OpenBMC. This has come with various challenges, some of which we discuss here with an explanation of how the open source nature of the firmware for the BMC enabled us to fix the issues and maintain a more stable fleet.",
    "document_uid": "e305a722f3",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961606",
    "title": "Benchmark GGUF models with a ONE line of code",
    "url": "https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval",
    "score": 1,
    "timestamp": "2024-10-27T12:11:54",
    "source": "Hacker News",
    "content": "Benchmark GGUF models with a ONE line of code. The fastest benchmarking tool for quantized GGUF models, featuring multiprocessing support and 8 evaluation tasks. Currently supports text GGUF models. Supports Windows, Linux, and macOS. Install Nexa SDK Python Pacakage Install Nexa Eval Package pip install 'nexaai[eval]' Choose a GGUF model from Nexa Model Hub to benchmark. You can also upload your own GGUF models. # Evaluate Llama3.2-1B Q4_K_M quantization with \"ifeval\" task nexa eval Llama3.2-1B-Instruct:q4_K_M --tasks ifeval # Use Multiprocessing. You can specify number of workerse to optimize performance. nexa eval Llama3.2-1B-Instruct:q4_K_M --tasks ifeval --num_workers 4 usage: nexa eval model_path [-h] [--tasks TASKS] [--limit LIMIT] positional arguments: model_path Path or identifier for the model in Nexa Model Hub. Text after 'nexa run'. options: -h, --help show this help message and exit --tasks TASKS Tasks to evaluate, comma-separated --limit LIMIT Limit the number of examples per task. If <1, limit is a percentage of the total number of examples. General Tasks ifeval: General language understanding mmlu_pro: Massive multitask language understanding Math Tasks Reasoning Tasks gpqa: General purpose question answering Coding Tasks Safety Tasks GGUF (GGML Universal Format) models are optimized for on-device AI deployment: Reduced memory footprint through quantization Cross-platform compatibility via llama.cpp No external dependencies Supported by popular projects: llama.cpp, whisper.cpp, stable-diffusion.cpp, and more Quantization affects three key factors: File size Model quality Performance Benchmarking helps you: Verify accuracy retention after quantization Select the optimal model for your specific use case Make informed decisions about quantization levels Adapted From Language Model Evaluation Harness.",
    "comments": [],
    "description": "Nexa SDK is a comprehensive toolkit for supporting ONNX and GGML models. It supports text generation, image generation, vision-language models (VLM), auto-speech-recognition (ASR), and text-to-speech (TTS) capabilities. - nexa-sdk/nexa/eval at main \u00b7 NexaAI/nexa-sdk",
    "document_uid": "dea1894d18",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961595",
    "title": "Top astronomy researchers call on FCC to study satellite mega-constellations",
    "url": "https://pirg.org/edfund/articles/120-top-astronomy-researchers-call-on-fcc-to-study-satellite-mega-constellations-spacexs-starlink/",
    "score": 5,
    "timestamp": "2024-10-27T12:10:23",
    "source": "Hacker News",
    "content": "Why have I been blocked? This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.",
    "comments": [
      {
        "author": "zhengiszen",
        "text": "Quote :&quot;The number of large satellites in low Earth orbit has increased 12 times in five years, in large part because of SpaceX. The new space race is ramping up quickly: Some experts estimate an additional 58,000 satellites will be launched by 2030....&quot;",
        "time": "2024-10-27T13:37:56"
      }
    ],
    "description": "No description available.",
    "document_uid": "74bb346202",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961566",
    "title": "Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model",
    "url": "https://dreamcraft3dplus.github.io/",
    "score": 3,
    "timestamp": "2024-10-27T12:06:40",
    "source": "Hacker News",
    "content": "@article{sun2024dreamcraft3d++, title={DreamCraft3D++: Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model}, author={Sun, Jingxiang and Peng, Cheng and Shao, Ruizhi and Guo, Yuan-Chen and Zhao, Xiaochen and Li, Yangguang and Cao, Yanpei and Zhang, Bo and Liu, Yebin}, journal={arXiv preprint arXiv:2410.12928}, year={2024}, } @article{sun2023dreamcraft3d, title={Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior}, author={Sun, Jingxiang and Zhang, Bo and Shao, Ruizhi and Wang, Lizhen and Liu, Wen and Xie, Zhenda and Liu, Yebin}, journal={arXiv preprint arXiv:2310.16818}, year={2023} }",
    "comments": [],
    "description": "DreamCraft3D++: Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model",
    "document_uid": "f55fc3af09",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961564",
    "title": "Easy real-time collision detection",
    "url": "https://arxiv.org/abs/2406.00026",
    "score": 2,
    "timestamp": "2024-10-27T12:06:23",
    "source": "Hacker News",
    "content": "Happy Open Access Week from arXiv! Open access is only possible with YOUR support. Give to arXiv this week to help keep science open for all.",
    "comments": [],
    "description": "Abstract page for arXiv paper 2406.00026: Easy real-time collision detection",
    "document_uid": "960cf152c8",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961550",
    "title": "Decomplexifying Curl",
    "url": "https://daniel.haxx.se/blog/2024/10/27/decomplexifying-curl/",
    "score": 10,
    "timestamp": "2024-10-27T12:04:23",
    "source": "Hacker News",
    "content": "(I wrote about this topic in my weekly email this week. This is the blog version, somewhat extended.) Easy to read Two contributing factors that make code hard to read are function length and function complexity. To keep source code easy to read, understand and debug we should strive towards keeping functions short and simple. Nothing ground-breaking in that conclusion. I know, it sounds really simple and straight forward but in a living project that goes on for decades, code develops, moves and grows over time. What started out small and simple risk gradually turning into something else. This of course because there are so many more factors involved that need to be given focus as well. Like security, bugfixes, performance, food on the table and getting more people involved. Graphs graphs graphs Last week I added two more graphs to the curl dashboard showing function complexity and function length growth in curl code over the decades: one plot for the worst function and one plot for the 99th percentile in each graph. For both graphs, the 99th percentile plots shrink gradually over time but the worst offenders grow. This means that there are a few functions that with attention could improve readability and code maintainability but that in general things are under control. One of the main points for me with graphing the project from as many angles as possible is to unveil things like this. Areas that might need attention, and then keep a check on these areas going forward. Details like these are otherwise rather subtle and not easily detected when manually browsing around. It has been said that whatever measurement you use to track engineering progress, that will then become the goal for what engineers work towards. I hope to combat this by measuring (and graphing) as many angles as possible of the curl project. To help push us in the right direction in as many different areas as possible. Improve I took it upon myself to improve the situation: to reduce the size of the largest function in the code base and to simplify the most complex one. Incidentally they were different functions: the largest function was the big switch handling curl_easy_setopt options, and the most complex one was the main curl tool function setting up a single transfer. These two functions had simply just slowly and consistently been growing over time, in size and complexity. No one\u2019s \u201cfault\u201d really and not with any specific plan or intention. The graph helped me decide to act and the pmccabe tool helped me identify them. We can of course argue about the specific method or number that pmccabe presents for complexity, but I think it at least is pretty good at actually identifying the correct functions and the exact particular score it sets is not terribly important. Both pull-requests became > 2000 modified lines monsters, but they also had immediate and distinct effects on the graphs; which ideally should mean that the code readability is now a little better than before, making the functions easier to improve and work with going forward Complexity The single worst function in production code had gotten quite complex. I spent a work day on the case and look at the drop on the right edge of the graph below, made after my fix landed. Most of the job was to properly split the function into several smaller ones that made sense. The single worst offender at this particular time was the function in the curl tool that sets up a single transfer job. There are still some pretty complex ones remaining. Room for further improvements no doubt. Function length The worst offenders in terms of function size in curl have been of two kinds: state machines with many states and functions handling big switches for options. In this particular case, this was the big function handling curl_easy_setopt(), and as we have over three hundred options having them all handled in a single function made it very big. The new setup splits that handling up into multiple smaller functions, one for each kind of input. The largest one is now at over 1,500 lines. Still on the too large side of things but way better than before. Going forward Yes, I am a graphaholic and I seem to keep finding new ways to illustrate project status and development using plots on timelines. I am also most likely the biggest consumer of these graphs as I monitor them daily to make sure I have full control of how we are in the project, in every imaginable aspect. I intend to try to continue simplifying a few more of the functions in the pmccabe toplist. Let\u2019s see what the graph shows in another three years.",
    "comments": [
      {
        "author": "immibis",
        "text": "Focusing on cyclomatic complexity and function length just leads to spaghetti with meatballs code. You have to keep following different strands of pasta to find where the meat is, instead of having it all in one place, like hamburger code.",
        "time": "2024-10-27T13:15:17"
      }
    ],
    "description": "(I wrote about this topic in my weekly email this week. This is the blog version, somewhat extended.) Easy to read Two contributing factors that make code hard to read are function length and function complexity. To keep source code easy to read, understand and debug we should strive towards keeping functions short and simple. \u2026 Continue reading decomplexifying curl \u2192",
    "document_uid": "244417a46d",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961547",
    "title": "The Zero Click Internet",
    "url": "https://www.techspot.com/article/2908-the-zero-click-internet/",
    "score": 3,
    "timestamp": "2024-10-27T12:04:09",
    "source": "Hacker News",
    "content": "The internet is in the midst of undergoing the biggest change since its inception. It's huge. And there is no going back. The web is changing into the Zero Click Internet, and it will change everything about how you do everything. Zero Click Internet means you'll no longer click on links to find the content you want. In fact, much of the internet already works this way. Editor's Note: Guest author Josh Tyler is an entrepreneur and early internet pioneer, best known as the founder of CinemaBlend and Giant Freakin Robot. He currently serves as the CEO of Walk Big Media. This opinion column was originally posted on X. Platforms like Facebook and Twitter stopped promoting posts with links a few years ago. This forced users on those platforms to post content directly there instead of directing followers to external sites. And now with generative AI in the mix, platforms are even more incentivized to surface content directly, whether it's pulled from their databases or created by AI. This phenomenon isn't entirely new; it began when Google started answering simple queries directly on its search results page. But it's escalated significantly with the rise of AI chatbots and advanced recommendation algorithms. Google is now aggressively scraping content from websites and displaying it directly in search results. What few search results remain are buried so far down the page that almost no one sees or clicks on them anymore. And Google's plan is to bury them even further in coming months. And that's what a Zero Click Internet means. It means an end to users visiting websites, entirely. Instead you'll spend all your time on a small handful of platforms and apps like Google or TikTok and never leave them. The impact this will have, not just on your experience but on the world, will be massive. What a Zero Click Internet means is an end to users visiting websites, entirely. It means an end to digital publishers. The small ones at first. Most of those will be out of business by the end of this year. Then the medium ones will vanish, most likely they will all be gone by spring. Eventually, most of the big publishers will go under. Those that survive will do so by signing contracts with platforms like Google or OpenAI (ChatGPT) to create content specifically for them to scrape. Domain names will no longer have any value, since visiting websites will no longer be a significant portion of most internet traffic. This will spell the end for most domain registrars like GoDaddy and other companies whose entire business revolves around selling domains. The web hosting business will contract as the number of websites in existence shrinks and the number of resources needed to run the remaining ones will decrease. Most smaller hosting companies will likely be out of business by the end of 2025. The independent internet advertising business is also done for. All advertising will be controlled by the major platforms (Google, Meta/Facebook, Amazon, and company already account for over 60% of all online ad spend). Case in point, Google and TikTok don't buy ads from independent networks like Raptive or Playwire (which sell ads on medium-sized websites) because they do it themselves. As publishers vanish, these networks will have no websites to sell ads on, meaning they'll likely be out of business by the end of 2025 as well. The SEO industry is also doomed. SEO becomes meaningless when there are no clicks. Currently, SEO firms can survive the death of the digital publishing industry by focusing on local or niche SEO, but even those areas will eventually be absorbed into the Zero Click environment. And that environment is largely driven by who will pay top dollar to be at the top of a platform's results (Facebook, Google, TikTok, Instagram, LinkedIn, etc.), and who is willing to do all their business on that platform. Visibility will be determined by contracts, rendering SEO irrelevant. Each of these industries I've mentioned are multi-billion dollar industries, and none of them will exist as a significant business by the end of 2026. As for users, information will totally vanish. You'll only be able to find information someone has paid to allow you to find. The Zero Click Internet will limit you to the kind of surface-level information you'd once find in a good set of encyclopedias. And the deeper resources you might have once turned to will no longer exist \u2013 except perhaps as fragmented remnants in the mind of an unreliable Large Language Model. And that's what the future looks like, in the now totally inevitable, all encompassing ecosphere of the new Zero Click Internet. This very column is a product of the Zero Click Internet (it was originally published as a thread on X). A year ago, I would have posted it as an article on my website. But if I had done that now, no one would have seen it. That's why the fuss over what's happening with WordPress seems a bit ridiculous. WordPress has no future in a Zero Click Internet. It's irrelevant. And this is just scratching the surface of the impact a Zero Click Internet will have. Own a web design business? Prepare to go out of business. Are you a graphic designer? Hopefully, you work for Google, because otherwise, your future is bleak. It's the end of how things are, and a shift into something else. Whether it's good or bad in the long run remains to be seen, but in the short term, we're going to witness upheaval on a scale few are prepared to handle.",
    "comments": [
      {
        "author": "082349872349872",
        "text": "Having seen <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41960914\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41960914</a> , how is this for a scale of internet interaction?<p>- creating (includes writing)<p>- critiquing<p>- searching (see also <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41955332\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41955332</a> )<p>- clicking<p>- scrolling<p>[I&#x27;m just waiting for &quot;sabbath mode&quot; internet, which generates content automagically enough that, beyond opening the app early friday evening, you don&#x27;t have to interact with it <i>at all</i>]",
        "time": "2024-10-27T13:27:10"
      }
    ],
    "description": "The internet is undergoing the biggest change since its inception. It's huge. And there is no going back. The web is changing into the Zero Click Internet,...",
    "document_uid": "f8838702cd",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961536",
    "title": "Ask HN: If the NSA offered free backups, would you use them?",
    "url": "https://news.ycombinator.com/item?id=41961536",
    "score": 1,
    "timestamp": "2024-10-27T12:02:51",
    "source": "Hacker News",
    "content": "So, I'm not the only one. ;-)I'm thinking something functionally identical to Backblaze, but with versions going back to the 1950s.I'd love to be able to go back and read all my emails and BBS messages, I'm sure they're all sitting in Utah, and I paid for them to be there, after all.I don't think that there's any use in arguing about encrypting before the backups, they'll break it no matter what.",
    "comments": [
      {
        "author": "mikewarot",
        "text": "So, I&#x27;m not the only one. ;-)<p>I&#x27;m thinking something functionally identical to Backblaze, but with versions going back to the 1950s.<p>I&#x27;d love to be able to go back and read all my emails and BBS messages, I&#x27;m sure they&#x27;re all sitting in Utah, and I paid for them to be there, after all.<p>I don&#x27;t think that there&#x27;s any use in arguing about encrypting before the backups, they&#x27;ll break it no matter what.",
        "time": "2024-10-27T12:20:18"
      },
      {
        "author": "dave4420",
        "text": "If it\u2019s via an NSA app I have to download, then hell no.<p>If they provide dumb file storage, so I can write my own script to compress and encrypt stuff before uploading via a standard protocol, then maybe? Other backup services are available, as they say, but they\u2019d either be paid or (probably) sketchier.",
        "time": "2024-10-27T12:50:37"
      },
      {
        "author": "nabla9",
        "text": "If authenticated encryption is allowed, absolutely.<p>NSA would probably be very secure against criminals and hackers. If authenticated encryption is not enough, NSA, other clouds services are on same category against big brother snooping.",
        "time": "2024-10-27T12:25:32"
      },
      {
        "author": "mikequinlan",
        "text": "Possibly. Can I be certain that<p>1. They will protect them so only I can access them?<p>2. Not secretly modify them?<p>3. I can receive a copy whenever I need one?",
        "time": "2024-10-27T12:06:30"
      },
      {
        "author": "chvid",
        "text": "Yes. But would encryption be allowed? If not, how would they know if the data was encrypted?",
        "time": "2024-10-27T12:17:44"
      },
      {
        "author": "WhereIsTheTruth",
        "text": "it already exist, they have various flavors<p>- Google Drive<p>- iCloud Drive<p>- OneDrive<p>Free source control too: GitHub<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CLOUD_Act\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CLOUD_Act</a>",
        "time": "2024-10-27T13:28:00"
      },
      {
        "author": "FrankWilhoit",
        "text": "Who says they don&#x27;t?  Who says I&#x27;m not?",
        "time": "2024-10-27T12:08:45"
      },
      {
        "author": "JSDevOps",
        "text": "Is it free? Yeah they will have some of the most talented people looking after the infrastructure, would be fine to backup non sensitive stuff. Pictures of my cat etc.",
        "time": "2024-10-27T12:14:38"
      }
    ],
    "description": "No description available.",
    "document_uid": "3b9e7cd6de",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961531",
    "title": "DuckDuckGo has an AI chat (beta)",
    "url": "https://duckduckgo.com/?q=DuckDuckGo+AI+Chat&ia=chat&duckai=1",
    "score": 3,
    "timestamp": "2024-10-27T12:01:55",
    "source": "Hacker News",
    "content": "You are being redirected to the non-JavaScript site.Click here if it doesn't happen automatically.",
    "comments": [
      {
        "author": "guld",
        "text": "Just saw that the duck has an AI chat (duck.ai). The best feature in my opinion is that you can select different chat models such as Llama and Claude to generate answers.<p>The UX itself reminds me of Bing Chat&#x2F;Copilot. \nDoes anyone know if it is powered by MSFT? Or does duckduckgo run its own AI stack?",
        "time": "2024-10-27T12:01:55"
      }
    ],
    "description": "DuckDuckGo. Privacy, Simplified.",
    "document_uid": "a48e7a2630",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961504",
    "title": "Living With Our Machine Sidekicks (2024-02)",
    "url": "https://taoofmac.com/space/blog/2024/02/24/1600",
    "score": 2,
    "timestamp": "2024-10-27T11:56:25",
    "source": "Hacker News",
    "content": "I\u2019ve been using GitHub Copilot (and various other similar things) for a long while now, and I think it\u2019s time to take stock and ponder their impact. It\u2019s All About The Use CaseFirst of all, since there is a 50% chance you have something to do with technology or code by landing here, I\u2019m not going to focus on code generation, because for me it\u2019s been\u2026 sub-optimal. The reasons for that are pretty simple: the kind of code I write isn\u2019t mainstream web apps or conventional back-end services (it\u2019s not even in conventional languages, or common frameworks, although I do use JavaScript, Python and C++ a fair bit), so for me it\u2019s been more of a \u201csmarter autocomplete\u201d than something that actually solves problems I have while coding. Disclaimer: I\u2019m a Microsoft employee, but I have no direct involvement with GitHub Copilot or Visual Studio Code (I just use them a lot), and I\u2019m not privy to any of the inner workings of either other than what I can glean from poking under the hood myself. And in case you haven\u2019t noticed the name of this site, I have my own opinions. LLMs haven\u2019t helped me in: Understanding the problem domain Structuring code Writing complex logic Debugging anything but the most trivial of issues Looking up documentation \u2026which is around 80% of what I end up doing. They have been useful in completing boilerplate code (for which a roundtrip network request that pings a GPU that costs about as much as a car is, arguably, overkill), and, more often than not, for those \u201cwhat is the name of the API call that does this?\u201d and \u201ccan I do this synchronously?\u201d kinds of discussions you\u2019d ordinarily have with a rubber duck and a search engine in tandem. I have a few funny stories about these scenarios that I can\u2019t write about yet, but the gist of things is that if you\u2019re an experienced programmer, LLMs will at best provide you with broad navigational awareness, and at worst lead you to bear traps. But if you\u2019re a beginner (or just need to quickly get up to speed on a popular language), I can see how they can be useful accelerators, especially if you\u2019re working on a project that has a lot of boilerplate or is based on a well-known framework. If you\u2019re just here for the development bit, the key takeaway is that they\u2019re nowhere good enough to a point where you can replace skilled developers. And if you\u2019re thinking of decreasing the number of developers, well, then, I have news for you: deep insight derived from experience is still irreplaceable, and you\u2019ll actually lose momentum if you try to rely only on LLMs as accelerators. People who survived the first AI winter and remember heuristics will get this. People who don\u2019t, well, they\u2019re in for a world of hurt. Parenthesis: Overbearing Corporate PoliciesThis should be a side note, but it\u2019s been on my mind so often that I think it belongs here: One thing that really annoys me is when you have a corporate policy that tries to avoid intellectual property issues by blocking public code from replies: Sorry, the response matched public code so it was blocked. Please rephrase your prompt. I get this every time I am trying to start or edit a project with a certain very popular (and dirt common) framework, because (guess what) their blueprints/samples are publicly available in umpteen forms. And yes, I get that IP is a risky proposition. But we certainly need better guardrails than overbearing ones that prevent people from using well-known, public domain pieces of code as accelerators\u2026 But this is a human generated problem, not a technical one, so let\u2019s get back to actual technological trade-offs. What About Non-Developers Then?This is actually what I have really been pondering the most, although it is really difficult to resist the allure of clever marketing tactics that claim massive productivity increases in\u2026 well, in Marketing, really. But I am getting ahead of myself. What I have been noticing when using LLMs as \u201csidekicks\u201d (because calling them \u201ccopilots\u201d is, quite honestly, too serious a moniker, and I actually used the Borland one) is their impact on three kinds of things: Harnessing drudgery like lists or tables of data (I have a lot of stuff in YAML files, whose format I picked because it is great for \u201cappend-only\u201d maintenance of data for which you may not know all the columns in advance). Trying to make sense of information (like summarizing or suggesting related content). Actual bona fide writing (as in creating original prose). Eliding The DrudgeryA marked quality of life improvement I can attribute to LLMs over the past year is that when I\u2019m jotting down useful projects or links to resources (of which this site has hundreds of pages, like, for instance, the AI one, or the Music one, or a programming language listing like Go or even Janet), maintaining the files for that in Visual Studio Code (with GitHub Copilot) is a breeze. A typical entry in one of those files looks like this: - url: https://github.com/foo/bar link: Project Name date: 2024-02-21 category: Frameworks notes: Yet another JavaScript framework What usually happens when I go over and append a resource using Visual Studio Code is that when I simply place the cursor at the bottom, it suggests - url: for me, which is always an auspicious start. I then hit TAB to accept, paste the URL and (more often than not) it completes the entire entry with pretty sane defaults, even (sometimes) down to the correct date and a description. As a nice bonus, if it\u2019s something that\u2019s been on the Internet for a while, the description is quite likely correct, too. Praise be quantization of all human knowledge, I guess. This kind of thing, albeit simple, is a huge time saver for note taking and references. Even if I have to edit the description substantially, if you take the example above and",
    "comments": [],
    "description": "I\u2019ve been using GitHub Copilot (and various other similar things) for a long while now, and I think it\u2019s time to take stock and ponder their...",
    "document_uid": "f9371b5ee6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961492",
    "title": "A Brief History of Defragging",
    "url": "https://eclecticlight.co/2024/10/05/a-brief-history-of-defragging/",
    "score": 1,
    "timestamp": "2024-10-27T11:53:38",
    "source": "Hacker News",
    "content": "When the first Macs with internal hard disks started shipping in 1987, they were revolutionary. Although they still used 800 KB floppy disks, here at last was up to 80 MB of reliable high-speed storage. It\u2019s worth reminding yourself of just how tiny those capacities are, and the fact that the largest of those hard disks contained one hundred times as much as each floppy disk. By the 1990s, with models like the Mac IIfx, internal hard disks had doubled in capacity, and reached as much as 12 GB at the end of the century. Over that period we discovered that hard disks also needed routine maintenance if they were to perform optimally, and all the best users started to defrag their hard disks. Consider a large library containing tens or even hundreds of thousands of books. Major reference works are often published in a series of volumes. When you need to consult several consecutive volumes of such a work, how they\u2019re stored is critical to the task. If someone has tucked each volume away in a different location within the stack, assembling those you need is going to take a long while. If all its volumes are kept in sequence on a single shelf, that\u2019s far quicker. That\u2019s why fragmentation of data has been so important in computer storage. The story of defragging on the Mac is perhaps best illustrated in the rise and fall of Coriolis Systems and iDefrag. Coriolis was started in 2004, initially to develop iPartition, a tool for non-destructive re-partitioning of HFS+ disks, but its founder Alastair Houghton was soon offering iDefrag, which became a popular defragging tool. This proved profitable until SSDs became more widespread and Apple released APFS in High Sierra, forcing Coriolis to shut down in 2019, when defragging Macs effectively ceased. All storage media, including memory, SSDs and rotating hard disks, can develop fragmentation, but most serious attention has been paid to the problem on hard disks. This is because of their electro-mechanical mechanism for seeking to locations on the spinning platter they use for storage. To read a fragmented file sequentially, the read-write head has to keep physically moving to new positions, which takes time and contributes to ageing of the mechanism and eventual failure. Although solid-state media can have slight overhead accessing disparate storage blocks sequentially, this isn\u2019t thought significant and attempts to address that invariably have greater disadvantages. Fragmentation on hard disks comes in three quite distinct forms: file data across most of the storage, file system metadata, and free space. Different strategies and products have been used to tackle each of those, with varying degrees of success. While few doubt the performance benefits achieved immediately after defragging each of those, little attention has been paid to demonstrating more lasting benefits, which remain more dubious. Manually defragging HFS+ hard disks was always a questionable activity, as Apple added background defragmentation to Mac OS X 10.2, released two years before Coriolis was even founded. By El Capitan and Sierra that built-in defragging was highly effective, and the need for manual defragging had almost certainly become a popular myth. Neither did many consider the adverse effects on hard disk longevity of those intense periods of disk activity. The second version of TechTool Pro in 1999 offered a simplified volume map for what it termed optimisation, offering the options to defragment only files, or the whole disk contents including free space. By the following year, TechTool Pro was paying greater attention to defragging file system metadata, here shown in white. This view was animated during the process of defragmentation, showing its progress in gathering together all the files and free space into contiguous areas. TechTool Pro is still developed and sold by MicroMat, and now in version 20. A similar approach was adopted by its competitor Speed Disk, here with even more categories of contents. By 2010, my preferred defragger was Drive Genius 3, shown here working on a 500 GB SATA hard disk, one of four in my desktop Mac; version 6 is still sold by Prosoft. One popular technique for defragmentation with systems like that was to keep one of its four internal disks empty, then periodically clone one of the other disks to that, and clone it back again. Alsoft\u2019s DiskWarrior is another popular maintenance tool, shown here in 2000. This doesn\u2019t perform conventional defragmentation, but restructures the trees used for file system metadata, and remains an essential tool for anyone maintaining HFS+ disks. Since switching to APFS on SSDs several years ago, I have missed defragging like a hole in the head.",
    "comments": [],
    "description": "As Mac hard disks grew larger, fragmentation of file data, file system metadata and free space slowed the disk down. Here's how we used to solve that.",
    "document_uid": "818ee3f8a6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961474",
    "title": "Beryllium OS, a Unix-like operating system for CircuitPython",
    "url": "https://github.com/beryllium-org/OS",
    "score": 1,
    "timestamp": "2024-10-27T11:49:26",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "Beryllium OS, a unix-like operating system for CircuitPython powered microcontrollers. (Formerly known as ljinux) - beryllium-org/OS",
    "document_uid": "5a6d25d15a",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961471",
    "title": "Stop Pretending to Do Domain-Driven Design",
    "url": "https://www.jamesmichaelhickey.com/stop-pretending-domain-driven-design/",
    "score": 12,
    "timestamp": "2024-10-27T11:48:51",
    "source": "Hacker News",
    "content": "Warning: This is an opinionated article and could be classified as a rant if you really want\u2026 The Trend There\u2019s a trend by software developers to approach domain-driven design as a collection of useful programming/coding patterns and abstractions: Entities Aggregates Repositories Factories Services Value Objects Bounded Contexts Domain Events Etc. I see this idea popularized on all the social platforms by content creator X software developer with 3 years of experience who is an expert on building distributed microservices using domain-driven design. They can tell you all about creating entities, aggregates, and repositories. And distributed microservices too \u2013 as a cherry on the top. The courses on popular learning platforms are the same. Here\u2019s part of a description from a popular learning platform\u2019s course on DDD: Structure your application around domains and entities. Establish aggregate classes and determine appropriate behaviors. Interact with your domains through events and commands. I Disagree I disagree with this code pattern-centric characterization of DDD. Here are a couple of questions to consider: Can I do domain-driven design without using tactical tools? Can I do domain-driven design without using strategic tools? The answer to 1 is yes. The answer to 2 is no. Tactical Tools Are Optional In Vaughn Vernon\u2019s \u201cRed Book\u201d he says: DDD is about discussion, listening, understanding, discovery and business value, all in an effort to centralize knowledge. If you aren\u2019t doing these things \u2013 you aren\u2019t doing DDD. Period. Vaughn also writes about a fictitious scenario of developers focused on these tactical tools \u2013 calling it \u201cDDD-Lite\u201c: In fact, they were really using what amounted to DDD-Lite, employing the tactical patterns mostly for a technical payoff. In other words, it doesn\u2019t matter how many entities, aggregates, factories, services, and repositories you have. Side note: This is like saying you \u201cdo TDD\u201d because you write unit tests and use assertions, mocks, stubs, etc. Sure, those things are useful to know. But TDD is not about individual tests. It\u2019s about using testing as a process of designing and building software. It\u2019s a higher-level plane of thinking. Strategic Patterns Are Essential The strategic aspects of DDD are about figuring out what parts of the system matter and which don\u2019t. For example, where should you focus your efforts if you are building a trading platform for a bank? Customer profile management? Authentication? Interoperability with external banking systems? Building a trading platform is going to be hard. But at face value, we all know this business will most likely not focus on the first two domains (profile management and authentication). Or maybe I should say should not focus\u2026 Why? Those two domains or systems don\u2019t deliver differentiated business value. They only support the overall system. Have you ever used a trading platform and really cared whether you could upload a profile image? But you do care if the trading platform miscalculates your trade and some of your money disappears into the ether. The Norm: Entitify & Servicify All The Things In my experience, many (most?) software developers use entities, aggregates, services, repositories and factories to build something basic like a profile management module for their software. I\u2019ll quote Vaughn\u2019s book again: Businesses regularly put too much effort into developing glorified database table editors. Without the correct tool selection, CRUD-based solutions treated elaborately are too expensive\u2026 if the choice is correct, it should save time and money. Look at the software business you work in. Are your secondary/supportive products: Developed simply using something like CRUD? Meeting their budgets? Developed quickly? Saving money and time? In my experience, this is not the norm. If the organization is \u201cdoing DDD\u201d the norm is using aggregates, entities, repositories, factories, etc. everywhere. Strategic Design As Protection From Change In a nutshell, DDD is about talking to the business experts who do the job. To the ones who do the real work you are digitizing. If you aren\u2019t doing that\u2026 Another way to think about DDD is that it tries to help us to answer the question: Can you predict which parts of your system & code will change most often? The implicit premise here: The parts of your system that deliver the most business value and differentiate your business from others are exactly the parts that are apt to change the most. Supportive modules & functionality like authentication or profile pictures are probably not the pieces of a trading platform that break and change the most, for example. Your business has to choose where to focus or it will never get things done. Making Reasonable Predictions This leads to other questions like: Which code modules should you not be awfully particular about? Which code modules should you be incredibly careful about? You could answer this question at the level of code objects like classes and functions. But this level of granularity makes it hard to predict where the most change will be. It\u2019s too narrow and in the weeds. It\u2019s like predicting that an individual stock will do well over the next 5 years. DDD answers these questions at a higher level. Bounded contexts and domains correspond to larger pieces of the business. You\u2019re making predictions about larger pieces. It\u2019s like investing in an index fund. You get a broader view of the overall market and can therefore make much more probable predictions. Here\u2019s where DDD practitioners should focus: What are the domains and different parts of the business? Which parts aren\u2019t going to change that much? Don\u2019t fuss too much about these. Maybe even buy an off-the-shelf solution. Which parts represent \u201cthe secret sauce\u201d of your business \u2013 the parts you expect to change a lot? If this domain is legitimately complex, then using tactical patterns might help. Employing advanced software architectural patterns, coding patterns, etc. might help if they fit the context. The Source Of The Problem Is it because software developers like to complicate things more than is necessary? Maybe. Is it because those overseeing the various business domains aren\u2019t technical and don\u2019t think about Conway\u2019s Law? Maybe. Because",
    "comments": [
      {
        "author": "jimberlage",
        "text": "In the past, I\u2019ve really wanted better strategies for making this work organizationally.<p>If you say, \u201cwe have CRUD apps and some event-driven, truly bespoke stuff,\u201d you have the problem of devs not wanting to work on the CRUD apps and only wanting the bespoke stuff.<p>Things I have tried<p>1. Hire seniors who are jaded by overengineering; they are happier to work on CRUD apps and will do things well.<p>This is my favorite strategy but there are not as many of these people.<p>2. Have contractors do CRUD and full-time employees work on core features.<p>This can create an uncomfortable culture split and if you\u2019re not confident in managing contractors, can mean delays or having the customer-facing stuff look sloppy.  I\u2019ve also found some contract shops are not set up well to do security-critical things that aren\u2019t a differentiator, like auth.<p>3. Pay for non-core stuff through vendors, and have employees focus on core things and a smaller set of non-differentiators.<p>This requires higher vendor spend but is probably the easiest to have a consistent culture and hiring setup.",
        "time": "2024-10-27T14:37:56"
      },
      {
        "author": "dondraper36",
        "text": "I really like the idea of bounded contexts and aggregates in theory. What I still don&#x27;t understand is how defining them can be feasible at the beginning of a project when you have too many unknowns (some of which are unknown unknowns).<p>I am a DDD noob (currently reading the book Domain Modeling Made Functional), but maybe someone can elaborate on how one&#x27;s supposed to make such decisions when there&#x27;s plenty of uncertainty.",
        "time": "2024-10-27T15:09:23"
      },
      {
        "author": "klabb3",
        "text": "I sympathize with the \u201crant\u201d. In the book I read about DDD it was very clear you should work side by side with the domain expert (client usually) to the point of agreeing on the same terminology (nouns &amp; verbs mostly). In order to have a shared mental model, of the business. This is helpful for both parties.<p>This factory-service-repository word salad sounds like a parallel fantasy based on preconceptions of a certain cohort of self-reinforcing \u201cbelievers\u201d. Fuzzy words get abused the most. And the more middle management, the faster the meaning gets drained from them. I guess DDD took off mostly in MS&#x2F;Enterprise environments, which would explain the butchering.",
        "time": "2024-10-27T13:58:31"
      }
    ],
    "description": "It doesn't matter how many entities, aggregates, factories, services, and repositories you have. That's not domain-driven design.",
    "document_uid": "59409c2ca9",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961431",
    "title": "I am my own legal department: the promise and peril of \"just go independent\"",
    "url": "https://www.citationneeded.news/i-am-my-own-legal-department/",
    "score": 5,
    "timestamp": "2024-10-27T11:43:11",
    "source": "Hacker News",
    "content": "I am my own legal department: the promise and peril of \u201cjust go independent\u201d Listen to a voiceover of this post, subscribe to the feed in your podcast app, or download the recording for later. When a major legacy news publication does something controversial, I sometimes hear my name. \u201cJust go independent! Do what Molly White does.\u201d You\u2019ve probably seen it too: advice to staff writers to \u201cjust go independent\u201d like Heather Cox Richardson, or Casey Newton, or those guys over at 404 Media. \u201cJust do a Substack! It\u2019s the future of journalism.\u201dWhen news broke yesterday that Washington Post owner Jeff Bezos had spiked the newspaper\u2019s planned endorsement of Kamala Harris, editor-at-large Robert Kagan resigned. Similar events transpired earlier this week at the Los Angeles Times, whose own billionaire owner nixed a Harris endorsement, prompting the resignation of the editor who drafted it. As staff members at those publications and elsewhere spoke out about their frustration and fury at the decisions by those who control their employers\u2019 purse strings, the familiar refrain began. \u201cWhy don\u2019t you just quit too? Just go independent.\u201dThe day before the Washington Post news broke, bleary-eyed at 7am as I scanned through my email inbox before getting out of bed to make some coffee, a subject line caught my eye: \u201cDMCA Takedown Notice\u201d.When I went to bed the night before, the plan for the next morning had been to fix a bug in my new presidential election spending tracker in Follow the Crypto, a page I urgently wanted to release as Election Day looms here in the United States. Now I knew those plans would have to wait until after I finished cosplaying as a lawyer, figuring out if and how to respond to some guy who\u2019d \u2014 as far as I can tell \u2014 been hired by the alleged operator of a multi-million dollar cryptocurrency pyramid scheme to file a fraudulent copyright claim in an attempt to force me to take down a Web3 is Going Just Great post I\u2019d already refused to delete.A week prior, I\u2019d received an email from an amateurish \u201creputation management company\u201d offering me $200 each to delete the story and the associated tweet. I replied to inform them that, while I\u2019m always happy to correct any errors, I do not remove posts simply because their subjects don\u2019t like them. \u201cI understand. You are right as such there are no errors,\u201d they replied, then upped their bribe to $500. I stopped responding, and assumed that was that. Instead, either the client or the \u201creputation management company\u201d decided to try a new tactic. Following a familiar playbook, they copied my Web3 is Going Great post to a free BlogSpot website, then backdated it by one day. The goal was to then abuse the notice and takedown process to claim that I had stolen their work, force my webhost to remove the post, and then hope that I was either intimidated by the legalese or too strapped for time to fight it out with my hosting provider. Unfortunately for them, \u201clegal department\u201d is only one of the many hats I wear as an independent writer and publisher, and \u201chosting provider\u201d is another. I informed them that no such infringement had occurred, and noted that there can be penalties for fraudulent copyright claims should they try to continue pursuing this tactic. I hope this is where they give up, and my legal department that contains no actual lawyers will have sufficed this time, as it has several other times in the past. Had I been a journalist at an outlet like the Post, this likely would never have required any of my time at all. Outlets like that have teams of actual lawyers who are in charge of handling copyright complaints. They also vet journalists\u2019 stories to ensure they are carefully written so as to minimize the chances of defamation lawsuits, and defend the publication and its journalists from any claims that come in anyway. They handle all of the many other types of legal concerns that impact journalists and publishers.As an independent writer and publisher, I am the legal team. I am the fact-checking department. I am the editorial staff. I am the one responsible for triple-checking every single statement I make in the type of original reporting that I know carries a serious risk of baseless but ruinously expensive litigation regularly used to silence journalists, critics, and whistleblowers. I am the one deciding if that risk is worth taking, or if I should just shut up and write about something less risky. I am the one who ultimately could be financially ruined by such a lawsuit. I am the one in charge of weighing whether I should spring for the type of insurance that is standard fare for big outlets to protect themselves and their staff, but often prohibitively expensive for independent writers. I am the one putting on even more hats: risk manager, insurance broker, business consultant, CPA.Occasionally I am reminded that many don\u2019t realize that independent publications like mine are often a one-man band: one person wearing many disguises. When I published my recent interview with Ryan Salame, I laughed when I saw a comment on social media: \u201cYour photo editor captured the spirit of the piece pretty well.\u201d I get comments like this a lot, more often when I make a mistake. \u201cWhoever edits your audio versions accidentally left in a retake.\u201d It\u2019s me, hi, I\u2019m the problem, it\u2019s me.It\u2019s not a silly assumption to make; after all, some independent writers do bring on outside help by hiring editors or research assistants or social media managers. It\u2019s a wise move, and maybe someday I will too. Maybe.But for now, Citation Needed is just one person with a tall stack of hats. The research team? Me, poring over FEC filings or the deluge of PACER alerts from dozens of court cases, so that I can be the one who first notices that Ryan Salame has formally declared his innocence after",
    "comments": [],
    "description": "Independent publishing is one important facet of the media ecosystem, and while I love it, I know it is not the path for everyone.",
    "document_uid": "26a8bff9d0",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961418",
    "title": "How to generate a QR code with Stable Diffusion",
    "url": "https://stable-diffusion-art.com/qr-code/",
    "score": 1,
    "timestamp": "2024-10-27T11:39:56",
    "source": "Hacker News",
    "content": "A recent Reddit post showcased a series of artistic QR codes created with Stable Diffusion. Those QR codes were generated with a custom-trained ControlNet model. Just like another day in the Stable Diffusion community, people have quickly figured out how to generate QR codes with Stable Diffusion WITHOUT a custom model. QR code, short for Quick Response code, is a common way to encode text or URL in a 2D image. You can typically use your phone\u2019s camera app to read the code. In this post, you will learn how to generate QR codes like these. We will study a few ways to generate QR codes with Stable Diffusion. Use a QR Code Control Model with text-to-image. Use the tile Control Model with image-to-image. Use a QR Code Control Model with image-to-image. Method 1 is easier to control, and the color comes out brighter. But not all prompts work. Method 2 allows a wider range of prompts but could have color issues. Method 3 produces image quality between methods 1 and 2. Software We will use AUTOMATIC1111 Stable Diffusion GUI to create QR codes. You can use this GUI on Google Colab, Windows, or Mac. You will need the ControlNet extension installed. Follow this tutorial to install. If you are using our Colab Notebook, simply select ControlNet at startup. Generating QR code You will first need a QR Code. To increase your chance of success, use a QR code that meets the following criteria. Use a high fault tolerance setting (30%). Have a white margin around the QR Code (the quiet zone). Use the most basic square fill with a black-and-white pattern. Avoid using generators that introduce a thin white line between black elements. We will use this QR Code generator in this tutorial. Step 1: Select the text type and enter the text for the QR code. Step 2: Set fault tolerance to 30%. Step 3: Press Generate. Step 4: Download the QR Code as a PNG file. Method 1: Generate a QR code with a QR control model in text-to-image This method requires you to download a ControlNet model trained for decorating QR codes. We have two choices. Step 1: Install the QR Code Control Model. We will use QR code Pattern v2. Download the ControlNet Model and put it in the following folder. stable-diffusion-webui\\models\\ControlNet Step 2: Enter the text-to-image setting Stable Diffusion checkpoint: Anything v3 The prompt is quite important to your success. Some prompts blend naturally with your QR Code. Prompt: Japanese painting, mountains, 1girl Negative Prompt: ugly, disfigured, low quality, blurry, nsfw Sampling Method: DPM++ 2M Karras Sampling Step: 20 Hires Fix: On \u2013 Upscaler: Latent. Upscale by 2. Denoising strength: 0.7 Width: 512 Height: 512 CFG Scale: 7 Step 3: Enter ControlNet Setting On the text2img page, expand the ControlNet section. You should see an empty image canvas under ControlNet Unit 0 > Single Image. Upload the QR code to the Image Canvas. Enable: Yes Pixel Perfect: Yes ControlNet Model: controlnetQRPatternQR_v2Sd15 Control Weight: 1.1 (This needs to be adjusted for each prompt) Control Mode: Balanced Resize Mode: Crop and Resize Step 4: Press Generate You should get 1024\u00d71024 beautiful QR codes like these: Method 2: Generate a QR code with the tile resample model in image-to-image This method starts with generating an image similar to the QR Code using img2img. But this is not enough to produce a valid QR Code. ControlNet is turned on during the sampling steps to imprint the QR code onto the image. Near the end of the sampling steps, ControlNet is turned off to improve the consistency of the image. In AUTOMATIC1111 WebUI, navigate to the Img2img page. Step 1: Select a checkpoint model. We will use GhostMix. Step 2: Enter a prompt and a negative prompt. The prompt is quite important to your success. Some prompts blend naturally with your QR Code. We will use the following prompt. a cubism painting of a town with a lot of houses in the snow with a sky background, Andreas Rocha, matte painting concept art, a detailed matte painting And the following negative prompt. ugly, disfigured, low quality, blurry, nsfw Step 3: Upload the QR code to the img2img canvas. Step 4: Enter the following image-to-image settings. Resize mode: Just resize Sampling method: DPM++2M Karras Sampling step: 50 Width: 768 Height: 768 CFG Scale: 7 Denoising strength: 0.75 Step 5: Upload the QR code to ControlNet\u2018s image canvas. Step 6: Enter the following ControlNet settings. Enable: Yes Control Type: Tile Preprocessor: tile_resample Model: control_xxx_tile Control Weight: 0.87 Starting Control Step: 0.23 Ending Control Step: 0.9 Step 7: Press Generate. Step 6: Check the QR code with your phone. Make sure to check with different sizes on the screen. Some tend to have issues when they are large. You won\u2019t get a functional QR Code with every single image. The success rate is about one in four. Generate more images and check for the keepers. Tips QR codes with shorter text have a higher success rate because the patterns are simpler. Not all QR codes work the same. Some could be marginally working and can only be read at a certain distance. Some prompts blend more naturally with QR codes. For example, the prompt for generating houses with snow on rooftops you saw previously blends well with QR codes simply because they share similar visual elements. The working parameters can be different for different models and prompts. You must adjust the following parameter slightly to blend the QR Code and the prompt well. Denoising strength: Decrease to have the initial composition follows the QR code more. But you will only see the QR code if you reduce it too much. It is typically set higher than 0.7. Control Weight: Decrease to show the prompt more. Starting Control Step: Increase to show the prompt more. Ending Control Step: Decrease to stop the ControlNet earlier so that the QR code and the image can blend more naturally. Method 3: Generate a QR code with",
    "comments": [],
    "description": "A recent Reddit post showcased a series of artistic QR codes created with Stable Diffusion. Those QR codes were generated with a custom-trained ControlNet",
    "document_uid": "c747392108",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961391",
    "title": "UK sleep experts say it's time to kill daylight saving for good",
    "url": "https://www.theregister.com/2024/10/27/abolish_daylight_saving_time/",
    "score": 20,
    "timestamp": "2024-10-27T11:34:15",
    "source": "Hacker News",
    "content": "The streets of Soho aren't alive with disco or bellbottoms anymore, so maybe it's time to ditch another '70s icon we should have outgrown by now, says the British Sleep Society (BSS). A group of researchers in the UK affiliated with the BSS published a paper this week calling for the permanent abolition of Daylight Saving Time (DST) and adherence to Greenwich Mean Time (GMT), in large part because modern evidence suggests having that extra hour of sunlight in the evenings is worse for our health than we thought back in the 1970s when the concept was all the rage in Europe. Not only does GMT more closely align with the natural day/light cycle in the UK, the boffins assert, but decades of research into sleep and circadian rhythms have been produced since DST was enacted that have yet to be considered. The human circadian rhythm, the 24-hour cycle our bodies go through, drives a lot about our health beyond sleep. It regulates hormone release, gene expression, metabolism, mood (who isn't grumpier when waking up in January?), and the like. In short, it's important. Messing with that rhythm by forcing ourselves out of bed earlier for several months out of the year can have lasting effects, the researchers said. According to their review of recent research, having light trigger our circadian rhythms in the mornings to wake us up is far more important than an extra hour of light in the evenings. In fact, contrary to the belief that an extra hour of light in the evenings is beneficial, it might actually cause health problems by, again, mucking about with the human body's understanding of what time it is and how we ought to feel about it. \"Disruption of the daily synchronization of our body clocks causes disturbances in our physiology and behavior \u2026 which leads to negative short and long-term physical and mental health outcomes,\" the authors said. That, and we've just plain fooled ourselves into thinking it benefits us in any real way. \"It is sometimes erroneously assumed that DST provides us with more sunlight, but, in fact, all we are doing is changing our behavior by moving our schedules forward by one hour,\" the researchers said. \"While this means there is an hour more sunlight after work/school, DST comes at an expense of one hour less sunlight before work/school, simply because we get up and travel to and from work/school one hour earlier.\" Of course, eliminating DST is easier said than done in the modern world where computers dictate everything from business services to transportation. Last-minute delays and advances of DST have triggered IT messes in various parts of the world in recent years, so let's hope for more advanced notice if the powers that be decide to take the sleep doctors' orders. And for the love of sleep, the researchers beg, don't spring forward permanently. \"Mornings are the time when our body clocks have the greatest need for light to stay in sync,\" said Dr Megan Crawford, lead author and senior lecturer in psychology at University of Strathclyde. \"At our latitudes there is simply no spare daylight to save during the winter months and given the choice between natural light in the morning and natural light in the afternoon, the scientific evidence favors light in the morning.\" \u00ae",
    "comments": [
      {
        "author": "rcarmo",
        "text": "This repeatedly comes up every year and yet politicians do absolutely nothing about it. The EU had a resolution it never enacted, and as a result I spend half the year off-kilter (waking up, having lunch and going to bed an hour off what feels \u201cnatural\u201d).<p>Worse, some local governments (like Portugal) opposed the resolution on the grounds that it would impact tourism, which just goes to show you how much they value their people\u2019s health\u2026",
        "time": "2024-10-27T13:25:02"
      },
      {
        "author": "jessekv",
        "text": "As a child, time changes were my first &quot;peak behind the curtain&quot; where I began to clue in to the artificiality of clocks, and later many other social constructs.<p>To me, the absurdity of time changes is a reminder of how much of our life is governed by strange conventions and practices from a bygone era.<p>I would like it if we stopped doing time changes, but until then, at some level I still enjoy observing this strange tradition.",
        "time": "2024-10-27T12:27:50"
      },
      {
        "author": "Sharlin",
        "text": "This was almost happening in the EU, with real momentum, but then came COVID and Russian invasion and it was buried under more pressing matters :&#x2F;",
        "time": "2024-10-27T12:24:46"
      },
      {
        "author": "mrtksn",
        "text": "In Turkey, they killed the daylight savings and people are hating it because it means waking up in the dark in the winter.<p>However It&#x27;s an active political topic, creates 1 hour difference with the European neighbors and the secular Turks believe that the government is doing it to separate them from Europe and bring them closer with the Arab neighbors(Which is somewhat plausible, the country&#x27;s population and economic activities are concentrated in the western part), which might be influencing the opinions on the matter.",
        "time": "2024-10-27T12:33:02"
      },
      {
        "author": "DamonHD",
        "text": "Today (fairly near Greenwich) we are back on sensible time, with solar noon at clock noon near enough.",
        "time": "2024-10-27T11:36:34"
      },
      {
        "author": "mytailorisrich",
        "text": "In Europe daylight saving started, depending on country I suppose, during WWI and then again after the 1973 oil crisis. This was to  save energy and it has been saving a lot of energy although apparently changes in technology and habits mean that savings are decreasing.<p>It&#x27;s that time year so plenty of articles on the topic. For example, read one saying that official stats in France show that daylight saving saved 440GWh in 2009.<p>So certainly not absurd.<p>With the climate crisis, net zero, EVs, etc it seems like a bad time to abolish it if it indeed does save significant amounts of energy.",
        "time": "2024-10-27T12:55:06"
      },
      {
        "author": "chiefalchemist",
        "text": "Not joking: I&#x27;m surprised this isn&#x27;t a campaign issue (in the USA). It would be an easy way to get attention, support, etc. And it wouldn&#x27;t be a fluff issue. There are real tangible benefits to getting rid of daylight savings.<p>That said we&#x27;re talking about an organization that can&#x27;t balance its own budget; that there&#x27;s a dedt ceiling crisis every few months. Why address something that&#x27;s easy to fix and the people want?",
        "time": "2024-10-27T12:00:17"
      },
      {
        "author": "downvotetruth",
        "text": "[flagged]",
        "time": "2024-10-27T12:20:09"
      }
    ],
    "description": "It turns out morning light beats evening rays for health benefits",
    "document_uid": "c88b7afec1",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961356",
    "title": "Some people with ADHD thrive in periods of stress, new study shows",
    "url": "https://www.theguardian.com/us-news/2024/oct/26/adhd-symptoms-high-stress",
    "score": 33,
    "timestamp": "2024-10-27T11:24:43",
    "source": "Hacker News",
    "content": "A recent study has revealed that some people with attention deficit hyperactivity disorder (ADHD) cope best during periods of high stress.Maggie Sibley, a clinical psychologist and psychiatry professor at the University of Washington and the study\u2019s lead author, initially set out to learn whether it is possible for adults to recover from ADHD. In an earlier study, published in 2022, she investigated a National Institute of Mental Health data set that tracked 600 patients with ADHD over 16 years, starting from childhood.\u201cWhat we found was this pattern of fluctuating ADHD, and most of the people that were getting better, they would then get back to ADHD again,\u201d she said.For the more recent study, published last week in the Journal of Clinical Psychiatry, she went back to that same data set to try and figure out what circumstances might lead to relief from ADHD symptoms.Sibley thought that ADHD patients would experience the most relief during periods of low stress. What she found was more counterintuitive.Her study identified three different groups of ADHD patients: those who experienced periods of apparent full recovery, those who experienced partial remission, and those whose ADHD symptoms remained steady over time.People who experienced temporary full recovery were most likely to experience it during times of \u201chigh environmental demand\u201d, or, put more simply, stress. Those who had periods of partial recovery were also more likely to have comorbid anxiety.Arij Alarachi, a psychology phD student at McMaster University who has researched ADHD and anxiety with St Joseph\u2019s hospital in Hamilton, Canada, says it makes sense that ADHD would respond differently to different circumstances.ADHD brains might not change that much, said Alarachi, but people can adapt their circumstances to better cope with their ADHD. As Sibley\u2019s study shows, though, even among people with ADHD, those strategies might look different, since \u201cADHD comes in a lot of different shapes and sizes,\u201d Alarachi added.\u201cADHD patients may do best when they have to rise to the occasion. And we see that on the micro level \u2026 deadlines [could feel] helpful, or when things are more urgent, you\u2019re able to be your most productive and hyperfocus,\u201d said Sibley.Although it\u2019s impossible to completely untangle how much this is a result of ADHD patients choosing to take on more stress when their symptoms are in check.Sara Vranes, who was diagnosed with ADHD at 36, relates to this idea. She said she sees her ability to hyperfocus under pressure as a \u201csuperpower\u201d. Vranes now works with homeless communities, but had 15 years of experience as a midwife and doula before that, and she says she was most calm in crisis.\u201cI don\u2019t want anyone to be hurt, but I was able to handle it because my brain just can hyperfocus. I could see everything clearly and see a process in my mind, and act on it in real time.\u201d During downtime, however, she\u2019s often anxious and can\u2019t focus.More than half of adults with ADHD also experience anxiety. But, Sibley\u2019s study shows this might not always be a bad thing.skip past newsletter promotionafter newsletter promotion\u201cWe call it a protective factor in ADHD,\u201d she said, explaining that multiple studies have found that children with ADHD and anxiety respond better to behavioral treatment, like cognitive behavioral therapy, than children who just have ADHD.Alarachi said that in her research, too, she\u2019s come across people with ADHD who say anxiety helps them keep impulsivity in check. They will say: \u201cMy anxiety [has] kind of helped me stop myself from maybe acting on some of those impulses, or it\u2019s kind of made me think about some of the consequences.\u201d\u201cThink about it like the gas and the brakes in a car, right? The ADHD might be the gas, and then the anxiety is putting the brakes on, like getting people to inhibit a little bit,\u201d said Sibley.Anxiety and impulsivity might be more extreme in people with ADHD, \u201cbut somehow they\u2019re canceling each other out in a way that kind of makes neither of those processes as problematic as they might be on their own, which is kind of an interesting concept\u201d, Sibley added.Alarachi and Sibley agree that people with ADHD should look within to figure out how best to relax and keep their anxiety to a reasonable level where it\u2019s useful. Vranes has a hard time just relaxing in front of the TV, but says playing phone games and watching TV at the same time can help stop her mind from wandering.Sibley has encountered ADHD patients who are most relaxed while exercising and socializing.\u201cI always tell people with ADHD, you have to learn to write your own owner\u2019s manual,\u201d Sibley said. \u201cSo you have to figure out, what is your brand of relaxation? What is your brand of decompressing?\u201d",
    "comments": [
      {
        "author": "zeta0134",
        "text": "This is not especially surprising. It&#x27;s not that I can&#x27;t focus. I can, for hours and hours and hours on end. The difficulty is <i>directing</i> that focus productively. High stress environments provide the necessary motivation, forcing my focus onto the thing demanding that attention more strongly than the usual cacophony of distractions. But high stress also leads to burnout; I can&#x27;t sustain that for months on end, so I have to cope in other ways most of the time. (Typically by altering my environment to reduce distractions.)",
        "time": "2024-10-27T12:13:25"
      },
      {
        "author": "TrackerFF",
        "text": "I wouldn&#x27;t say that I thrive, more that when the flight-or-fight instinct kicks in, I become extremely productive.<p>Prior to getting diagnosed and medicated, my life was pretty much long periods of nothing, with small bursts where I did everything.<p>Take college, for example. I was completely unable to partition a project into daily tasks. I&#x27;d get borderline anxiety from looking at the problem set, and then get distracted. It was only when the deadline started to get close, that my fight-or-flight instinct would set in. Then I&#x27;d sit up 1-2-3 days straight and work on the problem - not 100%, but small bursts of 100% focus and attention, before eventually getting distracted. I&#x27;d oscillate between those states.<p>To outsiders it will look completely ridiculous. You could literally be dealing with life-altering decisions, with the clock ticking down, and then you suddenly start looking at cute cat vids. Before shifting back and grinding away at the problem, but even more stressed.<p>Luckily medication has worked wonders for me. I&#x27;m able to just start on the tasks, and work consistently on them. Work and life now feels like a marathon, rather than a bunch of impossible sprints.",
        "time": "2024-10-27T12:32:19"
      },
      {
        "author": "al_borland",
        "text": "I don\u2019t know if I have ADHD (though I recently had some tests done and am waiting on results), but I found this relatable. If I\u2019m interested in something I will hyper focus on it all day. If I\u2019m not, and there is no deadline, I\u2019ll find it impossible to get started. As a deadline approaches (a real one, not something made up), a point is reached where I think I can get it done if I literally do nothing else, and then I can focus and get it done.<p>Deadlines that our made up, I know are made up, have no consequences, and the where the whole project seems pointless, do not work the same way. Those are still ignored in spite of the stress and external person might try to apply.<p>My old boss saw this pattern and if he gave me a project I didn\u2019t instantly latch onto and turn around, he\u2019d give it to someone else and I\u2019d get something I was actually interested in and spent most of my days working long hours because I\u2019d be hyper focused on whatever project I was working on and put in 12+ hour days every day.",
        "time": "2024-10-27T12:33:35"
      },
      {
        "author": "cja",
        "text": "This is obvious - stress is motivating.<p>Give me a deadline in six months and I&#x27;ll procrastinate for at least five of them, feeling terrible about it throughout. Give me an impossible deadline tomorrow and I become a productivity machine, with no time for anxious overthinking.",
        "time": "2024-10-27T12:43:50"
      },
      {
        "author": "viraptor",
        "text": "It&#x27;s weird when this is described as surprising. I mean I&#x27;m glad that precise research is being done. But this topic has memes about it. We know. It&#x27;s on the wiki <a href=\"https:&#x2F;&#x2F;romankogan.net&#x2F;adhd&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;romankogan.net&#x2F;adhd&#x2F;</a>",
        "time": "2024-10-27T12:47:49"
      },
      {
        "author": "ycombinete",
        "text": "Seeing hyperfocus described as a superpower makes me want to flip a table over.",
        "time": "2024-10-27T12:30:05"
      },
      {
        "author": "dgfitz",
        "text": "I was today years old when I learned I have ADHD.",
        "time": "2024-10-27T12:38:28"
      },
      {
        "author": "skywhopper",
        "text": "&gt; Sibley thought that ADHD patients would experience\n&gt; the most relief during periods of low stress. What\n&gt; she found was more counterintuitive.<p>Did this researcher not talk to anyone with ADHD before starting this project?",
        "time": "2024-10-27T12:15:00"
      },
      {
        "author": "Uptrenda",
        "text": "Isn&#x27;t this just another way of saying their environment wasn&#x27;t boring though?",
        "time": "2024-10-27T12:42:44"
      }
    ],
    "description": "Patients responded well in times of \u2018high environment demand\u2019 because sense of urgency led to hyperfocus",
    "document_uid": "a25eec2441",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41966224",
    "title": "Scientists Were Wrong: Plants Absorb 31% More CO2 Than Previously Thought",
    "url": "https://scitechdaily.com/scientists-were-wrong-plants-absorb-31-more-co2-than-previously-thought/",
    "score": 1,
    "timestamp": "2024-10-27T23:34:04",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "28aa8930b6",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966193",
    "title": "Boeing could offload space business",
    "url": "https://www.rt.com/news/606525-boeing-space-business-sale/",
    "score": 2,
    "timestamp": "2024-10-27T23:28:00",
    "source": "Hacker News",
    "content": "Boeing could offload space business",
    "comments": [],
    "description": "Error fetching meta description: [Errno -2] Name or service not known",
    "document_uid": "7d37bd209b",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966185",
    "title": "Dropcap Generation with AI",
    "url": "https://gwern.net/dropcap",
    "score": 1,
    "timestamp": "2024-10-27T23:25:44",
    "source": "Hacker News",
    "content": "Midjourneyv5 prompts: Capital letter T in simple blackletter, monochrome, abstract, German, machine clockwork, simplistic vector art, typography, Dave gibbons, robotic expressionism, mid-century illustration, bold use of line, autopunk, cyberpunk, steampunk --no intricate complex, detailed, color, red, yellow Capital letter T in simple blackletter, monochrome, abstract, German, machine, simplistic, vector art, typography, mid-century illustration, bold use of line, autopunk, cyberpunk, steampunk --no intricate complex, detailed, color, red, yellow Capital letter T in simple fraktur blackletter, artificial intelligence, neural network, deep learning, human brain, perceptron, Transformer, German, linocut Capital letter T in simple fraktur blackletter, artificial intelligence, neural network, deep learning, brain, German, Gothic, monochrome, linocut Capital letter T in simple fraktur blackletter, German, Gothic, monochrome, linocut, vector, neural net, circuitboard, AI, SF Capital letter 'T': a black and white drawing of a letter, high contrast, arabesque/scroll, striking simple design, art deco 1969, precisionist lines and detail-oriented, quadratura, bold lettering, linocut Capital letter 'T': a black and white drawing of a letter, simple abstract design, calligraphic arabesque/scroll, renaissance, art deco 1969, precisionist lines and detail-oriented, quadratura, sabattier filter, bold lettering, simplified woodblock, linocut Capital letter 'T': a black and white drawing of a decorative fancy ornamental letter, simple abstract design, calligraphic arabesque/scroll, renaissance, art deco 1969, precisionist lines and detail-oriented, quadratura, sabattier filter, bold, woodblock, linocut Capital letter 'T': a black and white drawing of a decorative fancy letter, arabesque/scroll, quadratura, paper cut-outs, art nouveau, precisionist lines and detail-oriented, quadratura, sabattier filter, bold lettering, high resolution Capital letter T, initial, dropcap, Fraktur, yinit, blackletter, monochrome, black and white, vector art, simplified outline A minimalist black-and-white dropcap letter 'D' intertwined with a stylized neural network pattern, encapsulating the theme of artificial intelligence. An elegant capital letter 'D' in black and white, with a subtle background of interconnected nodes and lines representing a machine learning model. \"A sleek and modern dropcap letter 'D' with a digital circuit design, reflecting the concept of generative AI, in a monochrome palette.\" \"The letter 'D' designed as if composed of tiny, pixelated bits, symbolizing digital data processing, in stark black and white contrast.\" \"A dropcap letter 'D' where the inner spaces are filled with a binary code pattern, evoking the theme of computing and AI, in black and white.\" \"A sleek dropcap letter 'D' with a shadow of binary code cascading down the page, symbolizing data flow in AI, all presented in a monochromatic scheme.\" \"A capital letter 'D' designed with a fluid, organic network of neurons, capturing the dynamism of a neural network in a black-and-white color palette.\" \"A bold dropcap letter 'D' merged with an abstract representation of a computer chip's intricate pathways, emphasizing the hardware aspect of AI, in grayscale tones.\" \"A minimalist 'D' where the contours are made of a continuous line that loops and winds like an algorithm's path, depicted in high-contrast black and white.\" \"The letter 'D' portrayed with a matrix of tiny, scattered pixels that seem to be self-assembling, reflecting the self-learning aspect of AI, executed in stark black-and-white.\" \"A bold dropcap letter 'D' with a clear silhouette of a neural network, stylized with broad strokes for visibility at small sizes, in black and white.\" \"A capital letter 'D' incorporating a simple, stylized digital circuit design, with visible lines and nodes for clarity when scaled down, in monochrome.\" \"The letter 'D' with a prominent binary code pattern inside its form, designed with contrasting black and white areas for legibility at 150x150px.\" \"A minimalist 'D' designed with a single, thick winding line that suggests an algorithm's path, in a high-contrast black-and-white style suitable for small dimensions.\" \"A dropcap 'D' in a chunky pixel art style, resembling the building blocks of digital imagery, easily discernible when resized to a small scale, in grayscale.\" A minimalist 'D' designed with a single, thick winding line that suggests an algorithm's path, in a high-contrast black-and-white style suitable for small dimensions. The letter 'D' should be clearly visible and the winding line should be thick enough to be discernible even when scaled down. The overall design should emphasize simplicity and clarity, with strong contrast between black and white to ensure visibility at a small scale. A minimalist capital letter 'I' with a motif that mimics a vertical data stream, represented in high-contrast black-and-white suitable for small dimensions. A minimalist capital letter 'H' featuring a design that resembles a digital bridge structure, in high-contrast black-and-white suitable for small dimensions. A minimalist capital letter 'G' with an abstract design of a simplified chip-like pattern, in high-contrast black-and-white suitable for small dimensions. A minimalist capital letter 'F' with a design inspired by circuit board traces, bold and readable, in high-contrast black-and-white suitable for small dimensions. A minimalist capital letter 'E' crafted with bold, digital block patterns, suggestive of pixelation, in high-contrast black-and-white suitable for small dimensions. A minimalist capital letter 'C' with a motif of stylized digital waveforms, representing data transmission, in high-contrast black-and-white suitable for small dimensions. A minimalist capital letter 'B' with a design featuring clean lines and geometric shapes that imply a computational theme, in a high-contrast black-and-white style suitable for small dimensions. A minimalist capital letter 'A' designed with a thick, abstract line suggesting a digital flow, in high-contrast black-and-white style suitable for small dimensions. A minimalist capital letter 'D' designed with a single, thick winding line that suggests an algorithm's path, in a high-contrast black-and-white style suitable for small dimensions.",
    "comments": [],
    "description": "We develop AI image generation workflows for webpage dropcap typography, creating PNGs &amp; SVGs using image generators. As demos, we create new Gwern.net logos and several custom FLOSS dropcap sets, including c\u200bats, Gene\u200b Wolfe horror fiction, and neural-net-inspired dropcaps.",
    "document_uid": "c763eb2208",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966180",
    "title": "Bulma CSS Framework",
    "url": "https://bulma.io/",
    "score": 1,
    "timestamp": "2024-10-27T23:24:19",
    "source": "Hacker News",
    "content": "Just dropped for my next project PureCSS \u2013 which I love \u2013 to try #Bulma by @jgthms \ud83e\udd1d I'm already 100% in \ud83d\udc9a with it.So many classes. Super cool #UI elements. A complete documentation. #OpensourceI mean, if you didn't try Bulma #css yet, check it out \ud83d\udc49 bulma.io",
    "comments": [],
    "description": "Bulma is a free, open source CSS framework based on Flexbox and built with Sass. It's 100% responsive, fully modular, and available for free.",
    "document_uid": "95e2ac4ac3",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966127",
    "title": "Data-driven fingerprint nanoelectromechanical mass spectrometry",
    "url": "https://www.nature.com/articles/s41467-024-51733-8",
    "score": 1,
    "timestamp": "2024-10-27T23:16:11",
    "source": "Hacker News",
    "content": "The fingerprint approach is validated using experimental measurements on two current NEMS mass spectrometry devices: (1) a doubly-clamped elastic beam operating in vacuum, and (2) a suspended microchannel resonator (SMR) for buoyant mass measurements in liquid. The purpose is to demonstrate and benchmark the fingerprint approach on actual experimental data. Nominally identical particles are used in both the learning and measurement phases so that the measured masses are implicitly calibrated; the use of different particles would require independent calibration of their masses and generate additional uncertainty in this benchmark study.Doubly-clamped beam measurements in vacuumWe first apply the fingerprint approach to measure the mass of individual proteins physisorbed onto the surface of a doubly-clamped NEMS beam in a high vacuum. These macromolecules are of identical (fixed) mass to within a degree of high precision; modulated only by the binding of hydrogen, water molecules etc. A hybrid Orbitrap-NEMS system illustrated in Fig. 3a is used to perform single-molecule nanomechanical mass measurements of E. coli GroEL chaperonin, a noncovalent complex consisting of 14 identical subunits; its theoretical molecular mass is 800.7664 kDa for which independent measurements have confirmed a value of 800.7822 \u00b1 0.0236 (SD) kDa25,26,27. GroEL is pre-selected using the quadrupoles of the orbitrap system, ensuring only intact GroEL molecules are delivered to the NEMS. A 20-device NEMS array of doubly-clamped beams (Fig. 3b) is used to localize the focal point of the ion beam. Subsequently, the smallest NEMS device in the array with the best mass resolution (length of 7 \u00b5m) is operated in isolation. Details concerning the operation and fabrication of this array are provided in refs. 25,26,27,28.Fig. 3: Mass measurements of GroEL molecules with a doubly-clamped NEMS beam in high vacuum.a Architecture of the Hybrid Q Exactive-NEMS System that delivers intact proteins to the Orbitrap chamber for analysis of mass-to-charge ratio and then onto the NEMS for single molecule analysis. b SEM image of a 20-device array of doubly-clamped beams showing their metallization layers, AlSi (colorized in yellow), used to interconnect the electrical connections of each resonator. c As GroEL molecules physisorb to a single NEMS resonator, the resonant frequency of each tracked flexural eigenmode abruptly shifts. Sub-figures a\u2013c taken from ref. 26. d All 72 measured fingerprints of the first two eigenmodes in their configuration space: [open circles] 24 fingerprints in the learning phase; [closed circles] 48 fingerprints in the measurement phase; [triangles] 2 fingerprints that are outliers of the Dohn method; [solid line] best fit to the E-B model; [shaded region] exclusion zone of E-B model where the Dohn method does not apply. e Measured mass of 48 particles using: [blue dots] the measurement phase of the fingerprint approach; [orange dots] the Dohn method; [triangles] the Dohn method which does not give a converged solution. Error bars refer to 95% confidence intervals.Individual molecular adsorption events of intact GroEL molecules abruptly shift the resonant frequencies of the first two flexural eigenmodes of the smallest NEMS device; see experimental data in Fig. 3c. The tracked frequency data collected for each flexural eigenmode are time series, with jump events due to molecular adsorption. These jump events are detected and analyzed using a statistical algorithm described in ref. 29. This produces a dataset consisting of pairs of frequency shifts (for flexural eigenmodes 1 and 2 of the device) for each molecular adsorption event, each of which is a fingerprint.The fingerprint approach is used to analyze this measured fingerprint dataset to recover the mass of each molecule. The result is compared to a conventional mode-shape-based method reported by Dohn et al. 18 (termed, the \u201cDohn method\u201d), which fits the relative frequency shifts of multiple eigenmodes\u2014measured at a single particle position\u2014to E-B theory using a weighted least-squares approach. Uncertainty in the mass measurements is reported, allowing for a robust comparison. In the learning phase, the fingerprint database is generated from the first 24 entries of the overall frequency shift dataset. The remaining 48 entries in the frequency shift dataset are then analyzed in the measurement phase of the fingerprint approach, to determine the masses of their corresponding GroEL molecules. The Dohn method is applied directly to each entry of the fingerprint database, with the median of the resulting 24 measured masses used as the mass reference in the measurement phase.Figure 3d shows a plot of all fingerprints, for the learning and measurement phases, in their configuration space. Figure 3d also shows the exclusion zone of E-B theory which is not accessible by the Dohn method, i.e., no particle mass can be recovered from these positions in the configuration space. In contrast, the fingerprint approach has no such limitation and naturally handles this situation (which may naturally arise due to frequency noise). Comparison of the measured masses of each of the above-described 48 GroEL molecules using the fingerprint approach and the Dohn method is reported in Fig. 3e. Excellent agreement is observed using these independent approaches. This constitutes an experimental validation of the proposed fingerprint approach for a widely-used NEMS mass spectrometry set up: physisorption of a small analyte onto the surface of a NEMS device, showing that the fingerprint approach can be used with confidence.Next, we apply the fingerprint approach to another experimental configuration, that has established capability in performing highly sensitive measurements in liquid.Suspended microchannel resonator measurements in liquidThe suspended microchannel resonator (SMR) consists of a cantilevered beam with a microfluidic channel embedded in its interior, through which an analyte immersed in liquid can flow21,30,31,32; see Fig. 4a. As the analyte passes through the microfluidic channel of the device, the resonant frequencies of its multiple flexural eigenmodes are simultaneously recorded in real-time; see Supplementary Information Section E. Because the analyte is immersed in liquid, the resonant-frequency time series depend on the buoyant mass of the analyte21,30,31,32. Experimental data is taken from ref. 33. The data consists of frequency-shift time series as a single NIST-tracible polystyrene particle (ThermoFisher 4016 A) passes through the SMR, for flexural eigenmodes 2 to 6. A total of 341 individual polystyrene particles, and hence time series, are used.Fig.",
    "comments": [],
    "description": "Fingerprint analysis is a ubiquitous tool for pattern recognition with applications spanning from geolocation and DNA analysis to facial recognition and forensic identification. Central to its utility is the ability to provide accurate identification without an a priori mathematical model for the pattern. We report a data-driven fingerprint approach for nanoelectromechanical systems mass spectrometry that enables mass measurements of particles and molecules using complex, uncharacterized nanoelectromechanical devices of arbitrary specification. Nanoelectromechanical systems mass spectrometry is based on the frequency shifts of the nanoelectromechanical device vibrational modes that are induced by analyte adsorption. The sequence of frequency shifts constitutes a fingerprint of this adsorption, which is directly amenable to pattern matching. Two current requirements of nanoelectromechanical-based mass spectrometry are: (1) a priori knowledge or measurement of the device mode-shapes, and (2) a mode-shape-based model that connects the induced modal frequency shifts to mass adsorption. This may not be possible for advanced nanoelectromechanical devices with three-dimensional mode-shapes and nanometer-sized features. The advance reported here eliminates this impediment, thereby allowing device designs of arbitrary specification and size to be employed. This enables the use of advanced nanoelectromechanical devices with complex vibrational modes, which offer unprecedented prospects for attaining the ultimate detection limits of nanoelectromechanical mass spectrometry. The authors report a data-driven approach that enables mass spectrometry using any nanomechanical device without knowledge of its vibrational modes.",
    "document_uid": "93d70da940",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966125",
    "title": "Google is reportedly developing \u2018Jarvis\u2019 AI that could take over your browser",
    "url": "https://www.theverge.com/2024/10/26/24280431/google-project-jarvis-ai-system-computer-using-agent",
    "score": 1,
    "timestamp": "2024-10-27T23:15:54",
    "source": "Hacker News",
    "content": "Google could preview its own take on Rabbit\u2019s large action model concept as soon as December, reports The Information. \u201cProject Jarvis,\u201d as it\u2019s reportedly codenamed, would carry tasks out for users, including \u201cgathering research, purchasing a product, or booking a flight,\u201d according to three people the outlet spoke with who have direct knowledge of the project.Powered by a future version of Google\u2019s Gemini, Jarvis reportedly only works with a web browser (it\u2019s tuned specifically for Chrome). The tool is aimed at helping people \u201cautomate everyday, web-based tasks\u201d by taking and interpreting screenshots and then clicking buttons or entering text, The Information writes. In its current state, it apparently takes \u201ca few seconds\u201d between actions.The biggest AI companies are all working on models that do things like what The Information is describing. Microsoft\u2019s Copilot Vision will let you talk with it about webpages you\u2019re viewing. Apple Intelligence is expected to be aware of what\u2019s on your screen and do things for you across multiple apps at some point in the next year. Anthropic debuted a \u201ccumbersome and error-prone\u201d Claude beta update that can use a computer for you, and OpenAI is reportedly working on a version of that, too.The Information cautions that Google\u2019s plan to show Jarvis off in December is subject to change. The company is reportedly considering releasing it to some small number of testers to find and help the company work out bugs.",
    "comments": [
      {
        "author": "gnabgib",
        "text": "Discussion (50 points, 1 day ago, 35 comments) <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41958642\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41958642</a>",
        "time": "2024-10-27T23:21:53"
      }
    ],
    "description": "Google may preview \u201cProject Jarvis,\u201d an AI tool that would carry tasks out in web browsers on behalf of users, in December.",
    "document_uid": "4aceba3487",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966114",
    "title": "Ask HN: What Are You Working On? (October 2024)",
    "url": "https://news.ycombinator.com/item?id=41966114",
    "score": 4,
    "timestamp": "2024-10-27T23:14:45",
    "source": "Hacker News",
    "content": "What are you working on?  Any new ideas that you&#x27;re thinking about?",
    "comments": [
      {
        "author": "candiddevmike",
        "text": "Knitting hats.  The fall&#x2F;winter is when I bury myself in knitting projects to fight seasonal depression and cabin fever.  Audiobook + knitting == bliss.<p>The work most of us do isn&#x27;t tangible.  You have nothing to &quot;prove&quot; that you made something.  Doing something where I create something in meat space is really rewarding.",
        "time": "2024-10-27T23:33:01"
      },
      {
        "author": "dang",
        "text": "This is an ongoing experiment that david927 has been helping with (thanks!). The two previous in the sequence were:<p><i>Ask HN: What are you working on (September 2024)?</i> - <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41690087\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41690087</a> - Sept 2024 (1041 comments)<p><i>Ask HN: What are you working on (August 2024)?</i> - <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41342017\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41342017</a> - Aug 2024 (1424 comments)<p>One thing I&#x27;d like to hone in on is that these threads aren&#x27;t intended for promotion, but rather for the just-because sort of project, driven by idle interest or weird obsession\u2014the sort of thing people might spend their free time on.<p>I&#x27;m not sure yet what the official &quot;rule&quot; should be (if any), but if you&#x27;re working on a startup or have had attention via Show HN, maybe abstain from these discussions? It wouldn&#x27;t be good for the thread to get taken over by things HN already has a place for.",
        "time": "2024-10-27T23:28:42"
      },
      {
        "author": "seanwilson",
        "text": "<a href=\"https:&#x2F;&#x2F;www.inclusivecolors.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.inclusivecolors.com&#x2F;</a><p>It&#x27;s an accessible color palette creator, to make Tailwind-style palettes of multiple swatches, and check your colors have sufficient WCAG&#x2F;ACPA color contrast against a live mockup. You can export for use with Tailwind, CSS, Figma, and Adobe.<p>It&#x27;s really only usable on desktop right now but I&#x27;d love any feedback good or bad on if it&#x27;s useful and what to work on next!<p>Some tips:<p>- Look in the &quot;Load examples&quot; menu in the top-left to compare the colors from Tailwind, IBM Carbon and United States Web Design System.<p>- Look in the &quot;contrast&quot; menu to see how WCAG contrast checks compares against APCA when &quot;vs black&#x2F;white&quot; is turned on. WCAG has known inaccuracies. APCA is the candidate contrast method for WCAG 3 that&#x27;s meant to improve on this.",
        "time": "2024-10-27T23:26:55"
      },
      {
        "author": "johnea",
        "text": "I&#x27;m working on not working!<p>Isn&#x27;t this what everyone is working on?",
        "time": "2024-10-27T23:29:01"
      }
    ],
    "description": "No description available.",
    "document_uid": "6881f89d5f",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966110",
    "title": "Fog ransomware targets SonicWall VPNs to breach corporate networks",
    "url": "https://www.bleepingcomputer.com/news/security/fog-ransomware-targets-sonicwall-vpns-to-breach-corporate-networks/",
    "score": 1,
    "timestamp": "2024-10-27T23:14:16",
    "source": "Hacker News",
    "content": "Fog and Akira ransomware operators are increasingly breaching corporate networks through SonicWall VPN accounts, with the threat actors believed to be exploiting CVE-2024-40766, a critical SSL VPN access control flaw. SonicWall fixed the SonicOS flaw in late August 2024, and roughly a week later, it warned that it was already under active exploitation. At the same time, Arctic Wolf security researchers reported seeing Akira ransomware affiliates leveraging the flaw to gain initial access to victim networks. A new report by Arctic Wolf warns that Akira and the Fog ransomware operation have conducted at least 30 intrusions that all started with remote access to a network through SonicWall VPN accounts. Of these cases, 75% are linked to Akira, with the rest attributed to Fog ransomware operations. Interestingly, the two threat groups appear to share infrastructure, which shows the continuation of an unofficial collaboration between the two, as previously documented by Sophos. While the researchers aren't 100% positive the flaw was used in all cases, all of the breached endpoints were vulnerable to it, running an older, unpatched version. In most cases, the time from intrusion to data encryption was short, at about ten hours, even reaching 1.5-2 hours on the quickest occasions. In many of these attacks, the threat actors accessed the endpoint via VPN/VPS, obfuscating their real IP addresses. Arctic Wolf notes that apart from operating unpatched endpoints, compromised organizations did not appear to have enabled multi-factor authentication on the compromised SSL VPN accounts and run their services on the default port 4433. \"In intrusions where firewall logs were captured, message event ID 238 (WAN zone remote user login allowed) or message event ID 1080 (SSL VPN zone remote user login allowed) were observed,\" explains Artic Wolf. \"Following one of these messages, there were several SSL VPN INFO log messages (event ID 1079) indicating that login and IP assignment had completed successfully.\" In the subsequent stages, the threat actors engaged in rapid encryption attacks targeting mainly virtual machines and their backups. Data theft from breached systems involved documents and proprietary software, but the threat actors didn't bother with files that were older than six months, or 30 months old for more sensitive files. Launched in May 2024, Fog ransomware is a growing operation whose affiliates tend to use compromised VPN credentials for initial access. Akira, a far more established player in the ransomware space, has recently had Tor website access problems, as observed by BleepingComputer, but those are gradually returning online now.",
    "comments": [],
    "description": "Fog and Akira ransomware operators have increased their exploitation efforts of CVE-2024-40766, a critical access control flaw that allows unauthorized access to resources on the SSL VPN feature of SonicWall SonicOS firewalls.",
    "document_uid": "35708e1d75",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966102",
    "title": "New UI and UX patterns in Gen AI powered products",
    "url": "https://substack.com/@abebasu/p-150379384",
    "score": 2,
    "timestamp": "2024-10-27T23:13:41",
    "source": "Hacker News",
    "content": "New UI and UX patterns in Gen AI powered products",
    "comments": [
      {
        "author": "abe94",
        "text": "As we are considering adding AI features to our product, I wanted to do a quick overview of new UI&#x2F;UX patterns we&#x27;ve been seeing so wrote my thoughts here!",
        "time": "2024-10-27T23:13:41"
      }
    ],
    "description": "New programming paradigm, new UX?",
    "document_uid": "5936aa8b80",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966091",
    "title": "That's not an abstraction, that's just a layer of indirection",
    "url": "https://fhur.me/posts/2024/thats-not-an-abstraction",
    "score": 1,
    "timestamp": "2024-10-27T23:12:05",
    "source": "Hacker News",
    "content": "That's Not an Abstraction, That's Just a Layer of Indirection If you've ever worked on refactoring or improving performance in a software system, you've probably run into a particular frustration: abstraction-heavy codebases. What looks like neatly organized and modularized code often reveals itself as a labyrinth, with layers upon layers of indirection. The performance is sluggish, debugging is a nightmare, and your CPU seems to be spending more time running abstractions than solving the actual problem. This leads us to an important realization: not all abstractions are created equal. In fact, many are not abstractions at all\u2014they're just thin veneers, layers of indirection that add complexity without adding real value. So what makes a good abstraction? An abstraction is only as good as its ability to hide the complexity of what lies underneath. Think of a truly great abstraction, like TCP. TCP helps us pretend that we have a reliable communication channel, even though it's built on top of an unreliable protocol, IP. It takes on the complexity of error correction, retransmission, and packet sequencing so that we don't have to. And it does such a good job that, as developers, we very rarely have to peek into its inner workings. When was the last time you had to debug TCP at the level of packets? For most of us, the answer is never. That\u2019s the sign of a great abstraction. It allows us to operate as if the underlying complexity simply doesn't exist. We take advantage of the benefits, while the abstraction keeps the hard stuff out of sight, out of mind. The opposite of abstraction But what about bad abstractions\u2014or perhaps more accurately, what about layers of indirection that masquerade as abstractions? These \"abstractions\" don\u2019t hide any complexity: they often just add a layer whose meaning is derived entirely from the thing it's supposed to be abstracting. Think of a thin wrapper over a function, one that adds no behavior but adds an extra layer to navigate. You've surely encountered these\u2014classes, methods, or interfaces that merely pass data around, making the system more difficult to trace, debug, and understand. These aren't abstractions; they're just layers of indirection. The problem with layers of indirection is that they add cognitive overhead. They\u2019re often justified under the guise of flexibility or modularity, but in practice, they rarely end up delivering those benefits. Instead, they make the codebase more complex and more challenging to work with\u2014especially when it comes time to squeeze out more performance or fix a bug. The real cost of abstractions We like to pretend abstractions are free. We casually add another interface, another wrapper, and before we know it, we\u2019ve got a whole stack of them. This kind of thinking ignores a fundamental truth: abstractions have costs. They add complexity, and often, they add performance penalties too. Abstractions are the enemy of performance. The more layers you add, the further you get from the underlying metal. Optimizing code becomes an exercise in peeling back layer after layer until you finally get to the real work. Each layer represents a mental and computational burden. It takes longer to understand what's going on, longer to find the code that matters, and longer for the machine to execute the actual business logic. Abstractions are also the enemy of simplicity. Each new abstraction is supposed to make things simpler\u2014that\u2019s the promise, right? But the reality is that each layer adds its own rules, its own interfaces, and its own potential for failure. Instead of simplifying, these abstractions pile on the complexity, making systems harder to understand, maintain, and extend. All abstractions leak There\u2019s a well-known saying: \"All abstractions leak.\" It\u2019s true. No matter how good the abstraction, eventually, you\u2019ll run into situations where you need to understand the underlying implementation details. This leakage might be subtle\u2014like when you\u2019re trying to understand the performance characteristics (what\u2019s the big O complexity here?)\u2014or it could be more blatant, requiring you to dive deep into debugging to figure out why something isn\u2019t working as expected. A good abstraction minimizes these situations; a bad one turns every small bug into an excavation. A useful rule of thumb for assessing an abstraction is to ask yourself: How often do I need to peek under the hood? Once per day? Once per month? Once per year? The less often you need to break the illusion, the better the abstraction. Asymmetry of abstraction costs There\u2019s also a certain asymmetry to abstraction. The author of an abstraction enjoys its benefits immediately\u2014it makes their code look cleaner, easier to write, more elegant, or perhaps more flexible. But the cost of maintaining that abstraction often falls on others: future developers, maintainers, and performance engineers who have to work with the code. They\u2019re the ones who have to peel back the layers, trace the indirections, and make sense of how things fit together. They\u2019re the ones paying the real cost of unnecessary abstraction. Conclusion: Use abstractions wisely This isn\u2019t to say that abstractions are bad\u2014far from it. Good abstractions are powerful. They enable us to build complex systems without getting lost in complexity. But we must recognize that abstractions are not free. They have real costs, both in terms of performance and complexity. And if an \"abstraction\" isn\u2019t hiding complexity but is simply adding a layer of indirection, then it\u2019s not an abstraction at all. The next time you reach for an abstraction, ask yourself: Is this truly simplifying the system? Or is it just another layer of indirection? Use abstractions wisely, and remember\u2014if you\u2019re not truly hiding complexity, you\u2019re just adding to it.2024 \u00a9 Fernando Hurtado Cardenas.RSS",
    "comments": [],
    "description": "fhur's blog",
    "document_uid": "b4e95ed124",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966072",
    "title": "Node.js server with best-effort completions on functions",
    "url": "https://gist.github.com/kevinswiber/f6d49d591dcf425ba586ca86a97a7d20",
    "score": 1,
    "timestamp": "2024-10-27T23:08:30",
    "source": "Hacker News",
    "content": "import { AsyncResource } from \"node:async_hooks\"; import { createServer, request } from \"node:http\"; // A thin wrapper around AsyncResource to manage best-effort completions. // Best-effort resources are not guaranteed to complete, but they are // allowed time to complete before the process exits on SIGINT. // In a real-world scenario, you would likely include more details // in the resource context to help trace completed and // abandoned best-effort executions. class BestEffort extends AsyncResource { constructor(globalState) { super(\"BestEffort\"); this.globalState = globalState; } bind(fn, thisArg) { this.globalState.set(this.asyncId(), this); return super.bind(fn, thisArg); } runInAsyncScope(fn, thisArg, ...args) { this.globalState.set(this.asyncId(), this); return super.runInAsyncScope(fn, thisArg, ...args); } complete() { this.globalState.delete(this.asyncId()); this.emitDestroy(); } } const bestEffortResources = new Map(); const timeoutMilliseconds = [1_000, 10_000, 30_000, 50_000, 100_000]; let timeoutIndex = 0; const server = createServer((_req, res) => { // Create a new BestEffort resource for each execution scope. const bestEffort = new BestEffort(bestEffortResources); const ms = timeoutMilliseconds[timeoutIndex++ % timeoutMilliseconds.length]; console.log( \"Setting best-effort function:\", bestEffort.asyncId(), \"with timeout:\", ms, ); res.on( \"finish\", // Bind the event listener to execute in the best-effort's scope. bestEffort.bind(() => { setTimeout(() => { console.log(\"Executing best-effort function:\", bestEffort.asyncId()); // Signal completion of the best-effort resource. // If using try/catch, be sure to call complete() in the finally block. bestEffort.complete(); }, ms); }), ); res.end(bestEffort.asyncId().toString()); }); const port = process.env.PORT || 3000; server.listen(port, () => { console.log(\"Listening: \", server.address()); }); function forceExit(code) { server.closeAllConnections(); // Force complete all best-effort resources. Optional. if (bestEffortResources.size > 0) { for (const [id, bestEffort] of bestEffortResources.entries()) { console.log(\"Dropping best-effort function without completion:\", id); bestEffort.complete(); } } console.log(\"Exiting...\"); process.exit(code); } const successExitCode = 0; const sigIntExitCode = 1; let sigIntCount = 0; process.on(\"SIGINT\", () => { sigIntCount++; if (sigIntCount > 1) { console.log(\"Received SIGINT (Ctrl-C) twice, exiting...\"); process.exit(sigIntExitCode); return; } server.close(); // No best-effort resources, so exit immediately. if (bestEffortResources.size === 0) { forceExit(successExitCode); return; } // Set a failsafe to force exit after 60 seconds. const failsafe = setTimeout(() => forceExit(sigIntExitCode), 60_000); // Monitor best-effort resources waiting to complete. const monitor = setInterval(() => { const size = bestEffortResources.size; console.log( \"Waiting to complete best-effort functions:\", Array.from(bestEffortResources.keys()), ); if (size === 0) { clearTimeout(failsafe); clearInterval(monitor); forceExit(successExitCode); } }, 1_000); }); // Send a few requests to the server. for (let i = 0, max = 5; i < max; i++) { request({ port }, () => { if (i === max - 1) { console.log(\"Press Ctrl+C to exit.\"); } }).end(); }",
    "comments": [],
    "description": "Node.js server with best-effort completions on functions - server.js",
    "document_uid": "b8817e4b33",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966066",
    "title": "When will Apple stop supporting Intel Macs?",
    "url": "https://9to5mac.com/2024/10/27/when-will-apple-stop-supporting-intel-macs/",
    "score": 1,
    "timestamp": "2024-10-27T23:07:53",
    "source": "Hacker News",
    "content": "Apple transitioned the Mac lineup to Apple Silicon starting in 2020, and they completed the transition in 2023. Since the transition started, Apple quickly started dropping support for Intel Macs on newer versions of macOS. That raises the question: how much longer will Intel Macs receive software support? Current Sequoia support Apple delivers around 5 years of software updates for most of their devices, so that gives us a good idea of how much longer Intel Macs will be around. With Apple\u2019s latest release, macOS Sequoia, there\u2019s only a few Intel based machines remaining: 2018 Mac mini 2019-20 iMac models 2017 iMac Pro 2020 MacBook Air 2018-20 MacBook Pro models 2019 Mac Pro Nearly all of the remaining Intel machines were released right before the transition to Apple Silicon, or very close to it. The MacBook Pro line has received the most generous support so far, with 2018 models still supported. Although, that\u2019s likely because the 2018, 2019, and base model 2020 MacBook Pros all share the same (or very similar) versions of the same 8th generation ultra low power Intel Core i5 and i7 chipsets. The higher end 2020 13\u2033 MacBook Pro did take a step up to 10th generation Intel chips, however. One interesting anomaly to this is the iMac Pro, which Apple unveiled at WWDC17 and released in December of that year. It\u2019s definitely well past the typical 5-6 years of software support, but Apple seemingly wants to cater to the pro customers that might be using that computer still. What\u2019s next? In all likelihood, the version of macOS released in 2025 \u2013 macOS 16 \u2013 will probably still support one final wave of Intel Macs, bringing them all to have at least 5 years of software support. If I were to guess, the list of existing Intel Macs we have right now is probably pretty similar to what they\u2019ll support with macOS 16. They might drop a couple, like the 2019 iMac or 2018 Mac mini. Then, I\u2019d expect that macOS 17 in 2026 will probably be the first Apple Silicon exclusive version of macOS. That will mark 6 years since Apple began its transition and around 6 years since Apple released the last Intel Mac. It will also be over 3 years since Apple sold the last Intel Mac. Wrap up Intel users should likely count their days, seeing as there\u2019s probably around 2 years left to use your current machine with the latest macOS version. Of course, Apple will provide around 2 years of security patches after macOS feature updates are discontinued. That being said, there\u2019s a number of great deals out there for Apple Silicon Macs, including the M2 MacBook Air reaching an all time low of $699 at Amazon (with on page coupon.) Follow Michael: X/Twitter, Threads, Instagram FTC: We use income earning auto affiliate links. More.",
    "comments": [],
    "description": "Apple transitioned the Mac lineup to Apple Silicon starting in 2020, and they completed the transition in 2023. Since the...",
    "document_uid": "8e81c7b303",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966031",
    "title": "Fundamentals of Scanning Electron Microscopy (2006)",
    "url": "https://link.springer.com/chapter/10.1007/978-0-387-39620-0_1",
    "score": 1,
    "timestamp": "2024-10-27T23:01:11",
    "source": "Hacker News",
    "content": "O. C. Wells, Scanning Electron Microscopy, McGraw-Hill, New York (1974). Google Scholar S. Wischnitzer, Introduction to Electron Microscopy, Pergamon Press, New York (1962). Google Scholar M. E. Haine and V. E. Cosslett, The Electron Microscope, Spon, London (1961). Google Scholar A. N. Broers, in: SEM/1975, IIT Research Institute, Chicago (1975). Google Scholar J. Goldstein, D. Newbury, D. Joy, C. Lyman, P. Echlin, E. Lifshin, L. Sawyer, and J. Michael, Scanning Electron Microscopy and X-Ray Microanalysis, 3rd edn, Kluwer Academic/Plenum Publishers, New York (2003). Google Scholar C. W. Oatley, The Scanning Electron Microscope, Cambridge University Press, Cambridge (1972). Google Scholar J. I. Goldstein and H. Yakowitz, Practical Scanning Electron Microscopy, Plenum Press, New York (1975). Google Scholar Y. X. Chen, L. J. Campbell, and W. L. Zhou, J. Cryst. Growth, 270 (2004) 505.Article CAS ADS Google Scholar J. J. Liu, A. West, J. J. Chen, M. H. Yu, and W. L. Zhou, Appl. Phys. Lett., 87 (2005) 172505.Article ADS CAS Google Scholar T. E. Everhart and R. F. , J. Sci. Instrum., 37 (1960) 246.Article ADS Google Scholar D. C. Joy, C. S. Joy, and R. D. Bunn, Scanning, 18 (1996) 533.Article CAS PubMed Google Scholar H. Koike, K. Ueno, and M. Suzuki, Proceedings of the 29th Annual Meeting EMSA, G. W. Bailey (Ed.), Claitor\u2019s Publishing, Baton Rouge (1971), pp. 225\u2013226. Google Scholar L. Xu, W. L. Zhou, C. Frommen, R. H. Baughman, A. A. Zakhidov, L. Malkinski, J. Q. Wang, and J. B. Wiley, Chem. Commun., 2000 (2000) 997. Google Scholar K. Tanaka, A. Mitsushima, Y. Kashima, T. Nakadera, and H. Osatake, J. Electron Microsc. Tech., 12 (1989) 146.Article CAS PubMed Google Scholar K.-R. Peters, J. Microsc., 118 (1980) 429.CAS PubMed Google Scholar R. P. Apkarian and J. C. Curtis, Scan. Electron Microsc., 4 (1981) 165.CAS PubMed Google Scholar A. Boyde, Scan. Electron Microsc., 11 (1978) 303. Google Scholar T. F. Anderson, NY Acad. Sci., 13 (1951) 130\u2013134. Google Scholar R. P. Apkarian, Scanning Microsc., 8(2) (1994) 289.CAS PubMed Google Scholar K.-R. Peters, Scan. Electron Microsc., 4 (1985) 1519. Google Scholar R. P. Apkarian, 45th Annual Proceedings of the Microscopy Society of America (1987) 564. Google Scholar D. C. Joy, 52nd Annual Proceedings of the Microscopy Society of America (1994). Google Scholar E. L. Bearer, L. Orci, P. Sors, J. Cell Biol., 100 (1985) 418.Article CAS PubMed Google Scholar R. P. Apkarian, Scanning, 19 (1997) 361.Article CAS PubMed Google Scholar",
    "comments": [],
    "description": "'Fundamentals of Scanning Electron Microscopy (SEM)' published in 'Scanning Microscopy for Nanotechnology'",
    "document_uid": "8511fd5f79",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41966013",
    "title": "CopDB: Police Database",
    "url": "https://app.copdb.org",
    "score": 2,
    "timestamp": "2024-10-27T22:58:04",
    "source": "Hacker News",
    "content": "You need to enable JavaScript to run this app.",
    "comments": [
      {
        "author": "monkaiju",
        "text": "Also a blog associated with the project at <a href=\"https:&#x2F;&#x2F;copdb.org\" rel=\"nofollow\">https:&#x2F;&#x2F;copdb.org</a> and docs at <a href=\"https:&#x2F;&#x2F;docs.copdb.org\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.copdb.org</a>",
        "time": "2024-10-27T22:58:04"
      }
    ],
    "description": "No description available.",
    "document_uid": "53bd620654",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41965973",
    "title": "Show HN: Colab.ing \u2013 A Tool to Capture and Organize Team Ideas for Your Company",
    "url": "https://news.ycombinator.com/item?id=41965973",
    "score": 3,
    "timestamp": "2024-10-27T22:51:19",
    "source": "Hacker News",
    "content": "Hello HN community,<p>Over the past 11 months, I&#x27;ve been working on a side project called colab.ing. It&#x27;s a tool designed to help organizations capture, organize, and act on ideas and feedback from their team members.<p>Problem Statement:<p>In many companies, valuable ideas from employees often get lost in the daily workflow. Whether it&#x27;s process improvements, customer insights, or innovative concepts, these ideas sometimes don&#x27;t reach the right people or are forgotten amidst other priorities.<p>Solution: <a href=\"https:&#x2F;&#x2F;colab.ing\" rel=\"nofollow\">https:&#x2F;&#x2F;colab.ing</a><p>colab.ing aims to provide a simple and effective platform where team members can submit their ideas and feedback. These submissions are then organized and made accessible for decision-makers to review and act upon.<p>Key Features<p><pre><code>    Easy Submission: A straightforward way for employees to share their thoughts.\n    Organized Dashboard: Ideas are categorized for easy navigation and prioritization.\n    Feedback Ownership: Assign responsibility for implementing specific ideas within the team.\n    Slack integration: Connect a slack workspace, and your users, and submit feedback directly from          inside your Slack workspace!\n    Dashboard: Get a analytical breakdown of where your feedback is coming from, and who&#x27;s engaging with it.\n\n    Upcoming Enhancements:\n        Customizable tags and categories to fit your organization&#x27;s needs.\n        ROI tracking to measure the impact of implemented ideas.\n        Advanced customization options in a premium tier.\n</code></pre>\nWhy I Built This<p>I believe that the people working within an organization have invaluable insights that can drive growth and improvement. By providing a platform to harness this collective knowledge, companies can foster innovation and make more informed decisions. Research also backs this up!<p>Looking for Feedback and Early Users<p>I&#x27;m eager to get feedback from the HN community to help improve colab.ing. If you&#x27;re interested:<p><pre><code>    Try It Out: Visit colab.ing to explore the platform.\n    Early Adopters Program: For organizations willing to collaborate closely, you can join our Early Adopters program: Early Adopters Signup: https:&#x2F;&#x2F;www.colab.ing&#x2F;#early-adopters\n\n</code></pre>\nWhat I&#x27;d Love Feedback On<p><pre><code>    Usability: Is the platform intuitive?\n    Value Proposition: Does this address a need you&#x27;ve observed in organizations?\n    Feature Suggestions: What functionalities would make this tool more useful for you?\n</code></pre>\nContact<p>Feel free to share your thoughts in the comments or reach out directly at sam@colab.ing.<p>Thank you for taking the time to read this. I&#x27;m looking forward to your feedback!",
    "comments": [],
    "description": "No description available.",
    "document_uid": "fa746b5bb3",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41965957",
    "title": "The couple who took on Google and cost the tech giant \u00a32B",
    "url": "https://www.bbc.co.uk/news/articles/cjr431lr72jo",
    "score": 2,
    "timestamp": "2024-10-27T22:48:46",
    "source": "Hacker News",
    "content": "Foundem had been hit by a Google search penalty, prompted by one of the search engine\u2019s automatic spam filters. It pushed the website way down the lists of search results for relevant queries like \"price comparison\" and \"comparison shopping\".It meant the couple\u2019s website, which charged a fee when customers clicked on their product listings through to other websites, struggled to make any money.\"We were monitoring our pages and how they were ranking, and then we saw them all plummet almost immediately, \" says Adam.While the launch day for Foundem didn't go to plan, it would lead to the start of something else \u2013 a 15-year legal battle that culminated in a then record \u20ac2.4bn (\u00a32bn) fine for Google, which was deemed to have abused its market dominance. The case has been hailed as a landmark moment in the global regulation of Big Tech.Google spent seven years fighting that verdict, issued in June 2017, but in September this year Europe\u2019s top court \u2013 the European Court of Justice \u2013 rejected its appeals.Speaking to Radio 4\u2019s The Bottom Line in their first interview since that final verdict, Shivaun and Adam explained that at first, they thought their website\u2019s faltering start had simply been a mistake.\u201cWe initially thought this was collateral damage, that we had been false positive detected as spam,\u201d says Shivaun, 55. \u201cWe just assumed we had to escalate to the right place and it would be overturned.\u201d\"If you're denied traffic, then you have no business,\" adds Adam, 58.The couple sent Google numerous requests to have the restriction lifted but, more than two years later, nothing had changed and they said they received no response.Meanwhile, their website was \"ranking completely normally\" on other search engines, but that didn't really matter, according to Shivaun, as \"everyone's using Google\".The couple would later discover that their site was not the only one to have been put at a disadvantage by Google \u2013 by the time the tech giant was found guilty and fined in 2017 there were around 20 claimants, including Kelkoo, Trivago and Yelp.",
    "comments": [
      {
        "author": "ChrisArchitect",
        "text": "[dupe]<p>Discussion: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41958425\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41958425</a>",
        "time": "2024-10-27T22:56:37"
      }
    ],
    "description": "Shivaun Raff and her husband, Adam, describe their long court battle with technology giant Google.",
    "document_uid": "dc5e407d71",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41965942",
    "title": "Understanding CharAt(index) in JavaScript: A Guide",
    "url": "https://jsdevspace.substack.com/p/understanding-charatindex-in-javascript",
    "score": 1,
    "timestamp": "2024-10-27T22:47:11",
    "source": "Hacker News",
    "content": "The charAt(index) method is a built-in JavaScript string method that retrieves the character at a specified index within a string. Below is a detailed look at its usage, parameters, and practical examples.Syntax:string.charAt(index)Parameter:Return Value:Examplesconst str = \"Hello, World!\"; console.log(str.charAt(0)); // Output: \"H\" console.log(str.charAt(7)); // Output: \"W\" console.log(str.charAt(13)); // Output: \"\" (index out of range)const password = \"P@ssw0rd\"; if (password.charAt(0) === 'P') { console.log(\"Password starts with 'P'\"); }const dateString = \"2024-10-22\"; const year = dateString.charAt(0) + dateString.charAt(1) + dateString.charAt(2) + dateString.charAt(3); console.log(year); // Output: \"2024\"const text = \"abcde\"; for (let i = 0; i < text.length; i++) { console.log(text.charAt(i)); // Outputs: a, b, c, d, e }const unicodeString = \"Hello \ud83d\ude0a\"; console.log(unicodeString.charAt(6)); // Output: \"\ud83d\ude0a\"Index Out of Range: If an index beyond the string length is specified, charAt returns an empty string instead of throwing an error.Difference from Square Brackets ([]): You can also access characters using string[index], but this approach does not handle out-of-range indexes safely (it returns undefined instead).console.log(str[0]); // Output: \"H\" console.log(str[13]); // Output: undefinedThe charAt(index) method is a straightforward and versatile tool for character extraction in JavaScript. Its ability to return an empty string for out-of-range indexes makes it a safe choice for character processing, string parsing, looping, and handling special characters.",
    "comments": [],
    "description": "Mastering charAt(index) in JavaScript: Extracting and Handling Characters with Ease",
    "document_uid": "baed771279",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41965935",
    "title": "Key Flash USDT Currency",
    "url": "https://news.ycombinator.com/item?id=41965935",
    "score": 1,
    "timestamp": "2024-10-27T22:45:47",
    "source": "Hacker News",
    "content": "In the ever-evolving world of cryptocurrency, new innovations are constantly emerging to enhance the trading experience. One such innovation is Flash USDT, a digital currency that is gaining traction among traders and investors alike. In this blog, we will explore the benefits of Flash USDT currency, how to purchase it, and why you should consider making a purchase at Tradexpertz.com.<p>What is Flash USDT?\nFlash USDT is a stablecoin that is pegged to the US Dollar, providing a reliable and stable medium of exchange in the volatile cryptocurrency market. Unlike traditional cryptocurrencies, which can experience significant price fluctuations, Flash USDT offers a level of stability that is appealing to both new and experienced traders. This makes it an excellent choice for those looking to hedge against market volatility while still participating in the exciting world of digital assets.<p>Why Choose Flash USDT Currency?\nStability: As a stablecoin, Flash USDT maintains a consistent value, making it a safe haven during market downturns.\nLiquidity: With its growing popularity, Flash USDT is becoming increasingly liquid, allowing for quick and easy transactions.\nEase of Use: Purchasing Flash USDT currency is straightforward, especially with platforms like Binance and Tradexpertz.com.\nHow to Purchase Flash USDT Currency\nFor those looking for a more tailored experience, consider purchasing Flash USDT currency directly from Tradexpertz.com. They offer a minimum purchase of $2000 Flash USDT for just $200, making it an attractive option for both new and seasoned investors. You can find the product here.<p>Why Purchase Flash USDT at Tradexpertz.com?\nCompetitive Pricing: Tradexpertz.com offers a unique opportunity to purchase Flash USDT at a fraction of the cost compared to other platforms.\nUser-Friendly Interface: The website is designed for ease of use, making your purchasing experience seamless.\nEducational Resources: Tradexpertz.com provides valuable insights and resources to help you understand the benefits of Flash USDT and how to maximize your investment.\nTo make your purchase, simply visit Tradexpertz.com and follow the straightforward steps to acquire your Flash USDT currency.<p>Need Assistance?\nIf you have any questions or need assistance with your purchase, don\u2019t hesitate to reach out to the Tradexpertz.com team. You can contact them via:<p>WhatsApp: +12484534036\nTelegram: t.me&#x2F;denisblc\nTheir dedicated support team is available to help you every step of the way.<p>Conclusion\nIn conclusion, Flash USDT is a promising addition to the cryptocurrency landscape, offering stability and liquidity that can benefit traders and investors. Whether you\u2019re looking to hedge against market volatility or simply want to explore new investment opportunities, Flash USDT currency is worth considering.<p>Don\u2019t miss out on the chance to purchase Flash USDT at an unbeatable price. Head over to Tradexpertz.com today and take the first step towards enhancing your cryptocurrency portfolio. Remember, the minimum purchase is $2000 Flash USDT for just $200, making it an accessible option for everyone.<p>For more information and to make your purchase, visit Tradexpertz.com now!",
    "comments": [],
    "description": "No description available.",
    "document_uid": "214062f109",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41965915",
    "title": "EPDiy: An ESP32-based driver board for e-Paper / E-ink displays",
    "url": "https://github.com/vroland/epdiy",
    "score": 2,
    "timestamp": "2024-10-27T22:42:02",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "EPDiy is a driver board for affordable e-Paper (or E-ink) displays. - vroland/epdiy",
    "document_uid": "2f73968b0f",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41965907",
    "title": "Google creating an AI agent to use your PC on your behalf",
    "url": "https://www.tomshardware.com/tech-industry/artificial-intelligence/google-creating-an-ai-agent-to-use-your-pc-on-your-behalf-says-report",
    "score": 1,
    "timestamp": "2024-10-27T22:40:15",
    "source": "Hacker News",
    "content": "Google is reportedly working on an AI that takes over your web browser to complete various tasks. It is also busy creating an agent that will control your computer - beyond the browser. According to Reuters, this AI tool will arrive as 'Project Jarvis' in the browser and is intended to come out with the next release of its Gemini LLM. The search giant is not alone in developing a system like this, as OpenAI is reportedly also working on a computer-using agent, or CUA, which prowls the web autonomously in your browser.The browser-based AI would make it easier for users to conduct online research, as they no longer have to develop APIs or even use a screen recording so that the AI tool can read the user data. Instead, you can directly give it commands in your browser and it should automatically do everything you need, including filling out forms and clicking buttons. AI-tasked examples include opening pertinent web pages, compiling search data into easily readable tables, purchasing products, or booking flights.If Google is successful in deploying a system like this, it would make AI tools far more accessible, even allowing those with zero experience in AI to use it easily and effectively. That\u2019s because it removes the need to develop APIs or even find techniques that would allow the AI to access the required data \u2014 just type in what you want it to do in your browser and it will get down to business instantly.Reaching beyond the browser to control your computerAside from the reported Google AI browser-based intentions, there have been rumors that Anthropic and Google want to take this tool to the next level, going beyond browser control. The companies are reportedly interested in creating an agent that will control your computer for you, allowing you to give it a command (like opening all your work apps and arranging them on your screen), and it will interact with your system on your behalf.However, even the browser-based Project Jarvis would likely raise privacy and security issues, perhaps as much as Microsoft\u2019s controversial Recall feature did. After all, a lot of sensitive data can be accessed via the web browser \u2014 including emails, work files, and even banking details \u2014 so Google must place a safeguard around Project Jarvis and its future developments to ensure that it doesn\u2019t unnecessarily access your private information.Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.",
    "comments": [
      {
        "author": "ChrisArchitect",
        "text": "[dupe] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41958642\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41958642</a>",
        "time": "2024-10-27T22:56:58"
      }
    ],
    "description": "Google is reportedly working on an AI tool that will interact with your browser as another user.",
    "document_uid": "a48d2aed9f",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41970032",
    "title": "Best Tools for Indie Consultants",
    "url": "https://jxnl.co/writing/2024/08/23/consulting-stack/",
    "score": 1,
    "timestamp": "2024-10-28T12:35:19",
    "source": "Hacker News",
    "content": "As an indie consultant, having the right tools can make or break your business. Over the years, I've refined my \"consulting stack\" - a collection of software and services that keep my operations smooth and professional. This post is an extension of my thoughts on AI consulting and freelancing in the AI gold rush. In this guide, I'll share the key components of my stack and why they matter. Whether you're just starting out or looking to optimize your existing practice, these tools can help streamline your operations and enhance your professional image. I'll cover everything from setting up your business foundation to essential software for day-to-day operations. By the end, you'll have a comprehensive todo list for setting up your business foundation and implementing your own consulting stack. This advice stems from my personal experience and lessons learned from AI consulting, aimed at helping you avoid common pitfalls and accelerate your success in the consulting world. Consulting Stack Cheatsheet If you just want all the links and referal link here you go: Doola: A platform for hassle-free LLC setup in the US. Mercury: A startup-friendly bank with responsive support for business banking needs. Stripe: A payment processing platform for online transactions. Dropbox Sign: A tool for sending and tracking proposals, and getting contracts signed digitally. Cal.com: A scheduling tool for easily booking calls and meetings. Circleback: A platform for meeting notes and follow-ups. Anthropic Claude: An AI assistant for various tasks, including content creation and research. Spiral Computer: A tool for generating memos and blog posts from transcripts. Better Diction: A tool for transcribing and improving the quality of your audio recordings. Setting Up Your Business Foundation The first steps in building your consulting practice involve some unsexy but crucial administrative tasks: Form an LLC: I recommend using Doola for hassle-free LLC setup if you're in the US. Open a business bank account: Mercury is my go-to for its startup-friendly features and responsive support. Avoid big banks if possible - I've had issues with Chase in the past. Get business credit cards: Once you hit minimum account thresholds, look into cards for software expenses, travel, etc. This lets you maximize tax benefits of incorporation. Hire an accountant and set up QuickBooks: Proper bookkeeping is essential for taking advantage of self-employment tax benefits. Don't leave money on the table. Email me if you'd like a referral to an accountant I trust. Tax benefits of incorporation As a consultant, tracking expenses well can lead to significant tax savings. Make sure you're logging everything from office supplies to client meals. I made the mistake of using personal cards for business expenses and it just made my bookkeeping a mess, esp when it came to expensing and paying myself back. With the legal and financial foundation in place, here are the key software tools I rely on: Stripe for invoicing: Great for smaller clients. Larger companies often prefer ACH transfers. Note that if you don't allow ACH and you let companies pay you via card, you're goign to eat some fees, so always turn on DD and even ask them to pay by ACH instead. Mercury also allows invoice payments now. Dropbox + Dropbox Sign: This tool is essential for sending and tracking proposals, as well as getting contracts signed digitally. It adds a layer of professionalism to your operations. Additionally, it allows you to monitor who is downloading and viewing your documents, which is particularly useful when sending contracts, documents, or proposals to clients. Cal.com: For easily scheduling calls, especially free consultations. Whenever someone asks to hang out or jump on a call, I just send them a cow link immediately. In the future, you can also set up paid calls, which makes it very easy. It's a tool I love using, and it's basically free. That said, you can definitely go crazy with how you set up your availability schedules, and ultimately, I think the right answer is to hire an EA once you start making enough money. Circleback: My go-to for meeting notes and follow-ups, But I often will actually take those transcripts myself and pass them into something like Cloud or Spiral Computer to produce memos and blog posts. Anthropic Claude: An AI assistant crucial for content creation and research. I use it for generating proposals, leveraging example templates and detailed prompts. Unexpectedly, I've also found it valuable for creating mockups - I often share Claude-generated React code to visually communicate designs to clients, going beyond verbal descriptions. Spiral Computer: I use this to generate memos and blog posts from transcripts. It's a general-purpose tool that I use to convert transcripts into tweets and blog posts, and convert blog posts into tweets. If you're committed to producing enough content and recognize that one of the best ways of increasing your surface area is to write more, Spiral Computer is a no-brainer tool for $200 a year. You'll definitely make it back if you can double the amount of content you produce. Better Diction: A tool for transcribing and improving the quality of your audio recordings, which i used to control cursor, and writing. You can also use PROMO JASON20 for a discount. The power of standardization Having a standard engagement letter and proposal process saves time and projects professionalism. Don't reinvent the wheel for every client. Subribe to my news letter if you want to get notified when I publish more posts on how I set up my engagement letters and proposals examples If you found this helpful, subscribe to my newsletter for more posts on how I build my consulting practice. Scaling Your Practice As your consulting business grows, consider these additional investments: Hire an EA This can be a game-changer for managing your schedule and admin tasks. I don't really have much experience in hiring a team out of the Philippines, for example. I was lucky enough to go through my own network to find one. But I would say that the moment you start consulting seriously",
    "comments": [],
    "description": "Discover essential tools and strategies for indie consultants to streamline operations and enhance professionalism in your practice.",
    "document_uid": "0930855407",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41970028",
    "title": "InterLM: Open-source LLM with 1M context window",
    "url": "https://github.com/InternLM/InternLM",
    "score": 1,
    "timestamp": "2024-10-28T12:34:48",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "Official release of InternLM2.5 base and chat models. 1M context support - InternLM/InternLM",
    "document_uid": "62cce97bab",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969986",
    "title": "Infosec: WiFi Hacking Basics Podcast",
    "url": "https://open.spotify.com/show/47ZgW0a7aVvgqUC3hfnKez",
    "score": 1,
    "timestamp": "2024-10-28T12:29:21",
    "source": "Hacker News",
    "content": "Infosec: WiFi Hacking Basics Podcast",
    "comments": [],
    "description": "Listen to WiFi Hacking Basics on Spotify. EDUCATIONAL PURPOSES ONLY. Explore the basics of Wi-Fi hacking for ethical penetration testing. Each episode covers essential techniques, tools, and best practices to help you understand network vulnerabilities and enhance your cybersecurity skills. Tune in to master wireless security responsibly!",
    "document_uid": "39077ebb94",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969976",
    "title": "Pockethernet 2 \u2013 Preview Version",
    "url": "https://pockethernet.com/manual/#tdrgraph",
    "score": 1,
    "timestamp": "2024-10-28T12:27:59",
    "source": "Hacker News",
    "content": "Quickstart Make sure to read the regulatory and safety notices first. You can start testing in just a few seconds: Switch on your Pockethernet. The Power LED will come on. Open the Pockethernet App Tap the \"Connect\" button in the App. No Bluetooth pairing is required. Select measurements and press \"Measure\" Review the results and optionally save them in the \"Reports\" tab The device Power button When the device is powered off, press the power button until the Power LED becomes on. When Pockethernet is powered on, the button does this: Button pressAction Long (at least 2 sec)Switches the device off Short (<1.5sec)Starts the Quick test function Two quick pressesSwitches the Flashlight function on or off Very long (>8 sec)Resets the device Power LED When the device is powered on: Power LED colorState BlueBattery charge level > 50% GreenBattery charge level between 30%-50% YellowBattery charge level below 30% RedBattery fault USB cable connected for charging: Power LED colorState YellowBattery charging GreenFully charged RedBattery / charging fault When a USB cable is connected for charging while the device has been switched off, it will enter a charging standby mode indicated by a \"breathing\" power LED. In this state, you need to press the power button to actually turn the device on which is indicated by a solid LED color. Network, Link, Cable LEDs For the description of these, please see the section Quick test function Connectors The Ethernet port is for connecting to Ethernet cables and networks The USB-C port is used for charging and wired software updates The 3.5mm jack connector is reserved for future accessories Flashlight function With Pockethernet powered on, press the power button two times quickly to turn the flashlight function on or off. This will make all 4 LEDs light up with a bright light color. You can use it to locate cables and port in dark places like behind a rack or under a desk. The app The Pockethernet app allows to conduct tests and save the results as report The app consist of three main sections: Test, Report and Tools Test tab The test tab has a separate row for each measurement You can expand and close the detail section of a test by clicking on the row The switch icon in each row header represents 4 possible states Measurement not selected to run white Measurement selected to run blue Measurement in progress flashing blue Measurement finished and it's results OK:green/Not OK All performed tests with a green or red indicator will be included in the report. If you want to exclude a test from the report, disable the test. If you want to re-run a single test, deselect and select it again and press the Measure button. If you long press the Measure button, test will be repeated with the following logic: If only the Wiremap test (an no other tests) has been run previously, then Wiremap will be repeated. If any other tests than Wiremap have been run previoulsy, all of those will be repeated but not the Wiremap. The rationale is that for the Wiremap test, you need the the wiremap adapter attached, so you either want to perform Wiremap or Network tests. Report tab The report pane allows you to create and save new reports or list, review and export the already existing ones. Save report Here you can create a report of the measurements performed in the test tab. All tests which are red or green in the test tab will be included. You can add additional details about the measurements such as: The user performing the tests Address, Location, Port ID Comment Add a relevant photo These will all be included in the report. The \"Tag\" field is a special field as it will not only be included in the report, but also in the filename of the generated PDF document. You can use it to quickly identify a measurement or report. If you don't need too much detail about the measurement, specifying a tag may be enough for identification. The filename under which the report will be saved is: \"Pockethernet <Date> <Time> - <Tag>\".pdf The report can be viewed directly, shared via any application that supports receiving files, or saved locally on your device. View reports The View report subsection lists all your previously saved reports. Below the name of the report, you have four options in the \"Action\" row: View: Opens the report Send: Send the report via another application Delete: Delete the report from storage Select: Select the report to be included in the combined report There are two batch export options on the bottom of the report list: Combined PDF: This creates a single PDF document from all reports selected above All reports as ZIP: This creates a ZIP file all all reports that are stored in the app. It will include the PDF files, the attached images as JPEG files and the raw measurement data in JSON format. The tools tab contains tools, settings and informations. Cable toner The toner function allows identification and tracking of cables via an electronic signal that can be picked up with a tone probe. You can use it to identify individual cables in a bundle, identify individual wire pairs and track their path in culverts and walls. You can select on which wire pair the signal should be transmitted, which tone should be used at what volume. You can use any analogue tone probe to pick up the signal. Port blinker You can use the port blinker function to quickly identify a connection to a switch or router. Set up the connection parameters and either set to \"On\" for a constant link or to \"Blink\" and look for the Link LED on the switch. When using the Blink function, Pockethernet will repeatedly bring a link up and down, making the Link LED blink. For switches which use a different color for low-speed links (e.g. orange instead of green), you can set the link speed to 10/100 Mbit",
    "comments": [],
    "description": "Pockethernet user manual",
    "document_uid": "104cd751fc",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969974",
    "title": "The Birth of the Viral Inquisitor",
    "url": "https://human-as-media.com/2024/10/28/the-birth-of-the-viral-inquisitor/",
    "score": 1,
    "timestamp": "2024-10-28T12:27:26",
    "source": "Hacker News",
    "content": "Social media increasingly serve not to facilitate the simple exchange of written information but to sort out everyone\u2019s attitude toward the most pressing issues of the day. The wrong response to someone\u2019s hard-fought truth is punished by reciprocal aggression and various forms of ostracism. An excerpt from the chapter \u201cThe Viral Inquisitor\u201d in The Viral Inquisitor and other essays on postjournalism and media ecology. Digital media not only compressed the time and space that once separated people but also enabled a new language: digital speech, which has traits of both oral and written communication. Like oral speech, it permits the instant exchange of replies; like writing, it leaves behind a record and can be transmitted in time and space. These features mean that people\u2019s spontaneous and mostly emotive efforts to establish their social statuses in conversation are no longer transient. The reactions of millions of people are accumulated, spread, and displayed to everyone else. This new type of conversation, digital orality, has its benefits. It allows socialization at an unprecedented pace and scale. The Viral Editor still delivers the most relevant information to everyone. However, the ease of exchanging digital speech has shifted the focus of mass communication from reflections to reflexes, from substance to attitude. Social media demand that everyone relate to others, to their ideas, to their troubles and achievements \u2013 indeed, to their very existence. Social media increasingly serve not to facilitate the simple exchange of written information but to sort out everyone\u2019s attitude toward the most pressing issues of the day. The wrong response to someone\u2019s hard-fought truth is punished by reciprocal aggression and various forms of ostracism. Under these environmental conditions, affected by the algorithms and instantaneous exchange of digital reflexes, the Viral Editor morphed into the Viral Inquisitor, as Martin Gurri suggested to call it at a recent workshop. If the Viral Editor required from everyone participation in content selection, the Viral Inquisitor demands from everyone solidarity with the most widely held views of others. The Viral Inquisitor is a relentless tormentor. British anthropologist Robin Dunbar famously hypothesized that the human brain can maintain stable social relationships with 120 to 150 individuals, the size of a tribal group or a village. Social media override Dunbar\u2019s number, burdening users with connections more numerous than what we can handle. Excessive social connections make people feel impelled to know hundreds of strangers, whose digital existence intrudes upon their personal space \u2013 their screens. Through the same mechanisms that the Viral Editor used to customize content for everyone, the Viral Inquisitor gathers identity signals of others and delivers them precisely to those most likely to react. You may choose not to react, but when you eventually or accidentally react with a click, or even a longer pause in scrolling down, you fall into a trap of further customization and better-customized identity signalling. Sophisticated algorithms ensure that the Viral Inquisitor notes everyone\u2019s inclinations and preferences. The user can\u2019t escape from being exposed to the identity signals of others and their persisting demands for affirmation. If you react wrongly, you are guilty. This makes the Viral Inquisitor a much more effective warden than the notorious Big Brother. The sins and thoughtcrimes of everyone get delivered precisely to those who can be alarmed and enraged. No KGB is capable of such all-pervasive control over everyone\u2019s wrongdoings and wrongthink. The Viral Inquisitor is the collective high priest of cancel culture. But perhaps the worst element of this transformation is the way it abets the emergence of post-truth. A persistent interrogator, the Viral Inquisitor extracts users\u2019 testimonies and checks them against the truths held by others. The Viral Inquisitor changes the way propositions are verified, challenging our very epistemology of truth. Before literacy, a truth was confirmed by how well it comported with nature and its divine moving forces. Polytheism was the natural science of oral culture. To deal with nature, gods, and one another, humans made respective arrangements with all these. Better arrangements lasted longer and conferred better benefits on their participants \u2013 practice was the criterion of truth, to quote Marx with his materialism. Preliterate truths were conditional; they were negotiated and tested by outcomes. Literacy separated truth from practice. It became possible to inscribe truth, to carry it on through time and space, amplifying its sacred meaning until it gained the status of the ultimate law. The so-called alphabet effect,[i] according to physicist and media theorist Robert K. Logan, went further. The linear code of abstract signs for meaningless sounds enabled abstract logic, monotheism, and the concept of absolute truth. The multitude of practical truths held by varying groups in various situations was replaced by one moral law: that of God. The truth inscribed in the Book was unified. In different times, the Book was the scripture, the code of laws, the textbook; all contained absolute truth. In written culture, truth belongs to nobody but solely to the highest authority and can only be interpreted. In the same way that digital speech combines oral and written speech, digital orality combines the preliterate and literate epistemologies of truth. The emancipation of authorship by the Internet undermined preexisting authorities, including the authority of absolute truth. As millions of people entered the business of meaning-production, the broadcasting of absolute truth lost its monopoly. Scriptures and textbooks forfeited their power. The caste of priest-interpreters was replaced by multiple crowd-sourced interpretations of the world. People now vote for truth with clicks. Truth is again up for negotiation. In the digital world, the truth of a given statement can be confirmed \u2013 again \u2013 by the practical outcomes that it generates. But these practical outcomes now happen in digital, not physical, reality. Since digital reality presents the world through the views of others, the truth of everyone is defined by the truths of others. Social media have legitimized crowd-sourced truths as a side effect of their design. Since online engagement is built on responses, the Viral Inquisitor demands that everyone relate to the truths of others.",
    "comments": [],
    "description": "Social media increasingly serve not to facilitate the simple exchange of written information but to sort out everyone\u2019s attitude toward the most pressing issues of the day. The wrong response to someone\u2019s hard-fought truth is punished by reciprocal aggression and various forms of ostracism. An excerpt from the chapter \"The Viral Inquisitor\" in The Viral\u2026",
    "document_uid": "25a2de3925",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969948",
    "title": "Ask HN: Anyone know this is happening?",
    "url": "https://news.ycombinator.com/item?id=41969948",
    "score": 1,
    "timestamp": "2024-10-28T12:22:39",
    "source": "Hacker News",
    "content": "I havent&#x27; smoked weed since college, I&#x27;m about to turn 50. I started smoking it through a pipe recently, i ordered it from weedmaps, but what I&#x27;m wondering is the nug only gives me like 2-3 good hits, and then its dust -- is that normal? I havent&#x27; smoked since the 90s.<p>Back when I did smoke in highschool we used to pass the whole thing around a few times. Why am I only getting a few hits?",
    "comments": [],
    "description": "No description available.",
    "document_uid": "a9a89db211",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969910",
    "title": "SimBa: Simplicity Bias for Scaling Parameters in Deep Reinforcement Learning",
    "url": "https://sonyresearch.github.io/simba/",
    "score": 1,
    "timestamp": "2024-10-28T12:17:45",
    "source": "Hacker News",
    "content": "TL;DR Stop worrying about algorithms, just change the network architecture to SimBa Overview. SimBa infuses simplicity bias through architectural changes, without modifying the underlying deep RL algorithm. (a) SimBa enhaces sample efficiency: Sample efficiency across various RL algorithms, including off-policy model-free (SAC), off-policy model-based (TD-MPC2), on-policy model-free (PPO), and unsupervised (METRA) RL methods. (b) Off-policy RL Benchmark When applied to SAC, SimBa matches or surpasses state-of-the-art off-policy RL methods with minimal computational overhead across 51 continuous control tasks, by only modifying the network architecture and scaling up the number of network parameters. Abstract We introduce SimBa, an architecture designed to inject simplicity bias for scaling up the parameters in deep RL. Simba consists of three components: (i) standardizing input observations with running statistics, (ii) incorporating residual feedforward blocks to provide a linear pathway from the input to the output, and (iii) applying layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms\u2014including off-policy, on-policy, and unsupervised methods\u2014is consistently improved. Moreover, when SimBa is integrated into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across 51 tasks from DMC, MyoSuite, and HumanoidBench, solely by modifying the network architecture. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments. SimBa Architecture SimBa comprises three components: Running Statistics Normalization, Residual Feedforward Blocks, and Post-Layer Normalization. These components lower the network's functional complexity, enhancing generalization for highly overparameterized configurations. SimBa with On-Policy RL Comparison of PPO with and without SimBa for Craftax. SimBa with Unsupervised RL Comparison of METRA with and without SimBa for Humanoid in DMC. Paper SimBa: Simplicity Bias for Scaling Up Parameters for Deep RL Hojoon Lee&ast;, Dongyoon Hwang&ast;, Donghu Kim, Hyunseung Kim, Jun Jet Tai, Kaushik Subramanian, Peter R. Wurman, Jaegul Choo, Peter Stone, Takuma Seno arXiv preprint Citation If you find our work useful, please consider citing the paper as follows: @article{lee2024simba, title={SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning}, author={Hojoon Lee and Dongyoon Hwang and Donghu Kim and Hyunseung Kim and Jun Jet Tai and Kaushik Subramanian and Peter R. Wurman and Jaegul Choo and Peter Stone and Takuma Seno}, journal={arXiv preprint arXiv:2410.09754}, year={2024} }",
    "comments": [],
    "description": "SimBa: Simplicity Bias for Scaling Parameters in Deep Reinforcement Learning",
    "document_uid": "e9b67c1939",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969908",
    "title": "AgiBot X1, a modular humanoid robot with high dof",
    "url": "https://github.com/AgibotTech/agibot_x1_train",
    "score": 1,
    "timestamp": "2024-10-28T12:17:12",
    "source": "Hacker News",
    "content": "AgibotTech/agibot_x1_train You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "The reinforcement learning training code for AgiBot X1. - AgibotTech/agibot_x1_train",
    "document_uid": "d7efbf42ea",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969899",
    "title": "Pimp My Man",
    "url": "https://kszenes.github.io/blog/2024/Manpager/",
    "score": 1,
    "timestamp": "2024-10-28T12:15:23",
    "source": "Hacker News",
    "content": "Introduction This article follows up on my previous blog post on improving the experience of digging through the documentation of command line interface tools. In that article, I detailed my preferred method for fetching documentation using the cheat.sh website. This method, however, requires an internet connection. What if Microsoft decides to do another automatic software update, during the night, and crashes the worldwide web, again (looking at you CrowdStrike)? This would leave the rest of humanity with only the man pages to piece together all of human knowledge. Could we not make the experience of interacting with the man pages a bit more pleasant? That will be the goal of this article. Did you know that you can choose the application that is used for displaying the man pages? By default, it is set to the less pager, which doesn\u2019t provide the best user experience due to its lack of proper syntax highlighting. The environment variable MANPAGER defines the program used for viewing the man pages. bat My favorite alternative to less is the bat program. It is part of series of common GNU CLI tools that have been rewritten in Rust, and often extended with additional functionality. This list includes bat as a drop-in replacement for cat and less, fd for find and ripgrep for grep. bat provides syntax highlighting for most programming languages and has even replaced cat and less for me entirely: alias less=\"bat\" alias cat=\"bat -pp\" By default, bat acts like a pager similar to less. The -pp flag disables the paging option and prints the entire file, making bat function exactly like cat. Moreover, it can be used as the MANPAGER with the following command taken from their README export MANPAGER=\"sh -c 'col -bx | bat -l man -p'\" Check out the results: Comparison of less (left) and bat (right) as Manpagers On my Mac, this command worked out of the box. However, on my Linux machine I was encountering formatting issues due to lingering color codes in the output (e.g., 1mgrep). From the bat documentation, this issue can be resolved by setting the following environment variable export MANROFFOPT=\"-c\" Neovim Alternatively, you can use neovim as your MANPAGER which will even respect your color scheme export MANPAGER='nvim +Man!' Comparison of less (left) and neovim (right) as Manpagers However, since it also loads all of your plugins (unless you were diligent in setting up lazy loading of packages), it will necessarily be a bit slower than bat, which is why I use it instead. Source I was made aware of the MANPAGER environment variable by this video from DistroTube. Go check him out, he makes great Linux related content!",
    "comments": [],
    "description": "Replace the default `man` pager with something with a bit more bling",
    "document_uid": "713b924970",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969896",
    "title": "Subsurface Microbial Colonization in 2B-Year-Old Mafic Rock",
    "url": "https://link.springer.com/article/10.1007/s00248-024-02434-8",
    "score": 1,
    "timestamp": "2024-10-28T12:14:43",
    "source": "Hacker News",
    "content": "Whitman WB, Coleman DC, Wiebe WJ (1998) Prokaryotes: the unseen majority. Proc Natl Acad Sci 95:6578\u20136583Article CAS PubMed PubMed Central Google Scholar McMahon S, Parnell J (2014) Weighing the deep continental biosphere. FEMS Microbiol Ecol 87:113\u2013120Article CAS PubMed Google Scholar Magnabosco C, Lin L-H, Dong H, Bomberg M, Ghiorse W, Stan-Lotter H, Pedersen K, Kieft T, Van Heerden E, Onstott TC (2018) The biomass and biodiversity of the continental subsurface. Nat Geosci 11:707\u2013717Article CAS Google Scholar Bar-On YM, Phillips R, Milo R (2018) The biomass distribution on Earth. Proc Natl Acad Sci 115:6506\u20136511Article CAS PubMed PubMed Central Google Scholar Price PB, Sowers T (2004) Temperature dependence of metabolic rates for microbial growth, maintenance, and survival. Proc Natl Acad Sci 101:4631\u20134636Article CAS PubMed PubMed Central Google Scholar Hoehler TM, J\u00f8rgensen BB (2013) Microbial life under extreme energy limitation. Nat Rev Microbiol 11:83\u201394Article CAS PubMed Google Scholar Lin L-H, Wang P-L, Rumble D, Lippmann-Pipke J, Boice E, Pratt L, Lollar B, Brodie E, Hazen T, Andersen G, DeSantis T, Moser D, Kershaw D, Onstott T (2006) Long-term sustainability of a high-energy, low-diversity crustal biome. Science (New York, NY) 314:479\u2013482. https://doi.org/10.1126/science.1127376Article CAS Google Scholar Becraft ED, Lau Vetter MC, Bezuidt OK, Brown JM, Labont\u00e9 JM, Kauneckaite-Griguole K, Salkauskaite R, Alzbutas G, Sackett JD, Kruger BR (2021) Evolutionary stasis of a deep subsurface microbial lineage. ISME J 15:2830\u20132842Article CAS PubMed PubMed Central Google Scholar Bornemann TL, Adam PS, Turzynski V, Schreiber U, Figueroa-Gonzalez PA, Rahlff J, K\u00f6ster D, Schmidt TC, Schunk R, Krauthausen B (2022) Genetic diversity in terrestrial subsurface ecosystems impacted by geological degassing. Nat Commun 13:284Article CAS PubMed PubMed Central Google Scholar Suzuki Y, Trembath-Reichert E, Drake H (2022) The rocky biosphere: new insights from microbiomes at rock-water interfaces and their interactions with minerals. Front Microbiol 13:1102710Article PubMed PubMed Central Google Scholar Takamiya H, Kouduka M, Suzuki Y (2021) The deep rocky biosphere: new geomicrobiological insights and prospects. Front Microbiol 12:785743Article PubMed PubMed Central Google Scholar Templeton AS, Ellison ET, Glombitza C, Morono Y, Rempfert KR, Hoehler TM, Zeigler SD, Kraus EA, Spear JR, Nothaft DB (2021) Accessing the subsurface biosphere within rocks undergoing active low-temperature serpentinization in the Samail ophiolite (Oman Drilling Project). J Geophys Res: Biogeosci 126:e2021JG006315Article Google Scholar Morono Y, Terada T, Kallmeyer J, Inagaki F (2013) An improved cell separation technique for marine subsurface sediments: applications for high-throughput analysis using flow cytometry and cell sorting. Environ Microbiol 15:2841\u20132849Article CAS PubMed PubMed Central Google Scholar Sueoka Y, Yamashita S, Kouduka M, Suzuki Y (2019) Deep microbial colonization in saponite-bearing fractures in aged basaltic crust: implications for subsurface life on Mars. Front Microbiol 10:2793Article PubMed PubMed Central Google Scholar Suzuki Y, Yamashita S, Kouduka M, Ao Y, Mukai H, Mitsunobu S, Kagi H, D\u2019Hondt S, Inagaki F, Morono Y (2020) Deep microbial proliferation at the basalt interface in 33.5\u2013104 million-year-old oceanic crust. Commun Biol 3:1\u20139Article Google Scholar Buick IS, Maas R, Gibson R (2001) Precise U-Pb titanite age constraints on the emplacement of the Bushveld Igneous Complex, South Africa. J Geol Soc 158:3\u20136Article CAS Google Scholar Trumbull R, Ashwal L, Webb S, Veksler I (2015) Drilling through the largest magma chamber on Earth: Bushveld Igneous Complex Drilling Project (BICDP). Sci Drill 19:33\u201337Article Google Scholar Richardson SH, Shirey SB (2008) Continental mantle signature of Bushveld magmas and coeval diamonds. Nature 453:910\u2013913Article CAS PubMed Google Scholar Wilson AH, Zeh A, Gerdes A (2017) In situ Sr isotopes in plagioclase and trace element systematics in the lowest part of the eastern Bushveld Igneous Complex: dynamic processes in an evolving magma chamber. J Petrol 58:327\u2013360Article CAS Google Scholar Zeh A, Ovtcharova M, Wilson AH, Schaltegger U (2015) The Bushveld Igneous Complex was emplaced and cooled in less than one million years\u2013results of zirconology, and geotectonic implications. Earth Planet Sci Lett 418:103\u2013114Article CAS Google Scholar Wilson AH (2015) The earliest stages of emplacement of the eastern Bushveld Igneous Complex: development of the Lower Zone, Marginal Zone and Basal Ultramafic Sequence. J Petrol 56:347\u2013388Article CAS Google Scholar Warr O, Ballentine CJ, Onstott TC, Nisson DM, Kieft TL, Hillgonds DJ, Lollar BS (2023) 86Kr excess and other noble gases identify a billion-year-old radiogenically-enriched groundwater system. Nat Commun 13:3768. https://doi.org/10.1038/s41467-022-31412-2Article CAS Google Scholar Sherwood LB, Heuer VB, McDermott J, Tille S, Warr O, Moran JJ, Telling J, Hinrichs K-U (2021) A window into the abiotic carbon cycle\u2013acetate and formate in fracture waters in 2.7 billion year-old host rocks of the Canadian Shield. Geochim Cosmochim Acta 294:295\u2013314Article Google Scholar Lima C, Muhamadali H, Xu Y, Kansiz M, Goodacre R (2021) Imaging isotopically labeled bacteria at the single-cell level using high-resolution optical infrared photothermal spectroscopy. Anal Chem 93:3082\u20133088Article CAS PubMed Google Scholar Friese A, Kallmeyer J, Axel Kitte J, Monta\u00f1o Mart\u00ednez I, Bijaksana S, Wagner D, Team ILCDSTatITDS (2017) A simple and inexpensive technique for assessing contamination during drilling operations. Limnology and Oceanography: Methods 15:200\u2013211 Google Scholar Lever MA, Alperin M, Engelen B, Inagaki F, Nakagawa S, Steinsbu BO, Teske A (2006) Trends in basalt and sediment core contamination during IODP Expedition 301. Geomicrobiol J 23:517\u2013530Article CAS Google Scholar Keeling JL, Raven MD, Gates WP (2000) Geology and characterization of two hydrothermal nontronites from weathered metamorphic rocks at the Uley graphite mine, South Australia. Clays Clay Miner 48:537\u2013548Article CAS Google Scholar Ellerbrock R, Stein M, Schaller J (2022) Comparing amorphous silica, short-range-ordered silicates and silicic acid species by FTIR. Sci Rep 12:11708Article CAS PubMed PubMed Central Google Scholar Movasaghi Z, Rehman S, ur Rehman DI, (2008) Fourier transform infrared (FTIR) spectroscopy of biological tissues. Appl Spectrosc Rev 43:134\u2013179Article CAS Google Scholar Sforna MC, Brunelli D, Pisapia C, Pasini V, Malferrari D, M\u00e9nez B (2018) Abiotic formation of condensed carbonaceous matter in the hydrating oceanic crust. Nat Commun 9:1\u20138Article CAS Google Scholar Russell BF, Phelps TJ, Griffin WT, Sargent KA (1992) Procedures for sampling deep subsurface microbial communities in unconsolidated sediments. Groundwater Monit Remediat 12:96\u2013104Article Google Scholar Smith DC, Spivack AJ, Fisk MR, Haveman SA, Staudigel H (2000) Tracer-based estimates of drilling-induced microbial contamination of deep sea crust. Geomicrobiol J 17:207\u2013219Article",
    "comments": [],
    "description": "Recent advances in subsurface microbiology have demonstrated the habitability of multi-million-year-old igneous rocks, despite the scarce energy supply fro",
    "document_uid": "0c1a3799b5",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969891",
    "title": "Eng org seniority-mix model",
    "url": "https://lethain.com/engineering-cost-model/",
    "score": 1,
    "timestamp": "2024-10-28T12:14:03",
    "source": "Hacker News",
    "content": "One of the trademarks of private equity ownership is the expectation that either the company maintains their current margin and grows revenue at 25-30%, or they instead grow slower and increase their free cash flow year over year. In many organizations, engineering costs have a major impact on their free cash flow. There are many costs to reduce, cloud hosting and such, but inevitably part of the discussion is addressing engineering headcount costs directly.One of the largest contributors to engineering headcount costs is your organization\u2019s seniority mix: more senior engineers are paid quite a bit more than earlier career engineers. This model looks at how various policies impact an organization\u2019s seniority mix.In this chapter, we\u2019ll work to:Summarize this model\u2019s learnings about policy impact on seniority mixSketch the model\u2019s stocks and flowsUse lethain/systems to iteratively build and exercise the full modelTime to start modeling.This is an exploratory, draft chapter for a book on engineering strategy that I\u2019m brainstorming in #eng-strategy-book. As such, some of the links go to other draft chapters, both published drafts and very early, unpublished drafts.LearningsAn organization without a \u201cbackfill at N-1\u201d hiring policy, e.g. an organization that hires a SWE2 to replace a departed SWE2, will have an increasingly top-heavy organization over time.However, even introducing the \u201cbackfill at N-1\u201d hiring policy is insufficient, as our representation in senior levels will become far too high, even if we stop hiring externally into our senior-most levels.To fully accomplish our goal of a healthy seniority mix, we must stop hiring at senior-most levels, implement a \u201cbackfill at N-1\u201d policy, and cap the maximum number of individual at the senior-most level.Any collection of lower-powered policies simply will not impact the model\u2019s outcome.SketchWe\u2019ll start by sketching this system in Excalidraw. It\u2019s always fine to use whatever tool you prefer, but in general the lack of complexity in simple sketching tools focuses you on iterating on the stocks and flows\u2013without getting distracted by tuning settings\u2013much like a designer starting with messy wireframes rather than pixel-perfect designs.We\u2019ll start with sketching the junior-most level: SWE1.We hire external candidates to become SWE1s. We have some get promoted to SWE2, some depart, and then backfill those departures with new SWE1s.As we start sketching the full stocks and flows for SWE2, we also introduce the idea of backfilling at the prior level. As we replicate this pattern for two more career levels\u2013SWE3 and SWE4\u2013we get the complete model.The final level, SWE4, is simplified relative to the prior levels, as it\u2019s no longer possible to get promoted to a further level. We could go further than this, but the model will simply get increasingly burdensome to work with, so let\u2019s stop with four levels.ReasonReviewing the sketched system, a few interesting conclusions come out:If promotion rates at any level exceed the rate of hiring at that level plus rate of N-1 backfill at that level, then the proportion of engineers at that level will grow over timeIf you are not hiring much, then this problem simplifies to promotion rate versus departure rate. A company that does little hiring and has high retention cannot afford to promote frequently. Promotion into senior roles will become financially restrained, even if the policy is explained by some other mechanismMany companies use the \u201ccareer level\u201d policy as the mechanism to identify a level where promotions generally stop happening. The rationale is often not explicitly described, but we can conclude it\u2019s likely a financial constraint that typically incentivizes this policyWith those starter insights, now we can get into modeling the details,.Model & ExerciseWe\u2019re going to build this model using lethain/systems. The first version will be relatively simple, albeit with a number of stocks given the size of the model, and then we\u2019ll layer on a number of additional features as we iteratively test out a number of different scenarios.I\u2019ve chosen to combine the Model and Exercise steps to showcase how each version of the model can inspire new learnings that prompt new questions, that require a new model to answer.If you\u2019d rather view the full model and visualizations, each iteration is available on github.Backfill-at-levelThe first policy we\u2019re going to explore is backfilling a departure at the same level. For example, if a SWE2 departs, then you go ahead and backfill them at SWE2. This intuitively makes sense, because you needed a SWE2 before to perform the work, so why would you hire something less senior?There are two new systems concepts introduced in this model:For easier iteration, we\u2019re going to introduce the concept of using a stock as a variable by initializing HiringRate with a size of two, and then using that size as the rate that we hire additional.There are effectively an infinite number of potential candidates for your company, so we\u2019re going to use an infinite stock, represented by initializing a new stock surroundined by [ and ]. Specifically in this case this is [Candidates], if we wanted a fixed size stock with 100 people in it, we could have initialized it as Candidates(100). Depending on what you\u2019re modeling both options are useful.With those in mind, our initial model is defined as:HiringRate(2) [Candidates] > SWE1(10) @ HiringRate SWE1 > DepartedSWE1 @ Leak(0.1) DepartedSWE1 > SWE1 @ Leak(0.5) Candidates > SWE2(10) @ HiringRate SWE1 > SWE2 @ Leak(0.1) SWE2 > DepartedSWE2 @ Leak(0.1) DepartedSWE2 > SWE2 @ Leak(0.5) Candidates > SWE3(10) @ HiringRate SWE2 > SWE3 @ Leak(0.1) SWE3 > DepartedSWE3 @ Leak(0.1) DepartedSWE3 > SWE3 @ Leak(0.5) Candidates > SWE4(0) @ HiringRate SWE3 > SWE4 @ Leak(0.1) SWE4 > DepartedSWE4 @ Leak(0.1) DepartedSWE4 > SWE4 @ Leak(0.5) To confirm that we\u2019ve done something reasonable, we can model this using Graphviz.That looks like the same model we sketched before, without the downlevel backfill flows that we haven\u2019t yet added to the model, so we\u2019re in a good spot.With that confirmed, lets inspect the four distinct flows happening for the SWE2 stock. In order they are:External candidates being hired at the SWE2 level, at the fixed HiringRate defined here as 2 hires per roundSWE1s being promoted to",
    "comments": [],
    "description": "One of the trademarks of private equity ownership is the expectation that either the company maintains their current margin\nand grows revenue at 25-30%, or they instead grow slower and increase their free cash flow year over year.\nIn many organizations, engineering costs have a major impact on their free cash flow.\nThere are many costs to reduce, cloud hosting and such, but inevitably part of the discussion is\naddressing engineering headcount costs directly.",
    "document_uid": "e92a794d85",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969878",
    "title": "The Distaste for Housing Density",
    "url": "https://www.nber.org/papers/w33078",
    "score": 1,
    "timestamp": "2024-10-28T12:12:43",
    "source": "Hacker News",
    "content": "We thank Braydon Neiszner and Vivian Wang for helpful research assistance. We also appreciate the comments of Hector Blanco (discussant), Gilles Duranton, Fernando Ferreira, Ed Glaeser, Matthew Turner and seminar participants at the University of Wisconsin, the 2024 UEA North American Conference and the Urban Lunch Seminar at Wharton. Naturally, we remain responsible for the final product. Gyourko thanks the Research Sponsor Program of the Zell/Lurie Real Estate Center for financial support, and McCulloch thanks the Population Studies and Training Center at Brown University, which receives funding from the NIH, for training support (T32 HD007338) and for general support (P2C HD041020). The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.",
    "comments": [],
    "description": "Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.",
    "document_uid": "d349b562f7",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969859",
    "title": "Golang Nugget \u2013 Oct 28th Edition \u2013 From Testing Tools for gRPC to Delve",
    "url": "https://mondaynugget.com/golang/2024/10/28/golang-nugget/",
    "score": 1,
    "timestamp": "2024-10-28T12:10:47",
    "source": "Hacker News",
    "content": "Welcome to this week\u2019s edition of Golang Nugget, your go-to source for the latest insights and techniques in the Go programming world! This week, we\u2019re covering: Testing Tools for gRPC: Discover tools like the \u201ctests-coverage-tool\u201d for gRPC services to ensure thorough testing, and see how Mutexes can help prevent race conditions in concurrent programming. Goroutines vs. Threads: Understand why Goroutines outperform traditional threads, making them ideal for scalable applications. Simplify setup with sync.Once and learn tips to write cleaner Go code. Debugging and Task Management: Dive into debugging Go core dumps with Delve, and try out Go-Taskflow for managing complex task dependencies. Happy coding! The article introduces the tests-coverage-tool, a Golang-based utility for measuring requirements coverage in gRPC services, focusing on proto contracts rather than deep business logic. It highlights the importance of ensuring all gRPC service methods and fields are covered by automated tests, especially as services grow and evolve. The tool automatically gathers coverage data, requiring only proper configuration, and is designed to work with gRPC services, though the concept can be adapted for other protocols. The tool\u2019s architecture includes two projects: the main tool and a reporting submodule. It uses a gRPC interceptor to collect coverage data during test execution, saving results in JSON format. The tool\u2019s configuration is managed via a YAML file, and integration into tests is straightforward, requiring minimal code changes. The tool generates detailed HTML and JSON reports, providing insights into coverage percentages, covered methods, and fields, including deprecated ones. It aids QA engineers in identifying coverage gaps and planning test coverage for new functionalities. While it automates coverage measurement, it doesn\u2019t assess business logic, which requires manual evaluation. The tool is available on GitHub and is recommended for projects using gRPC. Here\u2019s a snippet of the interceptor code: func CoverageInterceptor() grpc.UnaryClientInterceptor { return func(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error { invokerErr := invoker(ctx, method, req, reply, cc, opts...) result, err := buildCoverageResult(method, req, reply) if err != nil { log.Printf(\"Error building coverage result: %v\", err) return invokerErr } toolConfig, err := config.NewConfig() if err != nil { log.Printf(\"Error building config: %v\", err) return invokerErr } filename := fmt.Sprintf(\"%s.json\", uuid.New().String()) resultsDir := toolConfig.GetResultsDir() if err = utils.SaveJSONFile(result, resultsDir, filename); err != nil { log.Printf(\"Error saving coverage result: %v\", err) } return invokerErr } } Read more\u2026 The post describes a race condition scenario in a banking system simulation using goroutines in Go, where two users attempt to withdraw funds from the same bank account concurrently. This race condition arises because both goroutines check the account balance simultaneously and proceed with withdrawals without synchronization, potentially leading to an incorrect final balance. For instance, if the initial balance is $1000, Goroutine 1 might withdraw $700, and Goroutine 2 might also withdraw $500 based on the same initial balance, resulting in a negative balance of $-200. To resolve this issue, a sync.Mutex is introduced to ensure mutual exclusion, allowing only one goroutine to access and modify the balance at a time, thus preventing the race condition. The critical section in the withdraw method is locked using the mutex, ensuring that Goroutine 2 waits for Goroutine 1 to complete its operation before proceeding. type BankAccount struct { balance int mu sync.Mutex } func (a *BankAccount) withdraw(amount int) { a.mu.Lock() // Lock to prevent race condition defer a.mu.Unlock() if a.balance >= amount { time.Sleep(time.Millisecond * 100) a.balance -= amount fmt.Printf(\"Successfully withdrew $%d, remaining balance: $%d\\n\", amount, a.balance) } else { fmt.Printf(\"Failed to withdraw $%d, insufficient balance. Current balance: $%d\\n\", amount, a.balance) } } Read more\u2026 The post discusses the advantages of using Goroutines in Go for handling concurrency over traditional system threads. Goroutines are lightweight, memory-efficient, and managed by Go\u2019s runtime, making them faster and more efficient than system threads, which suffer from high memory consumption, expensive management, and context switching overhead. The provided code example demonstrates launching 10,000 concurrent tasks using Goroutines, showcasing their ability to handle large workloads without overwhelming system resources. By setting GOMAXPROCS(1), the example highlights how Goroutines can efficiently multiplex tasks onto a single OS thread. The use of sync.WaitGroup ensures all Goroutines complete before the program exits. The post concludes that Goroutines are ideal for building scalable, efficient systems, offering significant performance benefits for modern applications like web servers and microservices. Here\u2019s a snippet of the code: package main import ( \"fmt\" \"runtime\" \"sync\" \"time\" ) func worker(id int) { fmt.Printf(\"Worker %d starting\\n\", id) time.Sleep(time.Second) fmt.Printf(\"Worker %d done\\n\", id) } func main() { runtime.GOMAXPROCS(1) var wg sync.WaitGroup for i := 1; i <= 10000; i++ { wg.Add(1) go func(id int) { defer wg.Done() worker(id) }(i) } wg.Wait() fmt.Println(\"All workers completed\") } Read more\u2026 DoltHub is developing Dolt, a SQL database with Git-like version control features, using Go. The blog post discusses Go\u2019s nil channel behavior, which blocks indefinitely on send and receive operations, potentially causing deadlocks. However, this behavior is useful for selectively disabling branches in a select statement, enabling idiomatic patterns like optional functionality and state machines. For instance, a channel can be optionally nil to enforce timeouts or coordinate sending values to multiple channels without additional synchronization primitives. The post provides examples, such as a Debounce function that manages input/output timing and a Batch function that transforms a channel into batches of a maximum size. These patterns simplify code by avoiding multiple select statements for different channel states. Here\u2019s a snippet illustrating optional functionality with a timeout: var timeout chan time.Time if enforceTimeout { timeout = time.After(10 * time.Second) } select { case recv := <-recvCh: return recv, nil case <-timeout: return nil, errors.New(\"timed out\") } Read more\u2026 If you\u2019ve ever faced race conditions while trying to initialize shared resources in Go, sync.Once is a lifesaver. It ensures a block of code runs only once, even when called from multiple goroutines, by using a mutex for synchronization. This is particularly useful for singleton initialization, resource management, configuration loading, and setting up event handlers. Here\u2019s a simple example: package main import ( \"fmt\"",
    "comments": [
      {
        "author": "moeinxyz",
        "text": "I\u2019m excited to share the latest edition of Golang Nugget, the weekly newsletter focused on curating the best Golang content to keep you informed and ahead in the community.<p>This week, we cover:<p>Testing Tools for gRPC: Discover essential tools like the \u201ctests-coverage-tool\u201d to ensure thorough testing of your gRPC services, and learn how Mutexes can help you prevent race conditions in concurrent programming.<p>Goroutines vs. Threads: Understand why Goroutines outperform traditional threads, making them ideal for scalable applications. We also share tips on using sync.Once to simplify setup and write cleaner Go code.<p>Debugging and Task Management: Dive into debugging Go core dumps with Delve, and explore Go-Taskflow for managing complex task dependencies.<p>Golang Nugget is all about bringing you concise insights and valuable resources each week. If you\u2019re looking to stay on top of the latest in Golang, consider subscribing!",
        "time": "2024-10-28T12:10:47"
      }
    ],
    "description": "Welcome to this week\u2019s edition of Golang Nugget, your go-to source for the latest insights and techniques in the Go programming world! This week, we\u2019re covering: Testing Tools for gRPC: Discover tools like the \u201ctests-coverage-tool\u201d for gRPC services to ensure thorough testing, and see how Mutexes can help prevent race conditions in concurrent programming. Goroutines vs. Threads: Understand why Goroutines outperform traditional threads, making them ideal for scalable applications. Simplify setup with sync.Once and learn tips to write cleaner Go code. Debugging and Task Management: Dive into debugging Go core dumps with Delve, and try out Go-Taskflow for managing complex task dependencies. Happy coding! Test coverage visualization for gRPC services The article introduces the tests-coverage-tool, a Golang-based utility for measuring requirements coverage in gRPC services, focusing on proto contracts rather than deep business logic. It highlights the importance of ensuring all gRPC service methods and fields are covered by automated tests, especially as services grow and evolve. The tool automatically gathers coverage data, requiring only proper configuration, and is designed to work with gRPC services, though the concept can be adapted for other protocols. The tool\u2019s architecture includes two projects: the main tool and a reporting submodule. It uses a gRPC interceptor to collect coverage data during test execution, saving results in JSON format. The tool\u2019s configuration is managed via a YAML file, and integration into tests is straightforward, requiring minimal code changes. The tool generates detailed HTML and JSON reports, providing insights into coverage percentages, covered methods, and fields, including deprecated ones. It aids QA engineers in identifying coverage gaps and planning test coverage for new functionalities. While it automates coverage measurement, it doesn\u2019t assess business logic, which requires manual evaluation. The tool is available on GitHub and is recommended for projects using gRPC. Here\u2019s a snippet of the interceptor code: func CoverageInterceptor() grpc.UnaryClientInterceptor { return func(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error { invokerErr := invoker(ctx, method, req, reply, cc, opts...) result, err := buildCoverageResult(method, req, reply) if err != nil { log.Printf(\"Error building coverage result: %v\", err) return invokerErr }...",
    "document_uid": "dacc6cbf50",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969852",
    "title": "AI-based medical transcription tool hallucinates, raises concerns",
    "url": "https://www.tomshardware.com/tech-industry/artificial-intelligence/concerns-about-medical-note-taking-tool-raised-after-researcher-discovers-it-invents-things-no-one-said-nabla-is-powered-by-openais-whisper",
    "score": 3,
    "timestamp": "2024-10-28T12:08:55",
    "source": "Hacker News",
    "content": "Researchers and engineers using OpenAI\u2019s Whisper audio transcription tool have said that it often includes hallucinations in its output, commonly manifested as chunks of text that don't accurately reflect the original recording. According to the Associated Press, a University of Michigan researcher said that he found made-up text in 80% of the AI tool\u2019s transcriptions that were inspected, which led to him trying to improve it.AI hallucination isn\u2019t a new phenomenon, and researchers have been trying to fix this using different tools like semantic entropy. However, what\u2019s troubling is that the Whisper AI audio transcription tool is widely used in medical settings, where mistakes could have deadly consequences.For example, one speaker said, \u201cHe, the boy, was going to, I\u2019m not sure exactly, take the umbrella,\u201d but Whisper transcribed, \u201cHe too a big piece of a cross, a teeny, small piece \u2026 I\u2019m sure he didn\u2019t have a terror knife so he killed a number of people.\u201d Another recording said, \u201ctwo other girls and one lady,\u201d and the AI tool transcribed this as \u201ctwo other girls and one lady, um, which were Black.\u201d Lastly, one medical-related example showed Whisper writing down \u201chyperactivated antibiotics\u201d in its output, which do not exist.Nabla, an AI assistant used by over 45,000 cliniciansDespite the above news, Nabla, an ambient AI assistant that helps clinicians transcribe the patient-doctor interaction, and create notes or reports after the visit, still uses Whisper. The company claims that over 45,000 clinicians across 85+ health organizations use the tool, including Children\u2019s Hospital Los Angeles and Mankato Clinic in Minnesota.Even though Nabla is based on OpenAI\u2019s Whisper, the company\u2019s Chief Technology Officer, Martin Raison, says that its tool is fine-tuned in medical language to transcribe and summarize interaction. However, OpenAI recommends against using Whisper for crucial transcriptions, even warning against using it in \u201cdecision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes.\u201dThe company behind Nabla says that it\u2019s aware of Whisper\u2019s tendency to hallucinate and that it\u2019s already addressing the problem. However, Raison also said that it cannot compare the AI-generated transcript with the original audio recording, as its tool automatically deletes the original audio for data privacy and safety. Fortunately, there\u2019s no recorded complaint yet against a medical provider due to hallucination by their AI notes-taking tools.Even if that\u2019s the case, William Saunders, a former OpenAI engineer, said that removing the original recording could be problematic, as the healthcare provider wouldn\u2019t be able to verify if the text is correct. \u201cYou can\u2019t catch errors if you take away the ground truth,\u201d he told the Associated Press.Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.Nevertheless, Nabla requires its users to edit and approve transcribed notes. So, if it could deliver the report while the patient is still in the room with the doctor, it would give the healthcare provider a chance to verify the veracity of its results based on their recent memory and even confirm information with the patient if the data delivered by the AI transcription is deemed inaccurate.This shows that AI isn\u2019t an infallible machine that gets everything right \u2014 instead, we can think of it as a person who can think quickly, but its output needs to be double-checked every time. AI is certainly a useful tool in many situations, but we can\u2019t let it do the thinking for us, at least for now.",
    "comments": [],
    "description": "Researchers have found that OpenAI's Whisper audio transcriber is prone to hallucination \u2014 and that it's what powers one of the more popular AI transcription services that doctors use.",
    "document_uid": "b56812f9b6",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969844",
    "title": "Show HN: I made a local web-based notes app in the spirit of \"One Big Text File\"",
    "url": "https://github.com/freetonik/textpod",
    "score": 2,
    "timestamp": "2024-10-28T12:07:09",
    "source": "Hacker News",
    "content": "Textpod is a simple UI on top of a single notes.md file + a few features I always wanted:<p>- add + in front of a link to fetch a local copy of the page (downloaded in the background using monolith[1])<p>- search is integrated into the note input, just start typing with &#x2F;<p>- attach images and other files (stored in &#x27;attachments&#x27; directory)<p>I made a 1-minute demo video (no sound) [2].<p>When you start the binary, the data is served from the current directory: the notes file and the attachments directory are created automatically. Run multiple instances from different directories, if needed (I do that to separate work and personal notes).<p>It&#x27;s a very rough initial version thrown together on a Sunday evening. I hope to add nicer error messages and some customizations later.<p>[1] <a href=\"https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;monolith\" rel=\"nofollow\">https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;monolith</a><p>[2] <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VAqJJxaJNVM\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VAqJJxaJNVM</a>",
    "comments": [],
    "description": "Extremely simple note-taking app inspired by \"One Big Text File\" - freetonik/textpod",
    "document_uid": "b3b7fe5f59",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969839",
    "title": "Golang developers should try Odin",
    "url": "https://rm4n0s.github.io/posts/2-go-devs-should-learn-odin/",
    "score": 1,
    "timestamp": "2024-10-28T12:06:37",
    "source": "Hacker News",
    "content": "(BEWARE: the article may cause seizures) Introduction I\u2019ve been using Golang for 10 years, and even though I tried other programming languages, I always went back to it because of its simplicity. It is so simple that reading the source code and playing around helped me pick up the majority of the syntax when I first got into Golang. However, it pains me to say that Golang has problems because of its garbage collector and runtime. Some of the problem are listed here: CGO It runs and compiles slowly You have to know and write C in comments!! It can not compile to WASM The language is slow compared to Rust It is easy to have memory leaks and they are hard to find. The error type is not good But even with these problems, I still prefer Golang to other programming languages, until I met Odin a week ago, and it was love at first sight. This article will show you the beauty of Odin. What is Odin and who is it for Odin is a new programming language that is 60% Golang, 20% features that you wished Golang had 10% array programming 10% low level, FFI, memory management People think that it is only for game development just because it includes 3D libraries with the compiler, but I disagree. Odin is also for backend development because it has a core library similar to Go\u2019s standard library. Which means that it has all the building blocks for us to copy the rest of Go\u2019s libraries like Chinese manufacturers. I know that some of you play around with Rust, Zig, C3 or Hare, but these do not have array programming and struct tags in their languages. Just ask yourself: how will you serialize data between DBs and markup languages without struct tags? how will you write a server side anticheat without array programming? Don\u2019t you want to make linux gamers happy? It even has quaternions! Do you know what that is? Me neither! But I am excited to learn. Odin is ready The language will not change and it will stay stable until you retire. However, the compiler, toolchain, and core library are still in development and always improved. Also, it is used in production from JangaFX and ChiAha\u2122 The characteristics of Odin The language has 31 keywords, but each keyword may have a #directive or @(attribute). It may seem too much for someone that cannot remember more than 26 keywords like me, but all the keywords, directives, and attributes connect so seamlessly together like reading English. For example, the switch-case that works like in Golang requires from the user to specify every case, unless you put the #partial directive in front of it. It will really make sense to you how easy it is for you to understand the code without knowing the language. Take 10 minutes to read demo.odin, it\u2019s got almost everything you need to get started. Yes, it does not have documentation (no, the overview is not a real documentation), but that is the beauty of it. You learn to code in Odin by reading other\u2019s people code. Most of the standard library does not have comments or examples, but you can easily understand how to use it through its source code. No, it does not have macros, or comptime, or constexpr or decorators and it is better that way because every language that has them makes developers to waste 90% of their time googling for every library that uses them. Trust me, stick with Odin\u2019s pre-defined directives and attributes. Yes, it has generics, and they are more well-designed than Golang\u2019s generics. No, it does not have a package manager, or go.mod file or anything else. The package is just a directory that you just copy to your project, and you call it relative to the path of the source code file. For example, // git clone https://github.com/laytan/odin-http // vim main.odin package main import http \"odin-http\" Yes, compiler is as simple as Golang\u2019s compiler. cd myproject odin build . odin run main.odin -file Yes, it has VSCode plugin with autocompletion. The trade offs Odin has manual memory management, which is why it does not have Closures, Composition, Goroutines, and Selectors. However, I will demonstrate how you can live without them. Second, it lacks libraries, which is why I\u2019m trying to get you to learn it and get your libraries from Go to Odin. Lastly, it does not have documentation or books, but if you have years of experience in Go, then Odin will feel like home. Living without a garbage collector Error handling Living without a garbage collector means that you don\u2019t have errors.New() or fmt.Errorf(). There is no error type in Odin. You only have Enums, Unions and Structs, and that is better than Go\u2019s errors. Go\u2019s errors suck Golang\u2019s errors makes you push server-side error messages to users, and you cannot do anything to stop it. That is not a good thing because users don\u2019t need to know what is SQL or that the bank_account is nil. Furthermore, another problem is that Go\u2019s errors do not have stack traces. Now look at the superpower of Odin For each function you create, also include an enum or a union or a struct to represent the error for that specific function. Payment_Error :: enum { None, Bank_Account_Is_Empty, } Pay_Half_Debt_Error :: union #shared_nil { Payment_Error, json.Marshal_Error, } Save_House_Error :: union #shared_nil { Pay_Half_Debt_Error, Is_Even_Worth_Saving_Error, } // etc When a function returns an error that it received from another function, and so forth, you will receive a type similar to this. err := Extend_Deadline_Error( Save_House_Error( Pay_Half_Debt_Error( Payment_Error.Bank_Account_Is_Empty ) ), ) From there you can create trees of switch statements that return proper user messages. #partial switch save_house in err { case Save_House_Error: pay_half := save_house.(Pay_Half_Debt_Error) #partial switch pay in pay_half { case Payment_Error: #partial switch pay { case .Bank_Account_Is_Empty: fmt.println(\"You have 24 hours to leave the house.\") } } } And also you can print it like a",
    "comments": [],
    "description": "",
    "document_uid": "b73841193f",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969834",
    "title": "From Freelance Struggles to Videiro: A New Way to Find Top Video Editors",
    "url": "https://medium.com/@thegreatgm20/from-freelance-struggles-to-videiro-a-new-way-to-find-top-video-editors-844f4d8a36c9",
    "score": 1,
    "timestamp": "2024-10-28T12:05:44",
    "source": "Hacker News",
    "content": "From Freelance Struggles to Videiro: A New Way to Find Top Video EditorsCredits: Videiro.comIt was 2020 when I first discovered video editing. I was in quarantine and was experimenting with youtube channels. I realized that video editing was essential for creating a viral youtube video.During the following year I got more and more into video editing until i finally landed my first client. This was a pivotal moment, because all the research and outreach had finally paid off.As time has passed, I realize that I was probably not the best freelance video editor I could have been.Finding jobs as a freelance editor can be hardThe problem with freelancing in general is that there is no official place for you to reach your audience. There are many tools you can use but most times they prove unsuccessful. I used to search all over Twitter(X), reddit and all that.Eventually, after building a decent portfolio I landed a client. What I slowly realized, is that there is no stable place to promote or have your services.This made me more and more interested in the idea of a forum, or a place were content creators can connect with Video Editors.Finding a good video editor can be even harderAs there are no official reliable sources to hire video editors, content creators tend to post listings wherever they can find. Although there are hundreds of thousands, potentially millions of video editors out there, only a really small part of those editors are actually reliable. Distinguishing between a newbie editor and a reliable sometimes can be hard. You don\u2019t want to give them a paid sample to do but you want to see they are a good fit.VideiroAfter researching a lot and from what I understood from my short lived freelancing career, there isn\u2019t a forum of any kind for content creators to find reliable, vetted, professional, curated hand picked individuals.That is why I decided to launch videiro.com.Videiro is a video editor recruitment website. Unlike other similar websites, videiro editors are of a certain quality standard. We have recruited some of the top talents in the industry.Building VideiroAfter trying the Agency style, I realized that that was not a viable option, since there was essentially no way to restrict Client-Editor contact. I essentially worked as a middleman between video editors and clients. Since I could not guarantee payment, contact had to be limited, that meant there were many problems with communications and time zones. The client was 10 hours ahead and wanted something delivered in 2 hours while the Editor was sleeping. This created many problems. Essentially, after finding 2 or 3 clients I realized this was not the way to go.Slowly after that, I started developing the idea for Videiro. A forum of some kind were there is open communication between clients and editors. That was the main idea. After that I slowly started to realize I had to make it happen. That was when I approached my dear friend 4rkal to help me build the website. With his extensive programming knowledge and my \u201cvision\u201d we worked for months to create a website.The development was quite brutal, every time we thought it was finished, we got a new idea or found something that was missing.Screenshot: Early version of the Videiro website (Credits: videiro.com)Of course, building the website was not the only challenge. One of the greatest challenges we faced was the recruitment process. How could we make sure that we had the best Video Editors we could possibly find.Our recruitment processThere are two ways we recruit editors for videiro. One way is we have an application form on our website available to anyone who wants to apply. We review those applications every week and only recruit a small percentage of them. Our editors have to have at least 1 reference, proven professional experience and a high number of edited views.Our second recruitment path is through direct recruitment. Our team is actively searching for the best Video Editors out there. If we come across an editor with an impressive portfolio we may recruit them to our website.How it works:For Editors:Video Editors easily apply using a form on our website. We actively review applications and only choose editors of a certain quality standard that have proven experience working with large YouTubers or other groups.For Clients:Clients, or as we call them at videiro \u201cCreators\u201d, can easily sign up to our website either by their email or with their google account. After that, they will be prompted with a posting screen. There, they can write all the requirements and qualifications they want from an ideal editor. When they finish writing the post, they are directed to a payment screen, where they have to pay the $99 listing fee. Then, the post is reviewed and uploaded within 24 hours.How our Video Editing Service Works (Screenshot: videiro.com website, credits: videiro.com)When the listing is live, only our select team of high skilled video editors can view the post. They can apply for the position and the rest is history! Also, we don\u2019t restrict contact between Editors and Creators because we want to make the experience as smooth as possible.Thank you for reading my article, if you enjoyed it please let me know. Also, please feel free to ask any questions in the comments and I will make sure to reply.Resources:If you would like to find out more about the development of Videiro, you can follow 4rkal\u2019s blog, were he writes, and will be writing more articles about how he programmed Videiro into what it is now.https://4rkal.com/tags/videiro",
    "comments": [],
    "description": "How a creator\u2019s journey from freelance editing struggles led to Videiro\u2014a platform built to connect YouTubers and content creators with top video editors, making high-quality, reliable editing talent accessible for everyone.",
    "document_uid": "5381334f5b",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969832",
    "title": "A Somewhat Opinionated Guide to Effective ZFS Snapshots",
    "url": "https://kimono-koans.github.io/opinionated-guide/",
    "score": 1,
    "timestamp": "2024-10-28T12:05:29",
    "source": "Hacker News",
    "content": "Much too long ago, I was asked by a user of httm to describe the way I (over)use snapshots, and, though I said I would explain, I never thought I had anything important enough to say about best practices. However, I'm increasingly seeing posts on r/ZFS requesting instruction of how best to use the snapshot mechanisms of ZFS, and though I can only share my opinions regarding my perhaps very idiosyncratic snapshot setup, maybe these opinions could be helpful for those just starting out. Of course, this Getting Started guide will make some assumptions that may not apply to your setup. This guide assumes you're using: ZFS for a data and a root pool Running Ubuntu But I'm certain at least some of this advice will be useful even if you're on a different setup. I believe an effective snapshot scheme is composed of possibly three types of snapshots: Periodic Snapshots Triggered Snapshots Dynamic Snapshots Let's discuss each. Periodic Snapshots\u00a7 Periodic snapshots, or snapshots taken at a regular intervals, should be considered the base of any good snapshot scheme, and I wholeheartedly recommend sanoid as a periodic snapshot tool. For those that don't already know, sanoid describes itself as \"a policy-driven snapshot management tool for ZFS filesystems\". Although other tools exist and perhaps deserve mention, sanoid and syncoid, its replication tools, have made periodic snapshots and replication simple for me. But note -- like any good tool -- sanoid isn't a perfect fit for every use case (and shouldn't be, see later zsys and the \"Curse of Trying to Do Too Much\"). But what sanoid is is simple and composable. To my mind, what makes sanoid great is that it doesn't force you into using all its features, all the time. It doesn't force you into a complicated policy/scheme (even I can do it!), and since it includes such good documentation, I won't waste time discussing how to initially configure your pools here. Instead, I'd like to highlight, how, in contrast to some other tools, it allows you to compose it with your other Linux utilities, and doesn't break when you do something a little different. For instance, suppose you want to sleep your NAS drives occasionally and not invoke sanoid when your drives are sleeping. Here's how this user solved this particular problem. First, you'll need to check if any of the spinning rust drives are sleeping: find /dev/disk/by-id/ -type l | \\ grep -v -e part -e wwn | \\ while read disk; do if [[ $(lsblk -o rota \"$disk\" | grep -c \"1\") -gt 0 ]]; then smartctl -d sat --nocheck=standby \"$disk\" fi done Then, it becomes simple to check that condition whenever you invoke sanoid: ... [[ $( /usr/local/sbin/checkHDstatus | grep -i 'Device' | /usr/bin/grep -i -c 'STANDBY' ) -gt 0 ]] || \\ /usr/sbin/sanoid --prune-snapshots --verbose --configdir=/etc/sanoid/ ... Now imagine you want to use an alternate configuration file when that spinning rust is asleep. Just change the configuration file path: ... if [[ $( /usr/local/sbin/checkHDstatus | grep -i 'Device' | /usr/bin/grep -i -c 'STANDBY' ) -gt 0 ]]; then /usr/sbin/sanoid --take-snapshots --verbose --configdir=/etc/sanoid/awake/ else /usr/sbin/sanoid --take-snapshots --verbose --configdir=/etc/sanoid/sleep/ fi ... sanoid handles all this in stride. It doesn't panic when you skip a few hours worth of snapshots on a certain pool. It doesn't need to control heaven and earth. It just keeps trucking. Replicating\u00a7 Pop Quiz Hotshot: now that you've made a few snapshots, how would you replicate your rpool to your local datapool using your own custom zfs send/recv options? Easy peasy, you say. Just like sanoid, syncoid has simple options, sane defaults, and an ability to cut a rug when you need to. /usr/sbin/syncoid -r --sendoptions=\"L ec\" --recvoptions=\"o recordsize=1M o compression=zstd\" \\ --force-delete --exclude=scratch --exclude=test --exclude=tmp rpool datapool/rpool 2>&1 | logger -t syncoid Triggered Snapshots\u00a7 Canonical's zsys promised snapshots of every system update and seamless rollback on boot as well as periodic snapshots of significant datasets/directories. It's unfortunate zsys wasn't ready for the Ubuntu 22.04 release, and may never be ready. I won't rehash \"Why?\" here, but one basic zsys premise is sound: Periodic Snapshots are not enough. You may ask -- why? Because, for me, it's sometimes important to know that a snapshot was triggered on a date and time certain. Let's discuss a few examples of triggers you might like to use for a snapshot and how you might take those snapshots. Before a System Upgrade\u00a7 The first triggers we might consider are snapshots upon apt upgrade and kernel updates. First, you'll need a snapshot script to execute (perhaps called /usr/local/sbin/snapPrepApt): DATE=\"$( /bin/date +%F-%T )\" # FYI a user helpfully notes there may be some issue with snapshot-ing a bpool and GRUB # See: https://github.com/kimono-koans/httm/issues/11#issuecomment-1860329869 #zfs snapshot -r bpool@snap_\"$DATE\"_prepApt zfs snapshot -r bpool/BOOT@snap_\"$DATE\"_prepApt zfs snapshot rpool@snap_\"$DATE\"_prepApt zfs snapshot -r rpool/ROOT@snap_\"$DATE\"_prepApt zfs snapshot -r rpool/USERDATA@snap_\"$DATE\"_prepApt Next, you'll need to execute such a script automatically upon apt upgrade. A simple script in /etc/apt/apt.conf.d will suffice: // Takes a snapshot of the system before package changes. DPkg::Pre-Invoke {\"[ -x /usr/local/sbin/snapPrepApt ] && /usr/local/sbin/snapPrepApt || true\";}; And you will also probably want to execute a script each time you update your kernel. A script invoked from /etc/kernel/preinst.d might look something like: [ -x /usr/local/sbin/snapPrepApt ] && /usr/local/sbin/snapPrepApt || true Before Service Launch\u00a7 Sometimes you will want to take a snapshot when a service starts up or shuts down. For instance perhaps you have a service, with a database, that needs to be cleanly shutdown so that its state can also be cleanly snapshot-ed. Just add a little script to execute before or after start up via systemctl edit: ... [Service] ExecStartPre=/bin/bash -c \"/usr/local/sbin/snapDataService\" ... On Network Mount\u00a7 Perhaps you want to take a snapshot every time a network drive is mounted or unmounted. So, when you or a program deletes something over the network, you have snapshot of the state just prior to mount or just after unmount. Your smb.conf allows you to execute scripts just like this: ... [TM Volume] path = \"/srv/timemachine\"",
    "comments": [],
    "description": "kimono koans' blog! A Somewhat Opinionated Guide to Effective ZFS Snapshots ",
    "document_uid": "1d31890366",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969829",
    "title": "Elon Musk Says a $25,000 Tesla Is Now Pointless",
    "url": "https://www.motortrend.com/news/tesla-q3-earnings-call-25k-tesla-roadster-elon-musk/",
    "score": 3,
    "timestamp": "2024-10-28T12:05:22",
    "source": "Hacker News",
    "content": "Access Denied You don't have permission to access \"http://www.motortrend.com/news/tesla-q3-earnings-call-25k-tesla-roadster-elon-musk/\" on this server. Reference #18.be361060.1730115429.37b635de https://errors.edgesuite.net/18.be361060.1730115429.37b635de",
    "comments": [],
    "description": "No description available.",
    "document_uid": "c8dc393d60",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41969826",
    "title": "Adobe Says Artists Should Embrace AI If They Want to Be Successful",
    "url": "https://80.lv/articles/adobe-says-artists-should-embrace-ai-if-they-want-to-be-successful/",
    "score": 1,
    "timestamp": "2024-10-28T12:04:57",
    "source": "Hacker News",
    "content": "Frankly speaking, Adobe's decision to triple down on AI, while absolutely despicable for many Digital Artists, was seen from miles away and evident to those who witnessed the massive controversy the company faced in June.Back then, the community lambasted the developer for changes to its General Terms of Use, which required users of Adobe products to allow the company to access and view their creations through both automated and manual methods, and even analyze their work using techniques such as machine learning.This led many to believe that the company planned to use all user-generated content to train its AI models, and even though Adobe has since clarified that they don't intend to do that, the belief can hardly be described as unfounded, considering that the developer is now disregarding the anti-AI sentiment of many creators in order to keep biggering on its artificial thneeds.",
    "comments": [
      {
        "author": "wengo314",
        "text": "* &quot;if they want Adobe to be successful&quot;.<p>i wonder how this all win play out in the end.",
        "time": "2024-10-28T12:15:54"
      }
    ],
    "description": "Photoshop and Substance 3D developer now openly antagonizes anti-AI creators by doubling down on generative AI.",
    "document_uid": "ebae72c306",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41971182",
    "title": "Dramatic drop in marijuana use among U.S. youth over a decade",
    "url": "https://www.fau.edu/newsdesk/articles/marijuana-use-teens-study.php",
    "score": 1,
    "timestamp": "2024-10-28T15:11:24",
    "source": "Hacker News",
    "content": "Dramatic Drop in Marijuana Use Among U.S. Youth Over a Decade Current marijuana use among adolescents decreased from 23.1% in 2011 to 15.8% in 2021. First-time use before age 13 also dropped from 8.1% to 4.9%. Marijuana has emerged as one of the most commonly used illicit substances among adolescents in the United States. Given the rising number of states legalizing recreational marijuana for adults and the decreasing perception of risk among adolescents, tracking trends in youth marijuana use is more crucial than ever. Researchers from Florida Atlantic University\u2019s Schmidt College of Medicine conducted a comprehensive study using data from the Youth Risk Behavior Survey between 2011 and 2021, which surveyed 88,183 adolescents in grades nine through 12. Researchers focused on the overall time trends in use of marijuana as well as variations by gender, race/ethnicity, and school grade. These data provide important insights into the changing landscape of marijuana use among American youth. The study, published in the journal Pediatric Reports, reveals that one of the most striking results from the analysis is the significant decrease in the percentage of adolescents reporting current marijuana use. In 2011, 23.1% of adolescents indicated they were current users, but by 2021, this figure had dropped to 15.8%. Additionally, the percentage of adolescents trying marijuana for the first time before age 13 also saw a notable decline, from 8.1% in 2011 to 4.9% in 2021. In 2021, marijuana use was most prevalent among 12th graders (22.4%), followed by 11th graders (18.7%), with lower usage rates in the earlier grades. From 2011 to 2021, all grades experienced a notable decline in current marijuana use, especially among ninth graders. While there was an overall downward trend over the years, there were slight increases in use in 2013 and again in 2019. \u201cWhile we observed an overall decline from 2011 to 2021 across all grades, older students consistently reported higher usage, particularly 12th graders. This suggests that as adolescents advance through high school, they may have greater access to marijuana, influenced by more developed peer networks and increased independence,\u201d said Panagiota \u201cYiota\u201d Kitsantas, Ph.D., corresponding author and professor and chair of the Department of Population Health and Social Medicine, FAU Schmidt College of Medicine. \u201cThis trend highlights the need for targeted interventions aimed at older adolescents, who are at a greater risk of regular marijuana use.\u201d One of the most significant findings of this study is the shift in trends by gender, with girls surpassing boys in reported marijuana use by 2021. In 2021, girls reported a higher prevalence of current marijuana use (17.8%) than boys (13.6%). This marks a significant change from 2011 when boys were more likely to use marijuana (25.9%) compared to girls (20.1%). The convergence of usage rates among genders highlights an evolving dynamic that could reflect broader societal changes in attitudes toward marijuana. The researchers also found variations among Asian, Hispanic and white adolescents, who experienced some of the steepest declines in current use. In 2021, however, Black adolescents reported a notably higher percentage of current marijuana use at 20.5%, compared to their white (14.8%), Hispanic (16.7%), and Asian (5.1%) counterparts. This indicates a persistent racial disparity in marijuana use among adolescents that warrants further examination. \u201cIn the U.S. the current landscape of marijuana legalization in adults adds a complex layer to the issues of adolescent marijuana use. As more states continue to legalize recreational marijuana, the accessibility and perceived normalcy of the drug may increase, particularly for adolescents who may view its legal status as an indication of safety or acceptability,\u201d said Charles H. Hennekens, M.D., FACPM, co-author, the first Sir Richard Doll Professor of Medicine and Preventive Medicine in the departments of medicine and population health and social medicine, and senior academic advisor, FAU Schmidt College of Medicine. \u201cResearch suggests that marijuana legalization in adults can influence adolescent behavior through their perceptions of less risk as well as increased availability, both of which may impede efforts to reduce adolescent use.\u201d The authors emphasize the importance of interventions like parental communication, supervision and modeling, alongside schools offering effective health education and fostering a positive school climate, to sustain the decline in adolescent marijuana use. Findings from the study highlight the need for ongoing monitoring and intervention strategies to address marijuana use among U.S. adolescents. By focusing on the specific needs of various demographic groups, including different grade levels, genders and racial/ethnic communities, public health initiatives can more effectively address the risks associated with adolescent marijuana use and foster healthier outcomes for future generations. Regular or heavy marijuana use during adolescence can adversely affect cognitive development, leading to poor learning, working memory issues, and attention deficits, regardless of educational background or verbal intelligence. Adolescents who use marijuana are two to three-and-a-half times more likely to have lower grade point averages and face a fourfold increase in psychosis diagnoses in adulthood. Research shows that marijuana use disrupts brain function by reducing synaptic pruning, resulting in increased gray matter volume and decreased communication efficiency in higher-order brain areas. Study co-authors are Jack Yang, first author and a second-year FAU medical student; Maria C. Mejia, M.D., a professor; and Lea Sacca, Ph.D., an assistant professor, both within the Department of Population Health and Social Medicine, FAU Schmidt College of Medicine. -FAU-",
    "comments": [],
    "description": "Using a national survey of 88,183 U.S. adolescents in grades nine to 12, a study shows marijuana use declined from 2011 to 2021. Findings also reveal changing landscapes of use by gender, race, and ethnicity.",
    "document_uid": "e2fedab151",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971170",
    "title": "The Tomb Raiders: How Freeports Enabled International Art Theft",
    "url": "https://lithub.com/the-real-tomb-raiders-how-freeports-enabled-international-art-theft/",
    "score": 1,
    "timestamp": "2024-10-28T15:10:37",
    "source": "Hacker News",
    "content": "On a steamy day in August 1995, a retired Italian customs cop named Pasquale Camera was driving home after lunch about an hour and a half south of Rome when he lost control of his beige Renault and went careening into the railing on the side of the highway. When the police arrived to investigate the accident, paramedics had already pronounced him dead at the scene; his car had flipped over, and without his seat belt he hadn\u2019t stood a chance.Article continues after advertisement The traffic police also found, in the glove compartment of Camera\u2019s car, photographs of vases, sculptures, and various other artifacts that looked like they could be antiques. By chance, the local chief had previously worked on a team that specialized in tracking down looted antiquities, so he called his former colleagues in Rome. Incredibly, they had been on Camera\u2019s trail for months. The photos got the police a search warrant, which led to a raid of Camera\u2019s apartment, which turned up a chart of the names of people in the antiques trade, along with their locations. That analog piece of evidence, laid out neatly in Camera\u2019s handwriting, led investigators down a rabbit hole of museum robbers, art dealers, collectors, and shell companies hidden in off- shore jurisdictions. The probe had every feature of an archaeological dig, only instead of excavating dirt and rocks to find buried treasure, investigators sifted through layer upon layer of fictitious corporate entities before landing in a Geneva storage room leased by an art dealer named Giacomo Medici. On the fourth floor of a steel-gray warehouse, a trove of illegally obtained Greek, Roman, and Etruscan antiquities\u2014some looted from archaeological digs in Italy, some with Sotheby\u2019s labels dangling from them\u2014were arranged on shelves and in boxes, like apples and bananas in a supermarket. \u201cAll the cupboards were shelved\u2014and each and every one of the shelves was packed\u2014crowded, teeming, overloaded with antiquities: with vases, statues, bronzes; with candelabra, frescoes, mosaics; with glass objects, faience animals, jewelry, and still more vases,\u201d write Peter Watson and Cecilia Todeschini in their book on the heist, The Medici Conspiracy. There were also invoices, checks, letters, and IOUs.Article continues after advertisement It was clear that the outer room was where Medici received prospective buyers, and where objects for sale were displayed in secure and discreet circumstances. It was equally clear\u2026that Medici had never expected anyone to come calling here\u2014everything was just lying around, with no attempt at concealment. The warehouse in question was not your average mini storage. It was the Geneva Freeport: a place where, since 1888, goods have entered the building and remained there, perhaps even for lifetimes, accruing value, hiding from scrutiny, evading taxes, even changing hands, all without leaving the confines of the warehouse. The warehouse had stood largely unnoticed in a nondescript commercial district of Geneva for years, until all of a sudden, it found itself at the center of spectacular highway accidents, unscrupulous millionaires, a team of cops called the \u201cart squad,\u201d and real-life tomb raiders. The warehouse had stood largely unnoticed in a nondescript commercial district of Geneva for years, until all of a sudden, it found itself at the center of spectacular highway accidents, unscrupulous millionaires, a team of cops called the \u201cart squad,\u201d and real-life tomb raiders. The investigation reached halfway across the world, prompting the Italian authorities to seek stolen artworks in museums from Boston to Toledo, Ohio. It ensnared figures linked to institutions as far away as the Metropolitan Museum of Art and the Getty. It also led the Swiss to regulate the storage of ancient artifacts and empower the police to conduct inspections of the warehouse. The Medici affair turned the lights on, if briefly, in this cloistral corner of the hidden globe. But as quickly as it began, the scandal faded from memory and the freeport returned to obscurity. The lights wouldn\u2019t be back on for more than a decade.Article continues after advertisement * Freeports are all around us. Known interchangeably as foreign-trade zones, free-trade zones, or free economic zones, they are designated areas, often but not always near an airport, seaport, or border, where goods can enter a country and be kept there without being subject to that country\u2019s trade tariffs or other tax regulations. They can be stand-alone warehouses or industrial sites, entire districts, or one floor of an office building. If you live in the United States, chances are, the car you drive, the appliances in your kitchen, and the Amazon packages on your doorstep all spent time in a freeport of some kind before making their way to you. The freeport was conceived hundreds of years ago in Italy, to serve two commercial needs. Merchants on long journeys could use the storage facilities to stow grain and other perishable merchandise at a foreign port for a short time without formally importing it or having to deal with customs authorities. And the governments in charge of their respective locations could use these zones\u2014which included warehouses or silos, but also land around them\u2014to relax certain rules and benefit from the influx of certain types of foreign commerce (and people) without committing to more wide- ranging domestic reforms. Freeports function thanks to what\u2019s essentially a legal hack that creates a new, different set of boundaries for new, different people and things: \u201ceconomic dualism\u201d is how one academic paper describes it. And like the empires and nations that established them, freeports have come in all shapes, sizes, and configurations. What they have in common is the way they fence off what happens inside.Article continues after advertisement The early Tuscan freeport of Livorno, for instance, emerged in the late sixteenth century, an era when the notion of a territorial state, with discrete boundaries, laws, and hierarchies, was inching away from the chaotic fiefdoms of the Middle Ages and toward something a little more orderly, with plenty of gray area left to exploit in between. The grand duke of Tuscany used Livorno as a kind of sandbox, inviting",
    "comments": [],
    "description": "On a steamy day in August 1995, a retired Italian customs cop named Pasquale Camera was driving home after lunch about an hour and a half south of Rome when he lost control of his beige Renault and\u2026",
    "document_uid": "8ab82fe50f",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971157",
    "title": "Glasstron",
    "url": "https://en.wikipedia.org/wiki/Glasstron",
    "score": 1,
    "timestamp": "2024-10-28T15:09:23",
    "source": "Hacker News",
    "content": "From Wikipedia, the free encyclopedia Head-mounted display Glasstron was a series of portable head-mounted displays released by Sony, initially introduced in 1996 with the model PLM-50.[1][2] The products featured two LCD screens and two earphones for video and audio respectively. The products are no longer manufactured nor supported by Sony.[citation needed] The Glasstron was not the first head-mounted display by Sony, with the Visortron being a previous exhibited unit.[3][4] The Sony HMZ-T1 can be considered a successor to Glasstron.[2] The head-mounted display developed for Sony during the mid-1990s[which?] by Virtual i-o is completely unrelated to the Glasstron.[1] One application of this technology was in the game MechWarrior 2, which permitted users to adopt a visual perspective from inside the cockpit of the craft, using their own eyes as visual and seeing the battlefield through their craft's own cockpit.[5] Five models were released.[citation needed] Supported video inputs included PC (15 pin, VGA interface), Composite and S-Video. A brief list of the models follows: Model number Year of release Notes PLM-50 1996[6] Released June 1996 in Japan.[6] PLM-A35 1997[7] The most basic model with opaque lenses and has SVGA input.[citation needed] Released June 1997 in USA.[7] PLM-A55 1997[7] This model had a mechanical shutter to allow the display to become see through, without SVGA.[citation needed] Released June 1997 in USA.[7] PLM-100 1998[citation needed] This model had a mechanical shutter to allow the display to become see through, with SVGA, somewhat unstable.[citation needed] The PLM-100 has two color LCD displays and requires an NTSC signal.[8] PLM-S700 / PLM-S700E 1998[9] The S700 allowed for see through mode using LCD shutters and had support for SVGA input.[citation needed] Its LCD had over 1.55 million pixels on a component the size of a ten-cent coin at SVGA (800\u00d7600) display resolution.[citation needed] The S700 has NTSC input, whilst the S700E has PAL input. The S700 was released on 10 November 1998 in Japan.[9] ^ a b \"Reality Check\". Electronic Gaming Monthly. No. 85. Ziff Davis. August 1996. pp. 14\u201316. ^ a b McCracken, Harry (2 February 2012). \"Sony's Highly Personal, Surprisingly Decent 3D Viewer\". Time. Retrieved 23 September 2016. ^ \"Visortron\". Baltimore Sun. AP. 10 October 1995. Retrieved 23 September 2016. ^ Free, John (1993). \"Electronics Newsfront: ...and Visortrons from Japan\". Popular Science (March 1993): 26. Retrieved 23 September 2016. ^ Tony Sperry. Beyond 3D TV, Lulu Pres, Inc., November 2003. ^ a b \"Sony Corporate Info: Projector Head Mounted Display\". Sony. Retrieved 23 September 2016. ^ a b c d \"VR Wiki: Sony\". VR Wiki. Retrieved 23 September 2016. ^ Edwards, J. (1999). Computer Science '99: Proceedings of the 22nd Australasian Computer Science Conference, ASCC '98, Auckland, 18-21 January 1998. Springer Singapore. pp. 126\u2013127. ISBN 978-981-4021-54-8. Retrieved 2024-03-30. ^ a b \"Sony Announces New Personal LCD Monitor PC Glasstron\". Sony. 29 September 1998. Retrieved 23 September 2016.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "4aa65037b4",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971121",
    "title": "40 years later, The Terminator still shapes our view of AI",
    "url": "https://theconversation.com/the-terminator-at-40-this-sci-fi-b-movie-still-shapes-how-we-view-the-threat-of-ai-236564",
    "score": 1,
    "timestamp": "2024-10-28T15:05:27",
    "source": "Hacker News",
    "content": "October 26, 2024 marks the 40th anniversary of director James Cameron\u2019s science fiction classic, The Terminator \u2013 a film that popularised society\u2019s fear of machines that can\u2019t be reasoned with, and that \u201cabsolutely will not stop \u2026 until you are dead\u201d, as one character memorably puts it. The plot concerns a super-intelligent AI system called Skynet which has taken over the world by initiating nuclear war. Amid the resulting devastation, human survivors stage a successful fightback under the leadership of the charismatic John Connor. In response, Skynet sends a cyborg assassin (played by Arnold Schwarzenegger) back in time to 1984 \u2013 before Connor\u2019s birth \u2013 to kill his future mother, Sarah. Such is John Connor\u2019s importance to the war that Skynet banks on erasing him from history to preserve its existence. Today, public interest in artificial intelligence has arguably never been greater. The companies developing AI typically promise their technologies will perform tasks faster and more accurately than people. They claim AI can spot patterns in data that are not obvious, enhancing human decision-making. There is a widespread perception that AI is poised to transform everything from warfare to the economy. Immediate risks include introducing biases into algorithms for screening job applications and the threat of generative AI displacing humans from certain types of work, such as software programming. But it is the existential danger that often dominates public discussion \u2013 and the six Terminator films have exerted an outsize influence on how these arguments are framed. Indeed, according to some, the films\u2019 portrayal of the threat posed by AI-controlled machines distracts from the substantial benefits offered by the technology. Official trailer for The Terminator (1984) The Terminator was not the first film to tackle AI\u2019s potential dangers. There are parallels between Skynet and the HAL 9000 supercomputer in Stanley Kubrick\u2019s 1968 film, 2001: A Space Odyssey. It also draws from Mary Shelley\u2019s 1818 novel, Frankenstein, and Karel \u010capek\u2019s 1921 play, R.U.R.. Both stories concern inventors losing control over their creations. On release, it was described in a review by the New York Times as a \u201cB-movie with flair\u201d. In the intervening years, it has been recognised as one of the greatest science fiction movies of all time. At the box office, it made more than 12 times its modest budget of US$6.4 million (\u00a34.9 million at today\u2019s exchange rate). What was arguably most novel about The Terminator is how it re-imagined longstanding fears of a machine uprising through the cultural prism of 1980s America. Much like the 1983 film WarGames, where a teenager nearly triggers World War 3 by hacking into a military supercomputer, Skynet highlights cold war fears of nuclear annihilation coupled with anxiety about rapid technological change. Read more: Science fiction helps us deal with science fact: a lesson from Terminator's killer robots Forty years on, Elon Musk is among the technology leaders who have helped keep a focus on the supposed existential risk of AI to humanity. The owner of X (formerly Twitter) has repeatedly referenced the Terminator franchise while expressing concerns about the hypothetical development of superintelligent AI. But such comparisons often irritate the technology\u2019s advocates. As the former UK technology minister Paul Scully said at a London conference in 2023: \u201cIf you\u2019re only talking about the end of humanity because of some rogue, Terminator-style scenario, you\u2019re going to miss out on all of the good that AI [can do].\u201d That\u2019s not to say there aren\u2019t genuine concerns about military uses of AI \u2013 ones that may even seem to parallel the film franchise. AI-controlled weapons systems To the relief of many, US officials have said that AI will never take a decision on deploying nuclear weapons. But combining AI with autonomous weapons systems is a possibility. These weapons have existed for decades and don\u2019t necessarily require AI. Once activated, they can select and attack targets without being directly operated by a human. In 2016, US Air Force general Paul Selva coined the term \u201cTerminator conundrum\u201d to describe the ethical and legal challenges posed by these weapons. The Terminator\u2019s director James Cameron says \u2018the weaponisation of AI is the biggest danger\u2019. Stuart Russell, a leading UK computer scientist, has argued for a ban on all lethal, fully autonomous weapons, including those with AI. The main risk, he argues, is not from a sentient Skynet-style system going rogue, but how well autonomous weapons might follow our instructions, killing with superhuman accuracy. Russell envisages a scenario where tiny quadcopters equipped with AI and explosive charges could be mass-produced. These \u201cslaughterbots\u201d could then be deployed in swarms as \u201ccheap, selective weapons of mass destruction\u201d. Countries including the US specify the need for human operators to \u201cexercise appropriate levels of human judgment over the use of force\u201d when operating autonomous weapon systems. In some instances, operators can visually verify targets before authorising strikes, and can \u201cwave off\u201d attacks if situations change. AI is already being used to support military targeting. According to some, it\u2019s even a responsible use of the technology, since it could reduce collateral damage. This idea evokes Schwarzenegger\u2019s role reversal as the benevolent \u201cmachine guardian\u201d in the original film\u2019s sequel, Terminator 2: Judgment Day. However, AI could also undermine the role human drone operators play in challenging recommendations by machines. Some researchers think that humans have a tendency to trust whatever computers say. \u2018Loitering munitions\u2019 Militaries engaged in conflicts are increasingly making use of small, cheap aerial drones that can detect and crash into targets. These \u201cloitering munitions\u201d (so named because they are designed to hover over a battlefield) feature varying degrees of autonomy. As I\u2019ve argued in research co-authored with security researcher Ingvild Bode, the dynamics of the Ukraine war and other recent conflicts in which these munitions have been widely used raises concerns about the quality of control exerted by human operators. Ground-based military robots armed with weapons and designed for use on the battlefield might call to mind the relentless Terminators, and weaponised aerial drones may, in time, come to resemble the franchise\u2019s",
    "comments": [],
    "description": "The original 1984 film\u2019s vision of rogue AI turning on humanity remains hugely influential.",
    "document_uid": "7c57f1db0e",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971094",
    "title": "Ask HN: Local Price Search",
    "url": "https://news.ycombinator.com/item?id=41971094",
    "score": 1,
    "timestamp": "2024-10-28T15:02:07",
    "source": "Hacker News",
    "content": "Anyone know of a company&#x2F;project that has started to maintain a price database of services? I know yelp exists for reviews and pictures and almost everything else but you always have to dig for prices. Ie: prices for barber, cpa, coffee ...",
    "comments": [],
    "description": "No description available.",
    "document_uid": "0b6da052f8",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971088",
    "title": "PivotalTracker Alternative",
    "url": "https://litetracker.dev/yc",
    "score": 1,
    "timestamp": "2024-10-28T15:01:10",
    "source": "Hacker News",
    "content": "PivotalTracker Alternative",
    "comments": [
      {
        "author": "neshaz",
        "text": "Hi, \nWe were sadden when we heard news that Pivotal EOL April 2025, and wanted to something about it. \nDo you think this is something that community would be interested in?<p>We are building software for the past 13 years using Pivotal and wanted to something about it.<p>Currently exploring market, you can see our goals: <a href=\"https:&#x2F;&#x2F;litetracker.dev&#x2F;yc\" rel=\"nofollow\">https:&#x2F;&#x2F;litetracker.dev&#x2F;yc</a><p>What do you think?",
        "time": "2024-10-28T15:01:10"
      }
    ],
    "description": "Error fetching meta description: Exceeded maximum allowed redirects.",
    "document_uid": "e49bf3fc5a",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971083",
    "title": "SpaceX has caught a rocket. So what's next?",
    "url": "https://arstechnica.com/space/2024/10/spacex-has-caught-a-massive-rocket-so-whats-next/",
    "score": 2,
    "timestamp": "2024-10-28T15:00:58",
    "source": "Hacker News",
    "content": "SpaceX is also rapidly iterating on the design of its boosters, so the vehicles it lands today may not be ones it wants to re-fly, preferring to use improved models. For these reasons, a Super Heavy re-flight is probably at least a year away. Ground systems and LOX We're not putting a date on this one, as it will happen gradually over time. However, preparing the way for frequent Starship launches represents a significant hurdle over the next couple of years. Musk has said that, initially, SpaceX plans to build two launch towers in South Texas and two in Florida. Presumably, this is the baseline ground infrastructure needed to support the Artemis Program and the multiple refueling flights needed to enable lunar touchdowns. SpaceX still has plenty of paperwork to process for this, in the form of working with the US Space Force in Florida and clearing environmental reviews with the Federal Aviation Administration to construct these four towers and reach a high cadence of launches. Another major but unappreciated issue is commodities. At liftoff, the Super Heavy booster alone carries a mass of 7.5 million pounds (3,400 metric tons) of cryogenic propellant. Starship requires about a third as much. That sounds like a lot because it is. Liquid oxygen comprises a significant majority of this, and each launch puts a serious dent into the US production of liquid oxygen, which is used by various customers, including hospitals. Put another way, launching four Starship rockets in a single day would consume all of the nation\u2019s liquid oxygen capacity for that day. Accordingly, SpaceX must find a way to scale production of liquid oxygen, and ensure a tremendous supply to both South Texas and its future Starship launch facilities in Florida. Long-duration flight test (late 2026) NASA has consistently had this milestone on its Human Landing System schedule since the first versions of the timeline were released in August 2021. (At the time, the propellant transfer test was due to occur in the fourth quarter of 2022 and the long-duration flight test about six months later, so we\u2019re running about three years behind).",
    "comments": [],
    "description": "As remarkable as SpaceX\u2019s Starship rocket catch was, it represents but a single step on a long path to the Moon for NASA, and on to Mars.",
    "document_uid": "ba89955eac",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971060",
    "title": "M-series MacBook SSD storage upgrade (non-official) [video]",
    "url": "https://www.youtube.com/watch?v=E3N-z-Y8cuw",
    "score": 2,
    "timestamp": "2024-10-28T14:59:05",
    "source": "Hacker News",
    "content": "M-series MacBook SSD storage upgrade (non-official) [video]",
    "comments": [],
    "description": "This Video is NOT Ai generated. .Apple has been holding us back. But today, we're doing what they refuse to \u2014meet the world's first Apple Silicon MacBook Pro...",
    "document_uid": "893540b18b",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971054",
    "title": "Exercise Paradox",
    "url": "https://en.wikipedia.org/wiki/Exercise_paradox",
    "score": 1,
    "timestamp": "2024-10-28T14:58:25",
    "source": "Hacker News",
    "content": "Physiologic phenomenon The exercise paradox,[1] also known as the workout paradox,[2] refers to the finding that physical activity, while essential for maintaining overall health, does not necessarily lead to significant weight loss or increased calorie expenditure.[3] This paradox challenges the common belief that more exercise equates to more calories burned and consequently, more weight loss.[4] Hadza tribe case study[edit] The exercise paradox emerged from studies comparing calorie expenditure between different populations. Fieldwork on the Hadza people, a hunter-gatherer tribe in Tanzania, revealed that despite their high levels of physical activity, the tribe burned a similar number of calories per day as sedentary individuals in industrialized societies.[5][6] This finding, led by Duke University professor Herman Pontzer, contradicted the expectation that more active lifestyles would result in higher energy expenditures.[7] In 2012, Pontzer and his team of researchers analyzed energy expenditure in 30 Hadza adults using the doubly labeled water method. Participants consumed water enriched with two distinct isotopes of hydrogen and oxygen. The team later assessed the concentration of these isotopes in urine samples, which correlates with the body's daily carbon dioxide production and, consequently, daily energy expenditure.[8] Results indicated that Hadza women burned an average of 1,877 calories per day, which was nearly the same as the 1,864 calories burned daily by women in industrialized nations. Hadza men expended about 2,649 calories per day, which was within the standard error distribution for average calories burned by men per day in industrialized nations.[9] The studies suggest that controlling caloric intake may be more necessary for managing weight than exercise alone.[10][11] Studies on other population groups[edit] A 2011 meta-analysis of 98 studies found that individuals in low to middle-HDI countries (specifically, Bolivia, Cameroon, China, Gambia, Guatemala, India, Jamaica, Nigeria, Russia, South Africa, and Eswatini) showed no significant differences in energy expenditure compared to individuals in middle to high-HDI countries (specifically, the countries of Europe, United States, Canada, Mexico, Cuba, Chile, Brazil, Japan, Australia, and New Zealand), despite large differences in each country's obesity rate.[12] ^ Pontzer, Herman (1 February 2017). \"The Exercise Paradox\". Scientific American. 316 (2): 26\u201331. Bibcode:2017SciAm.316b..26P. doi:10.1038/scientificamerican0217-26. PMID 28118335. Retrieved 20 July 2024. ^ Kurzgesagt \u2013 In a Nutshell (16 July 2024). We Need to Rethink Exercise \u2013 The Workout Paradox. Retrieved 20 July 2024 \u2013 via YouTube. ^ Burrell, Teal. \"Why doing more exercise won't help you burn more calories\". New Scientist. Retrieved 20 July 2024. ^ \"How exercising doesn't mean you burn calories\". www.bbc.com. Retrieved 20 July 2024. ^ Pontzer, Herman; Raichlen, David A.; Wood, Brian M.; Mabulla, Audax Z. P.; Racette, Susan B.; Marlowe, Frank W. (2012-07-25). \"Hunter-Gatherer Energetics and Human Obesity\". PLOS ONE. 7 (7): e40503. Bibcode:2012PLoSO...740503P. doi:10.1371/journal.pone.0040503. ISSN 1932-6203. PMC 3405064. PMID 22848382. ^ \"Living Like a Caveman Won't Make You Thin. But it Might Make You Healthy | Duke Today\". today.duke.edu. 17 January 2019. Retrieved 20 July 2024. ^ \"How Our Evolutionary Past Shapes Our Health Today\". American Scientist. 23 February 2017. Retrieved 20 July 2024. ^ \"Comparing Calories: How the Hadza Tribe Informs our Understanding of Obesity\". Ursa Sapiens. Retrieved 20 July 2024. ^ Pontzer, Herman; Raichlen, David A.; Wood, Brian M.; Mabulla, Audax Z. P.; Racette, Susan B.; Marlowe, Frank W. (25 July 2012). \"Hunter-Gatherer Energetics and Human Obesity\". PLOS ONE. 7 (7): e40503. Bibcode:2012PLoSO...740503P. doi:10.1371/journal.pone.0040503. ISSN 1932-6203. PMC 3405064. PMID 22848382. ^ \"Colloquy Podcast: Why Exercising More May Not Help You Lose Weight | The Harvard Kenneth C. Griffin Graduate School of Arts and Sciences\". gsas.harvard.edu. Retrieved 2024-07-20. ^ Zarracina, Javier (2016-04-28). \"Why you shouldn't exercise to lose weight, explained with 60+ studies\". Vox. Retrieved 2024-07-20. ^ Dugas, Lara R; Harders, Regina; Merrill, Sarah; Ebersole, Kara; Shoham, David A; Rush, Elaine C; Assah, Felix K; Forrester, Terrence; Durazo-Arvizu, Ramon A; Luke, Amy (2011-02-01). \"Energy expenditure in adults living in developing compared with industrialized countries: a meta-analysis of doubly labeled water studies\". The American Journal of Clinical Nutrition. 93 (2): 427\u2013441. doi:10.3945/ajcn.110.007278. ISSN 0002-9165. PMC 3021434. PMID 21159791.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "f6626c5bd0",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971038",
    "title": "Show HN: Query standup data in natural language",
    "url": "https://howsthisgoing.com/",
    "score": 1,
    "timestamp": "2024-10-28T14:56:23",
    "source": "Hacker News",
    "content": "Hey HN,<p>I was a PM for ~ 4 years before deciding to ditch everything to learn how to build software on my own.<p>As a PM, I was doing standups every day with our engineers, but whenever I&#x27;d need an update I&#x27;d still end up pinging them on Slack. Why?<p>- No notes from those standup calls\n- Folks being unavailable\n- No solid insight from those standups.<p>We ended up using a Standup tool but it didn&#x27;t do much apart from converting our calls to text + making standups async. Sadly, there was still no insight.<p>So decided to do build something myself: howsthisgoing - it&#x27;s an AI-powered standup bot that lets you query team updates using natural language (built with Gemini, Claude, Python&#x2F;Django, Celery, Tailwind).<p>Demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9ijCTM3PmaU\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9ijCTM3PmaU</a><p>Technical details:\n- Set up slack apis for running standups \n- Use gemini for standup based summaries\n- embeddings using voyageai \n- natural language queries handled in 2-parts (first with claude for extraction, then gemini for filtering) \n- first time setting up RAG pipeline was fun! (let me know if you&#x27;d like to know how I did this)<p>Currently exploring integrations with Github&#x2F;Linear for deeper context on technical updates. Particularly interested in solving the challenge of connecting different data sources (commits, tickets, standups, sales updates) into a queryable knowledge base.<p>Looking for feedback on:\n- Which data sources would be most valuable to integrate?\n- Planning to pivot to an &quot;all-in-one updates app with github&#x2F;linear&#x2F;hubspot etc. being as source. What do you guys think of this idea?",
    "comments": [],
    "description": "Transform your team's productivity with the HowsThisGoing slack bot. Set up Slack standups in seconds and eliminate time-wasting meetings.",
    "document_uid": "9237009917",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971029",
    "title": "How to Setup a Local PostgreSQL Using Docker Compose",
    "url": "https://jsdev.space/howto/postgresql-docker-compose/",
    "score": 1,
    "timestamp": "2024-10-28T14:54:51",
    "source": "Hacker News",
    "content": "How to Setup a Local PostgreSQL Using Docker Compose Running a PostgreSQL database locally with Docker Compose is an efficient way to manage databases without manual installation. Here\u2019s a step-by-step guide to set it up. Prerequisites Docker and Docker Compose installed on your machine. Step 1: Create a Project Directory Create a new directory for your project to keep all files organized: mkdir postgres-docker then cd postgres-docker Step 2: Create a docker-compose.yml File Inside your project directory, create a file named docker-compose.yml and add the following configuration: 1 version: '3.8'2 3 services:4 db:5 image: postgres:latest6 container_name: local-postgres7 environment:8 POSTGRES_USER: postgres_user9 POSTGRES_PASSWORD: postgres_password10 POSTGRES_DB: postgres_db11 ports:12 - \"5432:5432\"13 volumes:14 - postgres_data:/var/lib/postgresql/data15 - ./init.sql:/docker-entrypoint-initdb.d/init.sql 16 17 volumes:18 postgres_data: Environment Variables: POSTGRES_USER: Set the username for the PostgreSQL instance. POSTGRES_PASSWORD: Set the password for the PostgreSQL instance. POSTGRES_DB: Set the default database to be created. Ports: The 5432:5432 mapping lets you access the database locally on port 5432. Volumes: postgres_data: Persist PostgreSQL data even after stopping the container. init.sql: Optional SQL script that runs automatically when the container starts (you can customize this file for initial configurations). Step 3: Start the PostgreSQL Service Run the following command to start the PostgreSQL container with Docker Compose: docker-compose up -d This command pulls the PostgreSQL image, creates the container, and starts it in detached mode (-d). Step 4: Connect to PostgreSQL Once the container is running, connect to PostgreSQL from your local machine using a tool like psql or a database GUI like DBeaver or pgAdmin. Using psql Command: psql -h localhost -p 5432 -U postgres_user -d postgres_db Replace postgres_user and postgres_db with the values from your docker-compose.yml file. Step 5: Stop and Remove the Container (Optional) To stop and remove the container, use: docker-compose down This command will stop the PostgreSQL container and remove it but keep the volume data intact for reuse. Summary Setting up PostgreSQL with Docker Compose allows you to manage your database locally with ease. By following these steps, you\u2019ll have a fully functional PostgreSQL instance running on your machine without the need for manual installation.",
    "comments": [],
    "description": "Learn how to set up a local PostgreSQL database using Docker Compose with step-by-step instructions. Simplify your database management with Docker's easy setup, connection, and persistence options.",
    "document_uid": "fad5ba1555",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41971000",
    "title": "Are we living the good life? (2021)",
    "url": "https://www.marcnitzsche.de/are-we-living-the-good-life/",
    "score": 1,
    "timestamp": "2024-10-28T14:51:42",
    "source": "Hacker News",
    "content": "Are we living the Good Life? Meet the Hazda: A tribe still living as hunters and gatherers I recently learned about the Hazda tribe.The Hazda are some of the last human hunters and gatherers alive today. They live in the open and are exposed to most of life's challenges.This makes them strong. They don't care if they get stung by a bee (or bees, plural), they don't care about pain. Their answer to \"What is the most important thing in life?\" is \"meat and honey.\"This is such a simple answer to this question, which makes a part of me believe they are living The Good Life. A life in harmony with our genetic heritage, with nature. A life that's full of actual problems, like hunger, and not of fabricated ones, like exams.The life we are evolutionary designed to find happiness in.And then I look at the way we live. I see that we became picky about our food and too comfortable in our little homes; experiencing pain from the mildest scratch, and being out of breath at the slightest exertion.Survival became too easy, and we adapted by becoming less capable. Evolution is on hold because no effort is required to fill our stomachs.And while some of us are certainly content, most of us bury our potential in so-called \"problems\" like stress and work.Removing life's challenges, such as gathering or hunting for food, gives us the time to pursue other things. It also gives us time to fabricate problems and to search for something \"higher\".Of course, this life has many advantages. We don't have to fear being eaten by a lion when we go to sleep. And humans have achieved great, awe-inspiring feats that were only possible under our current conditions, and that move me whenever I hear about them.But in terms of our happiness, were they really necessary?We weren't made for this. The life we are currently living feels too comfortable and too complex at the same time. Our bodies and minds need the stressors of nature to function properly. No strain, no gain. We have lost something precious with the simplicity of the past.The answer isn't to go back living as hunters and gatherers. But I think there is something to find here, something valuable we can learn from them. Their connection to nature. The rawness of their lives. The level of toughness.We should still enjoy the delights of civilization and technological progress. But by emulating their way of life, we might gain another level of satisfaction.",
    "comments": [],
    "description": "Meet the Hazda: A tribe still living as hunters and gatherers",
    "document_uid": "0e02bc91f8",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970996",
    "title": "Prompts Are Programs \u2013 Sigplan Blog",
    "url": "https://blog.sigplan.org/2024/10/22/prompts-are-programs/",
    "score": 1,
    "timestamp": "2024-10-28T14:51:16",
    "source": "Hacker News",
    "content": "In this post, we highlight just how important it is to understand that an AI model prompt has much in common with a traditional software program. Taking this perspective creates important opportunities and challenges for the programming language and software engineering communities and we urge these communities to undertake new research agendas to address them. Moving Beyond Chat ChatGPT, released in December 2022, had a huge impact on our understanding of what large language models (LLMs) can do and how we can use them. The millions of people who have used it understand what a prompt is and how powerful they can be. We marvel at the breadth and depth of the ability of the AI model to understand and respond to what we say and its ability to hold an informed conversation that allows us to refine its responses as needed. Having said that, many chatbot users have experienced challenges in getting LLMs to do what they want. Skill is required in phrasing the input to the chatbot so that it correctly interprets the user intent. Similarly, the user may have very specific expectations of what the chatbot produces (such as data formatted in a particular way, such as JSON object), that is important to capture in the prompt. Also, chat interactions with LLMs have significant limitations beyond challenges in phrasing a prompt. Unlike writing and debugging a piece of code, having an interactive chat session does not result in an artifact that can then be reused, shared, parameterized, etc. So, for one-off uses chat is a good experience, but for repeated application of a solution, chat falls short. Prompts are Programs The shortcomings of chatbots are overcome when LLM interactions are embedded into software systems that support automation, reuse, etc. We call such systems AI Software systems (AISW) to distinguish them from software that does not leverage an LLM at runtime (which we call Plain Ordinary Software, POSW). In this context, LLM prompts have to be considered part of the broader software system and have same robustness, security, etc. requirements that any software has. In a related blog, we\u2019ve outlined how much the evolution of AISW will impact the entire system stack. In this post, we focus on how important prompts are in this new software ecosystem and what new challenges they present to our existing approaches to creating robust software. Before proceeding, we clarify what we mean by a \u201cprompt\u201d. First, our most familiar experience with prompting is what we type into a chatbot. We call the direct input to the chatbot the user prompt. Another, more complex prompt is the prompt that was written to process the user prompt, which is often called the system prompt. The system prompt contains application-specific directions (such as \u201cYou are a chatbot\u2026\u201d) and is combined with other inputs (such as the user prompt, documents, etc.) before being sent to the LLM. The system prompt is a fixed set of instructions that define the nature of the task to be completed, what other inputs are expected, and how the output should be generated. In that way, the system prompt guides the execution of the LLM to compute a specific result, much as any software function. In the following discussion, our focus is mainly on thinking of system prompts as programs but many of the observations also directly apply to the user prompts as well. An Example of a Prompt We use the following prompt as an example, loosely adapted from a recent paper on prompt optimization to illustrate our discussion. You are given two items: 1) a sentence and 2) a word contained in that sentence. Return the part of speech tag for the given word in the sentence. This system prompt describes the input it expects (in this case a pair of a sentence such as \u201cThe cat ate the hat.\u201d and a word, such as \u201chat\u201d), the transformation to perform, and the expected structure of the output. With this example, it is easy to see that all the approaches we take to creating robust software should now be rethought in terms of how they apply to prompts. If Prompts are Programs, What is the Programming Language? There are many questions related to understanding the best way to prompt language models and it is a topic of active PL and AI research. Expressing prompts purely in natural language can be effective in practice. In addition, best practice guidelines for writing prompts often recommend structuring prompts using traditional document structuring mechanisms (like using markdown) and clearly delineating sections, such as a section of examples, output specifications, etc. Uses of templating, where parts of prompts can be substituted programmatically, are also popular. Approaches to controlling the structure and content in the output of prompts both in model training and through external specifications, such as OpenAI JSON mode, or Pydantic Validators, have been effective. Efforts have also been made to more deeply integrate programming language constructs into the prompts themselves, including the Guidance and LMQL languages, which allows additional specifications. All of these methods (1) observe the value of more explicit and precise specifications in the prompt and (2) leverage any opportunity to apply systematic checking to the resulting model output. Prompting in natural language will evolve as the rich set of infrastructures that the LLMs can interact with become available. Tools that extend the abilities of LLMs to take actions (such as retrieval augmented generation, search, or code execution) become abstractions that are available to the LLM to use but must be expressed in the prompt such that the user intent to leverage them is clear. Much PL research is required to define such tool abstractions, help LLMs choose them effectively, and help prompt writers express their intent effectively. Software Engineering for Prompts If we understand that prompts are programs, then how do we transition our knowledge and tools for building POSW so that we can create robust and effective prompts? Tooling for authoring, debugging, deploying and maintaining prompts is required and",
    "comments": [],
    "description": "Prompts are our way of communicating intent to AI foundation models and large language models. The PL and SE communities have great experience understanding how to build robust software that should\u2026",
    "document_uid": "3411e55fe0",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970995",
    "title": "Hybrid Bonding Makes Strides Toward Manufacturability",
    "url": "https://semiengineering.com/hybrid-bonding-makes-strides-toward-manufacturability/",
    "score": 1,
    "timestamp": "2024-10-28T14:51:09",
    "source": "Hacker News",
    "content": "Hybrid bonding is gaining traction in advanced packaging because it offers the shortest vertical connection between dies of similar or different functionalities, as well as better thermal, electrical and reliability results. Advantages include interconnect scaling to submicron pitches, high bandwidth, enhanced power efficiency, and better scaling relative to solder ball connections. But while some chipmakers do have hybrid bonding in high-volume manufacturing (HVM), the cost of the process is too high at present for mass adoption. And because hybrid bonding pulls together front-end and back-end lines, assembly processes such as die placement now must meet front-end specifications. Other challenges include the need for better copper dishing uniformity, faster die-to-wafer placement with superior alignment, multiple bonding and de-bonding carriers that drive up cost, and lower temperature annealing capability. Finally, particle levels must be driven down especially during die placement and dicing steps. \u201cSuccessfully scaling hybrid bonding into high-volume manufacturing requires addressing challenges related to defect control, alignment precision, thermal management, wafer warpage, material compatibility, and process throughput,\u201d said Alice Guerrero, principal applications engineer at Brewer Science. AI chiplets and modules are a huge driver for hybrid bonding and advanced packaging. Their high performance and high price tag help fuel the industry. In fact, DRAM makers are evaluating the net gain of moving from solder-bump bonding (by thermocompression) to hybrid bonding (see figure 1). The next generation scaling after hybrid bonding is sequential 3D integration, where bonding extends to even thin films. Hybrid bonding is a pivotal enabler for the larger goal of breaking up SoCs into individual technology blocks called chiplets. \u201cToday we have a kind of disaggregation of a monolithic IC, where you will have specialized technologies such as logic and SRAM memory for SoCs, logic, and I/O devices,\u201d said Eric Beyne, senior fellow, vice president for R&D, and program director for 3D system integration at imec. \u201cWe need to drive to a seemingly monolithic or a fully integrated solution, where you don\u2019t see the boundaries between the different devices. We have to break this barrier that going off-chip creates a penalty in terms of bandwidth or energy usage.\u201d High bandwidth memory (HBM) makers could move to hybrid bonding or fusion bonding (dielectric-to-dielectric), but there are drawbacks. \u201cFusion bonding is really a proven manufacturing process right now for 300mm wafers, and that bonding works very well for HBM,\u201d said Thomas Uhrmann, business development director at EV Group (EVG). \u201d HBM is currently stacking 12 chips, and the manufacturers will soon go to 16 layers. But because the performance is not the same for every chip, it\u2019s basically the weakest link limits the performance of the whole stack. It\u2019s not so much a question of yield, because the DRAM wafers are yielding very well. Speed binning is actually a big hurdle. You need to implement pre-sorting in order to compensate.\u201d How the process works The wafer-to-wafer bonding scheme was the first technology out of the gate for hybrid bonding of CMOS image sensors, where a pixel array chip is bonded to a logic chip to maximize the area for backside illumination. Now, other applications are catching on, combining processor/cache, 3D NAND, microLEDs, as well as AI modules for LLM applications like ChatGPT. The concept of chiplet integration in advanced packages offers a new level of flexibility. \u201cIn advanced packaging you can customize the system,\u201d said Jon Herlocker, CEO of Tignis. \u201cYou can say, \u2018This piece of the logic is super complex, and so I\u2019m going to go do that at an advanced node in a 300mm foundry, but I\u2019m going to take the other functions from one or more more mature nodes and place it on the same package.\u2019 You can effectively take advantage of mature nodes with their predictable, high yielding processes, and therefore lower the overall risk. So once you\u2019ve committed to doing advanced packaging \u2014 and there is a certain risk in advanced packaging \u2014 then there\u2019s all sorts of benefits for pulling as much stuff as you can out of your complex chip and using more mature technologies, then connecting it via that advanced package.\u201d Power management and the need for power efficiency are additional drivers for chip stacking and new bonding methods. Hybrid bonding enables companies to create a \u201cpath of least resistance,\u201d which means shorter interconnects, greater interconnect density, as well as greater heat removal challenges. Within this evolution is the need to reduce power consumed by semiconductors. Scalability becomes critical (see figure 2). \u201cWe have power walls, so typically energy density today will be 100 watts per square centimeter but in the future we\u2019ll need to evacuate at 500 watts per square centimeter, so quite a significant increase,\u201d said Beyne. \u201cAnd if you take 500 amperes per mm squared, sending it through the microbumps and the solder bumps may not be the best approach with currents as high as 500 amps per square millimeter. \u201cThis can be solved by integrating your power management system close to your device. And possibly we don\u2019t apply just send 1.7V through the full stack but maybe you will come up for even a higher voltage of 48V, for instance, and then use DC/DC conversion at the package or board level to reach the final voltage.\u201d Fig. 1: Fine-pitch hybrid bonding, even with backside power distribution, leads to high heat concentration that requires heat sinks. Source: imec Uhrmann noted that testing adds another layer of complexity. \u201cWhereas bumped devices can be readily tested, with hybrid bonding it\u2019s not that easy. You could create a double layer for the hybrid bond, because then you have an underlying layer that you can test, but you still need to have the bonding layer on top.\u201d How the process works The wafer-to-wafer bonding process is more mature than the die-to-wafer schemes, but it has a major disadvantage \u2014 dies must be identical in size. This has worked out well for applications like SRAM on processor stacks, but much greater flexibility in design and manufacturing requires die-to-wafer bonding, where a smaller die is bonded",
    "comments": [],
    "description": "Companies are selecting preferred flows, but the process details are changing rapidly to meet the needs of different applications.",
    "document_uid": "ba24708cfd",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970983",
    "title": "Littering in China: A Surreal Epidemic Captured in Photos (2015)",
    "url": "https://allthatsinteresting.com/littering-in-china",
    "score": 1,
    "timestamp": "2024-10-28T14:49:00",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "291ddcc110",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970978",
    "title": "Stripe's Acquisition of Bridge Is a Stroke of Genius",
    "url": "https://www.piratewires.com/p/why-stripe-s-acquisition-of-bridge-is-a-stroke-of-genius",
    "score": 2,
    "timestamp": "2024-10-28T14:48:00",
    "source": "Hacker News",
    "content": "Subscribe to The IndustryThe biggest acquisition in crypto was announced this week. Bridge, which provides stablecoin issuance and orchestration APIs, was bought by Stripe, a payments processing infrastructure company, for $1.1 billion. The deal is a sorely needed reputation buff for an industry under persistent regulatory attack and plagued by memecoins, pump-and-dumps, and endemic boom and bust cycles.Together, Bridge and Stripe make up the best of both (onchain and offchain) worlds: while Bridge creates the infrastructure to move between any form of digital dollar, Stripe powers fiat payment processing for millions of businesses. And importantly, a successful Bridge and Stripe partnership means that Stripe could own more B2B payment flows \u2014 which banks currently control \u2014 via transacting with stablecoins using Bridge\u2019s infrastructure.To understand why Stripe\u2019s acquisition of Bridge is important, we need to first understand the way the global financial system works. In its current state, there\u2019s an intricate web of intermediaries (banks, payment processors, financial institutions) each taking a cut from when a business moves money from point A to point B. Transacting with stablecoins \u2014 or onchain, tokenized dollars \u2014 requires far fewer middlemen, and as a result, is much cheaper.Stripe has always been crypto-curious. In 2014, it rolled out support for Bitcoin payments before subsequently discontinuing it \u2014 something other major payments providers never experimented with. This was a prescient and radical move, considering Bitcoin was just five years old, Coinbase had barely been around for two years, and stablecoins as we know them today didn\u2019t exist. And over the next decade, Stripe launched a dedicated crypto team to build fiat-to-crypto infrastructure \u2014 enabling easy conversion between onchain and offchain assets \u2014 and partnered with crypto exchanges, allowing users to easily get money from their bank accounts into these platforms. For Stripe, a player that has already been strategically building out its crypto arsenal for the last 10 years, marrying its existing infrastructure with Bridge\u2019s was a natural evolution.Bridge ironically started during one of the all-time worst periods for crypto: FTX had just collapsed, Terra/LUNA had imploded, and industry sentiment was terrible. But just two and a half years later, Bridge now serves as the critical infrastructure behind stablecoins: issuing them, moving them around, handling compliance and regulatory requirements, and more. With Bridge\u2019s issuance APIs, companies can create their own stablecoins which automatically earn T-bill yields during settlement, regardless of what currency users pay in.In retrospect, the acquisition shouldn\u2019t come as a surprise. Bridge basically flew under the radar as it built its business during the FTX crypto winter, and Stripe\u2019s been quietly adding to its crypto arsenal for years. Thanks to Bridge, Stripe\u2019s customers will be able to natively and programmably own the underlying yield from T-bills (obviously, a yield-bearing dollar is better than a dollar). Buying Bridge also means Stripe now has a formidable in-house cryptonative team \u2014 hyperfocused on stablecoin infrastructure \u2014 that understands the many nuances of building onchain systems.And if, long-term, stablecoins become mainstream for online payments, Stripe\u2019s economics would be transformed. Structurally, Stripe\u2019s cost of payments in their terminal state would be lowered due to stablecoins\u2019 cheap and fast settlement and transfer properties, eliminating Visa and Mastercard\u2019s 2% transaction volume fee, which is essentially lost value for both Stripe and their customers. With Bridge\u2019s infrastructure integrated into its systems, Stripe can charge its customers less, helping the company win against competitors. But critically, Stripe\u2019s cost to process each transaction is lower as well, boosting their margins and enabling both parties to make more money.But the real prize for Stripe \u2014 uniquely enabled by Bridge \u2014 could be something orders of magnitude larger: winning the B2B payments market, which is currently controlled by banks. Businesses pay businesses much, much more than consumers pay businesses. Globally, B2B payments volume is roughly $120 trillion per year (compared with Stripe\u2019s $1 trillion annual volume), with banks extracting fees at every step of the way. Businesses ultimately bear these fees, while also losing time and working capital to settlement delays.Stablecoins offer a different way: namely, a higher velocity of money via instant and 24/7 settlement \u2014 a meaningful contrast from SWIFT and wire transfers, which take up to a week (!) and are limited to business hours. Here, Bridge\u2019s infrastructure is a natural solution \u2013 helping businesses save on fees and freeing up capital that just sits in the ether for days at a time. In short, the Bridge acquisition potentially enables Stripe to both process smaller-scale Internet payments (what they do today) as well as handle massive flows of global trade via stablecoins, putting them in a position to compete with the banks that process B2B payments.In an industry that many people deem unserious, Stripe\u2019s billion-dollar vote of confidence shifts the vibe. The industry at large is celebrating the acquisition because of the precedent it sets \u2014 showing that crypto can be used to solve real-world problems and make global money movement more efficient. What gives this acquisition its gravitas is not just the stamp of credibility by Stripe, but the fact that stablecoins will now become a de facto part of more and more peoples\u2019 lives \u2014 whether they know it or not.\u2014 Bridget HarrisSubscribe to The IndustryBridget is an Associate at Founders Fund, which is an investor in Stripe.",
    "comments": [],
    "description": "controlled by banks, b2b payments account for $120 trillion yearly \u2014 compared to stripe\u2019s $1 trillion",
    "document_uid": "b9d45828f1",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970962",
    "title": "Carbon dioxide capture from open air using covalent organic frameworks",
    "url": "https://www.nature.com/articles/s41586-024-08080-x",
    "score": 1,
    "timestamp": "2024-10-28T14:46:29",
    "source": "Hacker News",
    "content": "Lackner, K., Ziock, H.-J. & Grimes, P. Carbon dioxide extraction from air: is it an option? in 24th Annual Technical Conference on Coal Utilization and Fuel Systems (Clearwater, 1999).Lackner, K. S. et al. The urgency of the development of CO2 capture from ambient air. Proc. Natl Acad. Sci. USA 109, 13156\u201313162 (2012).Article ADS CAS PubMed PubMed Central Google Scholar Sanz-P\u00e9rez, E. S., Murdock, C. R., Didas, S. A. & Jones, C. W. Direct capture of CO2 from ambient air. Chem. Rev. 116, 11840\u201311876 (2016).Article PubMed Google Scholar Shi, X. et al. Sorbents for the direct capture of CO2 from ambient air. Angew. Chem. Int. Ed. 59, 6984\u20137006 (2020).Article CAS Google Scholar Zhu, X. et al. Recent advances in direct air capture by adsorption. Chem. Soc. Rev. 51, 6574\u20136651 (2022).Article CAS PubMed Google Scholar Brethom\u00e9, F. M., Williams, N. J., Seipp, C. A., Kidder, M. K. & Custelcean, R. Direct air capture of CO2 via aqueous-phase absorption and crystalline-phase release using concentrated solar power. Nat. Energy 3, 553\u2013559 (2018).Article ADS Google Scholar Keith, D. W., Holmes, G., St. Angelo, D. & Heidel, K. A process for capturing CO2 from the atmosphere. Joule 2, 1573\u20131594 (2018).Article CAS Google Scholar Shekhah, O. et al. Made-to-order metal-organic frameworks for trace carbon dioxide removal and air capture. Nat. Commun. 5, 4228 (2014).Article ADS CAS PubMed Google Scholar McDonald, T. M. et al. Capture of carbon dioxide from air and flue gas in the alkylamine-appended metal-organic framework mmen-Mg2(dobpdc). J. Am. Chem. Soc. 134, 7056\u20137065 (2012).Article CAS PubMed Google Scholar Bien, C. E. et al. Bioinspired metal-organic framework for trace CO2 capture. J. Am. Chem. Soc. 140, 12662\u201312666 (2018).Article CAS PubMed Google Scholar Chen, O. I.-F. et al. Water-enhanced direct air capture of carbon dioxide in metal-organic frameworks. J. Am. Chem. Soc. 146, 2835\u20132844 (2024).Article CAS PubMed Google Scholar Nugent, P. et al. Porous materials with optimal adsorption thermodynamics and kinetics for CO2 separation. Nature 495, 80\u201384 (2013).Article ADS CAS PubMed Google Scholar Deutz, S. & Bardow, A. Life-cycle assessment of an industrial direct air capture process based on temperature\u2013vacuum swing adsorption. Nat. Energy 6, 203\u2013213 (2021).Article ADS CAS Google Scholar Miao, Y., He, Z., Zhu, X., Izikowitz, D. & Li, J. Operating temperatures affect direct air capture of CO2 in polyamine-loaded mesoporous silica. Chem. Eng. J. 426, 131875 (2021).Article CAS Google Scholar Rim, G., Feric, T. G., Moore, T. & Park, A. H. A. Solvent impregnated polymers loaded with liquid-like nanoparticle organic hybrid materials for enhanced kinetics of direct air capture and point source CO2 capture. Adv. Funct. Mater. 31, 2010047 (2021).Article CAS Google Scholar Choe, J. H. et al. Boc protection for diamine-appended MOF adsorbents to enhance CO2 recyclability under realistic humid conditions. J. Am. Chem. Soc. 146, 646\u2013659 (2024).Article CAS PubMed Google Scholar Barsoum, M. L. et al. Probing structural transformations and degradation mechanisms by direct observation in SIFSIX-3-Ni for direct air capture. J. Am. Chem. Soc. 146, 6557\u20136565 (2024).Article CAS PubMed Google Scholar Carneiro, J. S. A. et al. Insights into the oxidative degradation mechanism of solid amine sorbents for CO2 capture from air: roles of atmospheric water. Angew. Chem. Int. Ed. 62, e2023028 (2023).Article Google Scholar Yaghi, O. M., Kalmutzki, M. J. & Diercks, C. S. Introduction to Reticular Chemistry: Metal\u2010Organic Frameworks and Covalent Organic Frameworks (Wiley, 2019).Diercks, C. S. & Yaghi, O. M. The atom, the molecule, and the covalent organic framework. Science 355, eaal158 (2017).Article Google Scholar Li, H., Dilipkumar, A., Abubakar, S. & Zhao, D. Covalent organic frameworks for CO2 capture: from laboratory curiosity to industry implementation. Chem. Soc. Rev. 52, 6294\u20136329 (2023).Article CAS PubMed Google Scholar Lyu, H., Li, H., Hanikel, N., Wang, K. & Yaghi, O. M. Covalent organic frameworks for carbon dioxide capture from air. J. Am. Chem. Soc. 144, 12989\u201312995 (2022).Article CAS PubMed Google Scholar Lin, J.-B. et al. A scalable metal-organic framework as a durable physisorbent for carbon dioxide capture. Science 374, 1464\u20131469 (2021).Article ADS CAS PubMed Google Scholar Quang, D. V. et al. Effect of moisture on the heat capacity and the regeneration heat required for CO2 capture process using PEI impregnated mesoporous precipitated silica. Greenhouse Gases Sci. Technol. 5, 91\u2013101 (2015).Article CAS Google Scholar Jin, E. et al. Two-dimensional sp2 carbon\u2013conjugated covalent organic frameworks. Science 357, 673\u2013676 (2017).Article ADS CAS PubMed Google Scholar Lyu, H., Diercks, C. S., Zhu, C. & Yaghi, O. M. Porous crystalline olefin-linked covalent organic frameworks. J. Am. Chem. Soc. 141, 6848\u20136852 (2019).Article CAS PubMed Google Scholar Pawley, G. S. Unit-cell refinement from powder diffraction scans. J. Appl. Crystallogr. 14, 357\u2013361 (1981).Article ADS CAS Google Scholar Brunauer, S., Emmett, P. H. & Teller, E. Adsorption of gases in multimolecular layers. J. Am. Chem. Soc. 60, 309\u2013319 (1938).Article ADS CAS Google Scholar Ji, W. et al. Removal of GenX and perfluorinated alkyl substances from water by amine-functionalized covalent organic frameworks. J. Am. Chem. Soc. 140, 12677\u201312681 (2018).Article CAS PubMed Google Scholar Mao, H. et al. A scalable solid-state nanoporous network with atomic-level interaction design for carbon dioxide capture. Sci. Adv. 8, eabo6849 (2022).Article CAS PubMed PubMed Central Google Scholar McCabe, W. L., Smith, J. C. & Harriott P. Unit Operations of Chemical Engineering 7th edn (McGraw Hill, 2004).Panda, D., Kulkarni, V. & Singh, S. K. Evaluation of amine-based solid adsorbents for direct air capture: a critical review. React. Chem. Eng. 8, 10\u201340 (2023).Article CAS Google Scholar Kolle, J. M., Fayaz, M. & Sayari, A. Understanding the effect of water on CO2 adsorption. Chem. Rev. 121, 7280\u20137345 (2021).Article CAS PubMed Google Scholar Ilkaeva, M. et al. Assessing CO2 capture in porous sorbents via solid-state NMR-assisted adsorption techniques. J. Am. Chem. Soc. 145, 8764\u20138769 (2023).Article CAS PubMed PubMed Central Google Scholar Fung, B. M., Khitrin, A. K. & Ermolaev, K. An improved broadband decoupling sequence for liquid crystals and solids. J. Magn. Reson. 142, 97\u2013101 (2000).Article ADS CAS PubMed Google Scholar Johnson, R. L. & Schmidt-Rohr, K. Quantitative solid-state 13C NMR with signal enhancement by multiple cross polarization. J. Magn. Reson. 239, 44\u201349 (2014).Article ADS CAS PubMed Google Scholar Kresse, G.",
    "comments": [],
    "description": "Capture of CO2 from the air offers a promising approach to addressing climate change and achieving carbon neutrality goals1,2. However, the development of a durable material with high capacity, fast kinetics and low regeneration temperature for CO2 capture, especially from the intricate and dynamic atmosphere, is still lacking. Here a porous, crystalline covalent organic framework (COF) with olefin linkages has been synthesized, structurally characterized and post-synthetically modified by the covalent attachment of amine initiators for producing polyamines within the pores. This COF (termed COF-999) can capture CO2 from open air. COF-999 has a capacity of 0.96\u2009mmol\u2009g\u20131 under dry conditions and 2.05\u2009mmol\u2009g\u20131 under 50% relative humidity, both from 400\u2009ppm CO2. This COF was tested for more than 100 adsorption\u2013desorption cycles in the open air of Berkeley, California, and found to fully retain its performance. COF-999 is an exceptional material for the capture of CO2 from open air as evidenced by its cycling stability, facile uptake of CO2 (reaches half capacity in 18.8\u2009min) and low regeneration temperature (60\u2009\u00b0C). A polyamine-functionalized covalent organic framework, COF-999, can be used as a material for direct air capture of CO2 from open air.",
    "document_uid": "24fd4b247d",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970940",
    "title": "I built my own 3D Game Engine with Open-Source Tools [video]",
    "url": "https://www.youtube.com/watch?v=SV8uBtUHAkQ",
    "score": 1,
    "timestamp": "2024-10-28T14:41:58",
    "source": "Hacker News",
    "content": "I built my own 3D Game Engine with Open-Source Tools [video]",
    "comments": [],
    "description": "Go to https://nordvpn.com/mvg to get a 2-year plan plus 4 additional months with a huge discount. It\u2019s risk-free with Nord\u2019s 30-day money-back guarantee!Fine...",
    "document_uid": "0bc17e12d7",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970929",
    "title": "Trimming down a Rust binary in half",
    "url": "https://tech.dreamleaves.org/trimming-down-a-rust-binary-in-half/",
    "score": 2,
    "timestamp": "2024-10-28T14:40:55",
    "source": "Hacker News",
    "content": "Trimming down a rust binary in half 2024-10-27 Lately, I've stumbled on a blog post about Rust binary sizes. I haven't done much compilation since I last touched C in school, but I was intrigued by the subject and decided to look at how binary size reduction could impact my own Rust project: advent-rs, a simple binary taking year/day/part/filename parameters and solving exercises from the AdventOfCode online contest. Starting point There's already some stuff I know and set up properly, so I already got the correct defaults for my Cargo.toml. [profile.release] opt-level = 3 Let's look at the resulting file through size and ls: $ size target/release/advent-rs __TEXT __DATA __OBJC others dec hex 1409024 16384 0 4295393280 4296818688 1001c4000 $ ls -lah target/release/advent-rs -rwxr-xr-x@ 1 red staff 1.8M Oct 27 08:49 target/release/advent-rs So we are standing at 1.8M, for a binary that's solving around 150 exercices. That doesn't sound so bad, but again, I haven't compiled code in a long time, so I have no idea where I stand. Let's check the first binary I can think of. $ ls -lah /bin/ls -rwxr-xr-x 1 root wheel 151K Sep 5 11:17 /bin/ls Well, I definitely need to do some trimming. Optimizing compilation First things first, let's check compilation. Start with stripping As I remember from my C days, and as the blog clearly states: not stripping symbols from a release is the root of all evil. This is not an absolute! As fredbrancz rightly said, if you release a binary to the public at large and later require debugging it, missing the symbols will come back at you fast! [profile.release] opt-level = 3 strip = true Build, check file. $ /bin/ls -lah target/release/advent-rs -rwxr-xr-x@ 1 red staff 1.5M Oct 27 09:03 target/release/advent-rs Already 300kb trimmed off! Halt and catch fire A backtrace is a very useful to have when debugging, all interpreted languages come with them, and for a while, I wasn't even surprised to see them pop up in Rust, in spite of C never printing one, only going as far as telling me \"Segmentation fault\" when my carefully-crafted binary would have the politeness to tell me on its own that I was doing something wrong. Well, Rust should not have them. Wait, let me explain myself here: backtraces are not a normal feature of a compiled language, since the backtraces you are used to actually come courtesy of the interpreter. So in a compiled language with no interpreter, where do they come from? Yes, from your binary. [profile.release] opt-level = 3 strip = true panic = \"abort\" Build, check file. $ /bin/ls -lah target/release/advent-rs -rwxr-xr-x@ 1 red staff 1.2M Oct 27 09:05 target/release/advent-rs Can you believe this shaved off another 300kb? Insane stuff I don't want The Cargo documentation gives us more options we can experiment with for fun. They don't work for me but your mileage may vary. Optimize for size After 3, there are opt-level values that target smaller binaries. I can try with sor z and they will respectively shave off 100 and 200kb, BUT the tradeoff here is execution speed, which is not a value I want to allow myself to play with. Don't check integer overflow You can disable checking for integer overflow with overflow-checks = false from your binary. Obviously, this is NOT recommended when you're playing with user input, and in my case, it won't even register any change. Wrong! As GamerCounter pointed out, these checks are removed by default in release mode. Link-time optimization It seems that other than compiling, you can also optimise linking with lto = true. I don't recommend it since it doubled my build time AND didn't give me a good size reduction... Attention! I wasn't clear enough on this, but as VorpalWay and hubbamybubba correctly pointed out, the benefits of this parameter depends on your project. Cleaning up crates The other thing that will take up space is...code. More specifically, code you wouldn't need. Let's check my dependencies: [dependencies] clap = { version = \"4.5.1\", features = [\"derive\"] } itertools = \"0.12.1\" md-5 = \"0.10.6\" mutants = \"0.0.3\" [dev-dependencies] assert_cmd = \"2.0.13\" predicates = \"3.1.0\" criterion = { version = \"0.5.1\", features = [\"html_reports\"] } Nothing too barbaric here, clap is the de-facto crate to parse command-line arguments, itertools is required because a ton of exercises require iterating over data in strange ways, I require md-5 for four exercices, and mutants is allowed here only to skip some tests when running mutations. As for the development dependencies, I know I got nothing to fear from them. Removing a crate You may feel like the above dependencies are sane, that I need all of them, that there is no way my project could function without any of them,... But what if it could? Past experience When I started writing all of these parsers I required for handling input data, I often used the regex crate. While it was very useful, I knew from my benchmarks that using old-school \"split-on-this-char\" approach was faster, so I slowly started phasing it out. As of Year 2017: Day 15, I removed the crate from my Cargo.toml. Future ideas While I know that \"rolling out your own crypto\" is a bad idea, the crate md-5can clearly be replaced. Plus the fact that its input and outputs are of different types mean one exercise could actually be way faster if I rolled out my own! Okay, now where's the bloat? I found out the appropriately-named tool cargo-bloat on GitHub. Let's install and try it, it's just a command after all. $ cargo bloat --release Finished `release` profile [optimized] target(s) in 0.01s Analyzing target/release/advent-rs File .text Size Crate Name 1.1% 1.6% 16.0KiB clap_builder clap_builder::parser::parser::Parser::get_matches_with 1.1% 1.6% 15.8KiB [Unknown] __mh_execute_header 0.7% 1.1% 10.5KiB clap_builder clap_builder::builder::command::Command::_build_self 0.6% 0.9% 8.9KiB std std::backtrace_rs::symbolize::gimli::resolve 0.6% 0.9% 8.7KiB std std::backtrace_rs::symbolize::gimli::Context::new 0.5% 0.7% 7.1KiB std gimli::read::dwarf::Unit<R>::new 0.4% 0.7% 6.5KiB clap_builder <clap_builder::error::format::RichFormatter as clap_builder::error::format::ErrorFormatter>::format_error 0.4% 0.6% 6.1KiB clap_builder clap_builder::parser::validator::Validator::validate 0.4% 0.6% 5.8KiB advent_rs core::slice::sort::stable::quicksort::quicksort 0.4% 0.6% 5.6KiB std addr2line::ResUnit<R>::find_function_or_location::{{closure}} 0.4% 0.5% 5.2KiB",
    "comments": [],
    "description": "No description available.",
    "document_uid": "1b8b1ec2c7",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41970919",
    "title": "Making money virtuous (eBook) [pdf]",
    "url": "https://socialtrade.nl/wp-content/uploads/2024/10/Making-Money-Virtuous.pdf",
    "score": 1,
    "timestamp": "2024-10-28T14:40:06",
    "source": "Hacker News",
    "content": "%PDF-1.6 %\u00e2\u00e3\u00cf\u00d3 2101 0 obj <> endobj 2133 0 obj <>/Filter/FlateDecode/ID[<253EC5DAACD6FB4E985BCC57A6DA2F22>]/Index[2101 71]/Info 2100 0 R/Length 136/Prev 4088298/Root 2102 0 R/Size 2172/Type/XRef/W[1 2 1]>>stream h\u015ebbd``b\u00e0\u203a$g \u00a6P \u00c1c $\u02dc? vM \u00c1 $\u00c4@\u00ea\u0152&\u0192\u00c4\u00ce\u01927\u20ac\u00b4\ufffd\ufffd \u00aeH\u00efV \u00c1r\u00c4\ufffd\u00b1V\u201aX\u00ee \u2014y\u02c6$XA) \u00a32@\u00ac*\ufffd\u00c4| \u00c1\u00b5Hp\u0153\u0153_Abm@B\u00d9\u2122\ufffd\u2030\u2018\u00e3\ufffd\u00c5\u00c0\u00c08\u00d4\u02c6\u00ff\u00bf\u00ef U\u00fc\u00db endstream endobj startxref 0 %%EOF 2171 0 obj <>stream h\u015e\u00ecX}PSW\u00bf\u00ef%\ufffd\u00f7B\u20ac|\ufffd \u00ac$!\ufffdq'\u201dO\u00e9#$R\u00c4\u20ac \u00b5\u00db 4h\u00d6:lq%X\u00ed<0\u00c1\u011eR \u0160Z\u0160*~\u011e\u0130\u00ee8;\u20ac:EKWP\u00b4\u00eaj\u2039\u2022\"\ufffd\u00b2\u00b3\u00acZG\u00cb\u00ce\u00ce\u00be\u2014\u00bc\u00c4\u00e8\u00f4\u00cf\u0131s3\ufffd{\u00eey\u00e7\ufffd\ufffd\u00df\u00f9\ufffd &~\u00a6\ufffd \u00cc\u00c0/_",
    "comments": [],
    "description": "No description available.",
    "document_uid": "5afd5bebf9",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41974882",
    "title": "Tesla introduces new standard electrical connector",
    "url": "https://www.tesla.com/en_CA/blog/standardizing-automotive-connectivity",
    "score": 1,
    "timestamp": "2024-10-28T20:09:22",
    "source": "Hacker News",
    "content": "Tesla introduces new standard electrical connector",
    "comments": [],
    "description": "Error fetching meta description: The read operation timed out",
    "document_uid": "671d86bf99",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974880",
    "title": "Rare 1975 Roosevelt dime fetches more than $500k at auction",
    "url": "https://news.sky.com/story/rare-1975-roosevelt-dime-fetches-more-than-500-000-at-auction-13243565",
    "score": 1,
    "timestamp": "2024-10-28T20:09:15",
    "source": "Hacker News",
    "content": "An extremely rare coin, which had been kept in a bank vault since the late 1970s, has sold at auction in the US for more than $500,000 (\u00a3385,000).The coin is a US dime and was produced by the US Mint in San Francisco in 1975. Described on the US Mint's website as \"one of the rarest coins of the 20th century\", it features a profile of former US President Franklin Roosevelt.But it is missing a distinctive 'S' mint mark - one of only two dimes made that year known to exist without it.Three sisters from Ohio, who wanted to remain anonymous, inherited the dime after the death of their brother, who had kept it in a bank vault for more than 40 years. It sold for $506,250 in an online auction that ended on Sunday, Ian Russell, president of California-based GreatCollections, said.The other \"1975 'no S' proof dime\" fetched $456,000 (\u00a3351,000) at auction in 2019 before being sold to a private collector.The two coins were not in general circulation, as they were part of 2.8 million special uncirculated proof sets made by the San Francisco mint in 1975 featuring six coins, which sold at the time for $7 (\u00a35).A few years later, collectors found that two dimes from the set were missing the mint mark. Mr Russell said the sisters told him they inherited one of the two dimes but that their brother and mother bought the first error coin discovered in 1978 for $18,200 (\u00a314,000), which would amount to roughly $90,000 (\u00a369,000) today.Read more from Sky News:Manchester United sack Erik Ten HagLabour MP video 'shocking' says StarmerLondon Zoo's missing parrots foundTheir parents, who ran a dairy farm, saw the coin as a nest egg, they said. The proof sets also included a [Abraham] Lincoln Cent, a [Thomas] Jefferson Nickel, a Bicentennial [George] Washington Quarter, a Bicentennial [John F] Kennedy Half Dollar and a Bicentennial [Dwight] Eisenhower Dollar.The latter three were dated 1976, in preparation, the US Mint said, for America's 200th birthday the following year and were included to boost sales.",
    "comments": [],
    "description": "The dime - described as  \"one of the rarest coins of the 20th century\" - was produced by the US Mint in San Francisco. But it is missing a distinctive mark, one of only two dimes made in 1975 known to exist without it.",
    "document_uid": "4872819a5f",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974878",
    "title": "Georgians join mass rally as president urges protest at 'rigged vote'",
    "url": "https://www.bbc.com/news/articles/cjr424zwv2po",
    "score": 1,
    "timestamp": "2024-10-28T20:09:07",
    "source": "Hacker News",
    "content": "Georgians join mass rally as president urges protest at 'rigged vote'ReutersGeorgia's president addressed the thousands of supporters that gathered in front of the parliament in Tbilisi on MondayTens of thousands of Georgians, many of them draped in EU and Georgian flags gathered outside parliament in Tbilisi on Monday night, in response to a call from the pro-Western president to press for the annulment of Saturday's election. Salome Zourabichivili, who has sided with the opposition, had earlier called for a rally outside parliament, telling the BBC's Steve Rosenberg that this was a \"crucial moment\". She appealed to the international community to stand behind her country's population after a disputed election that she says was \"totally falsified\".The ruling Georgian Dream party and the election commission are adamant the result, giving the government almost 54% of the vote, was free and fair.Pro-EU protesters filled Rustaveli Avenue outside parliament on Monday eveningZourabichvili said Georgia's partners needed to see what was happening, adding that the government's victory was \"not the will of the Georgian people\" who wanted to keep their European future.Georgian Dream Prime Minister Irakli Kobakhidze has told the BBC that alleged violations ahead and during the vote were confined to \"just a couple\" of polling stations.However, the The European Union, Nato and US have all called for a full investigation into a litany of allegations made by monitoring missions of vote fraud before and on the day of Saturday's vote. Thirteen of the EU's 27 foreign ministers said they stood \"at this difficult time at the side of Georgians\", adding \"violations of electoral integrity are incompatible with the standards expected from a candidate to the European Union\".European Commission President Ursula von der Leyen said that Georgians had a \"right to see that electoral irregularities were investigated swiftly, transparently and independently.\" She added that \"Georgians, like all Europeans, must be the masters of their own destiny.\"It was not entirely clear what the Georgian president and four opposition groups hoped to achieve by bringing Georgians on to the main avenue in front of the parliament on Monday.But Salome Zourabichvili made clear the protest would be \"very peaceful\", adding that she did not believe Georgia's authorities wanted confrontation.Protesters in the crowd on Rustaveli Avenue knew what they wanted.\"The main thing we want here is to get what we deserve - legal elections,\" said Lasha, 22. \"No-one had any idea this would happen. At first we were frustrated, then we realised what happened and now we're angry.\"Liza, 20, wanted \"another election that isn\u2019t forged\" and said she was pleased to hear speakers from the four main opposition parties telling Georgians not to give up. Another protester, Keta, told the BBC that she felt \"cheated and frustrated\". \"Me and my friends and my family deserve way better than we have right now... We will fight to the end until we get our justice.\"The president said it was up to the people and the political parties to decide what happened next.\"Maybe we won't be able to achieve it today or tomorrow,\" she said. \"There are a number of things that can be done. There can be an international review of some of the elements of the election, there can be a call for new elections. In what period of time I don't know.\"Georgian president calls for protest against vote 'falsification'The call for protest echoes weeks of demonstrations that brought Tbilisi's central Rustaveli Avenue to a standstill for weeks earlier this year.Back in May there were clashes with riot police, who responded with water cannon, tear gas and force, as Georgians tried to stop the government pushing through a Russian-style \"foreign agents\" law targeting media and civil society groups that have foreign funding. Ultimately the protests failed and the EU froze Georgia's bid to join the 27-country union, accusing it of democratic backsliding.The government has clearly prepared for further protests. Last week it emerged that the interior minister had bought new water cannon vehicles and other equipment for riot police, including lethal weapons, for use \"when it becomes necessary\".Speaking to the BBC on Sunday, Georgia's prime minister said \"the general content of the elections was in line with legal principles and the principle of democratic elections.\"But President Zourabichvili said the scale of election fraud was unprecedented: \"Everything was used that we've ever heard of in this country in a parallel way.\"As an example, she alleged that, before the elections, families who were dependent on state funds had seen their identity cards taken away. At the time it was difficult to tell why, she said, but it then became clear the identity cards were being used for so-called carousel voting in Georgia's new electronic voting system - \"when one person can vote 10, 15, 17 times with the same ID\".She has also described the result of the vote as a \"Russian special operation\", stopping short of accusing the Kremlin of direct intervention. Instead, she accused the government of using a \"very sophisticated\" Russian-inspired propaganda strategy as well as \"PR people\" from Russia.The government has vehemently denied having anything to do with Russia, pointing out it is the only country in the region not to have diplomatic ties with Moscow. Russia fought a five-day war with its southern neighbour in 2008 and still occupies 20% of Georgian territory.The Kremlin has has denied having anything to with the election and has ridiculed Georgia's pro-EU president, whose term in office comes to an end in December. A handful of international leaders have congratulated Georgian Dream for securing a fourth term in office in the contested election, including Hungary's prime minister, Viktor Orban.Orban arrived in Tbilisi on Monday evening on a two-day visit that has annoyed several of his European partners because of the message it sends the Georgian government. In their joint statement, 13 EU foreign ministers said Orban \"does not represent the EU\".Orban and Georgian Dream have much in common. Both place a heavy emphasis on socially conservative family values, and both have styled themselves as fighting an opposition that wants war rather",
    "comments": [],
    "description": "Salome Zourabichivili has called Georgians out on the streets, telling the BBC this is a \"crucial moment\".  ",
    "document_uid": "7268c5ce7f",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974861",
    "title": "Why Do Day Traders Quit After 5 Months",
    "url": "https://lucas-schiavini.com/why-day-traders-quit-after-5-months/",
    "score": 1,
    "timestamp": "2024-10-28T20:07:49",
    "source": "Hacker News",
    "content": "I'm going to present some thoughts I have on Day Trading compared with Value Investing.For those who aren't aware of the terms a simplification would be:Day Traders: Trade during a full day, trying to get market trends that happen in a single day. Make decisions daily.Value Investors: Invest with a long period of time for analysis (months to years), normal to have decisions on a monthly basis.Now that we are all on the same page, here's a journey of a regular Day Trader.The Lifetime of a TraderWhat I'm about to describe is the result of years of financial research compressed into simple words, if you want to take a look at the raw data feel free to check the resources below the post.1st Month. They get a trading app, either Bloomberg's terminal, BlackArrow, or something alike. They play with the safety on (meaning with fake money). They read a lot of news and follow some reputable traders on social media, maybe even buy a course or two.2nd Month. They decide it's time to get real, they turn the safety off and trade with their real money. If they're lucky, they get some nice little profits this first month, or they drop out sooner.3rd Month. They get more excited due to the previous month's net positive and decide to buy more courses and start upgrading their daily bets. Unfortunately, they get a net negative this month, but hey, it's part of the game, isn't it?4th Month. Now they want to recover their losses and start making riskier operations and start losing big time. If they hadn't a safety net before, this is the time the \"game\" gets too costly to be played on a long-term basis.5th Month. They accept their losses and quit. Nothing more, nothing less. It's not a game for everyone.Enters Value InvestingSee the difference between Day trading to Value investing is one big thing: Expectations.Value Investors tend to expect a much lower return on investment (ROI) for the same risk. Here's a simple heuristic to measure risk, if something returns more than 10% a year(or comparable to that year's S&P500) it either is riskier or it has more productivity than the top 500 companies in the US(Very unlikely), or both.This heuristic allowed me to check for scam businesses and pyramid schemes. 1% a month is about 12% a year, riskier but not as risky as some pyramid schemes promising 1% a day(greater than 37000%)!So if you know something is risky, it's better than not knowing it is risky at all, but there is an even better option: Knowing exactly WHY something is risky. That way you can have educated guesses and bet with more conviction on it.So if something returns more than 10% a year it's risky, now you need to know what exactly that business is doing to grow that much. Maybe it has a new technology that allows it to grow from innovation and is capturing more chunks of the market, or maybe it's playing the numbers to get more investors.This is how a value investor thinks, \"what value is this company really bringing to the table that allows it to outperform the biggest companies out there?\".And because you are thinking about these sorts of problems you need to carefully look into a company's numbers and check how they changed from previous years. You also might need to take into account if the company is buying its own shares or just diluting them over the years. You might look into the current board of administration, if they got a good history building companies and if they are the best to handle the current market opportunities.See how it's way too much stuff to take into account that you can't easily do it in a day? Most value investors take about 1 bet a month on a small-sized portfolio (if they don't decide to go with an ETF to have to think even less about it).Who competes in the market against value investors? Well, most value investors buy when they calculate the valuation of the company to be higher than its stock price. That way they are buying the company at a \"discount\". Of course, every value investor will calculate the valuation their own way, so a bit of luck is involved. In the end, a value investor competes with other value investors, in perfect conditions, they would never make any money out of each other, only with the growth of the company. That way, the one that manages to buy a company that does have a lower stock price than its \"inherent value\" tends to win over the long term.Day TradingNow things get a little bit interesting. Day traders are often looking at the graphs, trying to see patterns in the up and down movement of stocks. Most have more than one screen and work from the time the stock exchange opens till it closes.Now everyone has a different way of identifying patterns and trying to one-up the market.Let's see what you can extract from this graph from META's daily stock variation from last week:Now a day trader would take into account different ways to analyze this data, particularly the candle information:Here's a more detailed view of the candles meaning how much variation the price had in that particular time interval.They would put some indicators on the graph, you can take a look at a bunch of indicators available for day traders in the example below:So you see there are some indicators that I put below the candle graph:Now here's where things get interesting, what if the patterns you think you are seeing in the morning while you just had some coffee and became glued to your Bloomberg terminal aren't really a real pattern?Remember this picture below? I bet you thought \"yeah META's stock is plunging a lot in a single day\". Look at it again, but now at the original image I pulled the graph from:Your whole life is a lie, right? Yep day traders have",
    "comments": [],
    "description": "How much are you willing to risk?",
    "document_uid": "a6f07211a1",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974857",
    "title": "Show HN: A progressive web app to meet your favorite people without overplanning",
    "url": "https://joinifyouwant.com/",
    "score": 1,
    "timestamp": "2024-10-28T20:07:09",
    "source": "Hacker News",
    "content": "Join If You Want is a progressive web app that lets you share your plans with your favorite people and see who wants to join. If they\u2019re not interested, they don\u2019t have to do anything but can still see what you\u2019re up to, making it easy to stay connected.",
    "comments": [],
    "description": "Join If You Want (JIYW) is a web app that lets you share your plans with your favorite people and see who wants to join.",
    "document_uid": "a505c8deb2",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974843",
    "title": "Trump's MSG rally draws comparisons to 1939 pro-Nazi rally",
    "url": "https://www.msnbc.com/jonathan-capehart/watch/trump-s-msg-rally-draws-comparisons-to-1939-pro-nazi-rally-222807621632",
    "score": 3,
    "timestamp": "2024-10-28T20:05:30",
    "source": "Hacker News",
    "content": "IE 11 is not supported. For an optimal experience visit our site on another browser.Now PlayingTrump's MSG rally draws comparisons to 1939 pro-Nazi rally09:37UP NEXTMichelle Obama's speech warns what a second Trump term means for women's health05:49Washington Post's non-endorsement decision is 'abominable': Jonathan Capehart02:51Billy Porter unleashes on Donald Trump: 'It's time to be ENRAGED!'04:57Trump is 'literally running as an authoritarian': Ameshia Cross07:10Face it. Everything Trump does 'isn't normal': Jonathan Capehart03:43He is no friend of ours': Rep. Garcia on Trump's relationship with the Latino community09:04Author of new John Lewis biography: 'We think of him as a man of great courage'05:02How Republicans are laying the groundwork to dispute the 2024 election07:31FL & NC Dems on Trump's lies about Hurricane relief: 'Our people are hurting'09:08This election will be won on turnout': Jonathan Capehart05:41The goal here is violence': How the Trump family is inciting supporters against VP Harris06:14Hostage's brother on fight for his return: 'We pray for a ceasefire every day'07:52Our 'duty' is to defeat Donald Trump this November: Jonathan Capehart05:56'We want to go on the offense': Sen. Gary Peters bets on flipping GOP strongholds07:05North Carolina governor describes 'catastrophic' damage from Hurricane Helene07:15John Leguizamo slams Trump's anti-migrant comments: 'Latinos run the economy'05:26New York City mayor 'sounds like Trump,' digs in after federal indictment09:23'Hiding in plain sight': New book investigates Emmett Till's horrific lynching04:28Harris makes Trump look 'small'\u2014and it's swinging voters to her favor08:12Trump's MSG rally draws comparisons to 1939 pro-Nazi rally09:37Donald Trump's extreme rhetoric and rally at Madison Square Garden in New York City has drawn comparisons to when supporters of Hitler packed the Garden in 1939. Ruth Ben-Ghiat and Anne Applebaum join Jonathan Capehart to discuss Trump's rally and how it's being held days after Trump was described as a \"fascist\" by his former chief of staff.Oct. 27, 2024Read MoreNow PlayingTrump's MSG rally draws comparisons to 1939 pro-Nazi rally09:37UP NEXTMichelle Obama's speech warns what a second Trump term means for women's health05:49Washington Post's non-endorsement decision is 'abominable': Jonathan Capehart02:51Billy Porter unleashes on Donald Trump: 'It's time to be ENRAGED!'04:57Trump is 'literally running as an authoritarian': Ameshia Cross07:10Face it. Everything Trump does 'isn't normal': Jonathan Capehart03:43",
    "comments": [
      {
        "author": "caekislove",
        "text": "&quot;Who is making the comparisons?&quot;<p>MSNBC: &quot;We are!&quot;",
        "time": "2024-10-28T20:12:28"
      }
    ],
    "description": "Donald Trump's extreme rhetoric and rally at Madison Square Garden in New York City has drawn comparisons to when supporters of Hitler packed the Garden in 1939. Ruth Ben-Ghiat and Anne Applebaum join Jonathan Capehart to discuss Trump's rally and how it's being held days after Trump was described as a \"fascist\" by his former chief of staff.",
    "document_uid": "6988ef5366",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974841",
    "title": "US labor board wrongly ordered Tesla's Musk to delete anti-union tweet : court",
    "url": "https://www.msn.com/en-us/money/news/us-labor-board-wrongly-ordered-tesla-s-musk-to-delete-anti-union-tweet-court-rules/ar-AA1t38Q2",
    "score": 1,
    "timestamp": "2024-10-28T20:05:14",
    "source": "Hacker News",
    "content": "US labor board wrongly ordered Tesla's Musk to delete anti-union tweet : court",
    "comments": [],
    "description": "No description available.",
    "document_uid": "b8cbafc697",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974817",
    "title": "Apple Intelligence is here, but it still has a lot to learn",
    "url": "https://www.theverge.com/2024/10/28/24279804/apple-intelligence-ios-18-1-siri-ai",
    "score": 1,
    "timestamp": "2024-10-28T20:02:36",
    "source": "Hacker News",
    "content": "Apple Intelligence has finally arrived, and like most AI on smartphones so far, it\u2019s mostly underwhelming.The debut features of Apple Intelligence are all very familiar: there are glowing gradients and sparkle icons that indicate the presence of AI; writing tools that make your emails sound more professional; and an AI eraser in Photos that blots away distractions. It\u2019s all here, and it all works okay. But none of it is even close to the time-saving computing platform shift we\u2019ve been promised.Essentially, there are two Apple Intelligences: the one that\u2019s here now and the one we might see in the future. Even in today\u2019s launch announcement, Apple is busy teasing the features that haven\u2019t launched yet. What\u2019s here today is a handful of tools that loosely share a common theme: helping you weed out distractions and find the signal in the noise. That\u2019s the theory, anyway.Apple uses AI to summarize groups of notifications so you can catch up on what you missed faster. You can summarize long emails and use a new focus mode that filters out unnecessary distractions. In practice, these things kind of work, though, after a week of using them, I don\u2019t feel like I saved much time or energy.In the Mail app, AI summaries appear where the first line of an email would normally show up when you\u2019re viewing an entire inbox; there\u2019s also an option to summarize individual emails. Maybe it\u2019s a reflection of how useless email has become, but I didn\u2019t find either of these features terribly helpful. You know what feature we already use that summarizes an email pretty well? The subject line. At least that\u2019s true of most emails I get; they\u2019re usually short and to the point. Maybe Tim Cook saves himself a lot of time reading long emails, but personally, I could live without a little summary of every email the DNC sends me asking for three dollars by midnight.Notification summaries seem a little more promising to me \u2014 at the very least, it\u2019s pretty funny seeing AI try to summarize a string of gossipy texts or a bunch of notifications from your doorbell. But it also surfaced a bit of important information in a string of texts from a friend, and had I not seen that summary when glancing at my phone, I might have read the messages much later. That was helpful.Over in Photos, you\u2019ll find the new Clean Up tool in your editing options. It\u2019s designed to quickly remove objects from a scene; you can tap something that\u2019s been automatically highlighted by the tool or outline something yourself that you want removed. It runs on-device, so you only have to wait a few moments, and you\u2019ll see the selected object (mostly) disappear. I erased a table behind this kiddo with Google\u2019s older Magic Eraser tool in Google Photos.Apple\u2019s Clean Up does a better job of removing the table, but it\u2019s not exactly miles ahead of Google\u2019s tool.The tool does a good-enough job, especially for smaller objects in the background. But it\u2019s only about as good as Google\u2019s older Magic Eraser tool in Google Photos \u2014 occasionally, it\u2019s better, but it\u2019s not as good as Google\u2019s Magic Editor, which uses generative AI for incredibly convincing object removal. That tool runs in the cloud, so it\u2019s a little apples to oranges, but still. I can use Google Photos\u2019 on-device Magic Eraser tool on my four-year-old iPhone 12 Mini, and the results are pretty close to what I get with Clean Up running on the iPhone 16 \u2014 not a great argument for the AI phone upgrade cycle. There\u2019s also, of course, an upgraded Siri. Sure, it looks different, and typing queries is a handy addition, but you don\u2019t have to use it for long to realize it\u2019s basically the same old Siri with a new coat of paint. It handles natural language better and includes more product knowledge to help you find settings on your iPhone, but that\u2019s about it right now. Apple has promised big updates for Siri down the road, and features like a ChatGPT extension are scheduled to arrive by the end of the year. But the big stuff \u2014 contextual awareness, the ability to take action in apps \u2014 is all planned for 2025.Other features \u2014 like AI-generated photo memories and smart replies \u2014 do what they\u2019re supposed to do but lack a certain human touch. I didn\u2019t send any of the AI-suggested replies in my messages even though they conveyed the right sentiments. If I\u2019m going to take the time to respond to a text, I might as well just write \u201cThat\u2019s tough\u201d myself rather than have AI do it, you know? Isn\u2019t that part of the point of texting someone? I also prompted Photos to create a memory of moments of my kid, which it did but titled it the eerily impersonal \u201cJoyous Moments with Child.\u201dApple\u2019s playing a little catch-up.To be clear, criticism of Apple Intelligence is not an endorsement of other phones\u2019 intelligences; they\u2019re all varying degrees of unhelpful right now. If you want to make it look like a helicopter crashed in an empty meadow, then sure, there\u2019s AI for that. But if you want help getting things done? That\u2019s not quite ready. And in fairness, this is v1, and Apple has been pretty clear that its more impressive Intelligence features will drip out over the next year. But Apple also put a big, bright \u201cBuilt for Apple Intelligence\u201d bow on every new iPhone, iPad, and Mac it\u2019s selling right now, suggesting we\u2019d be sorry if we bought an Apple device that couldn\u2019t handle AI. If Apple Intelligence is a letdown right now, it\u2019s because Apple built it up to impossible heights.There\u2019s more to come, and some of it looks really promising. This first wave of AI features is Apple playing catch-up to Google and Samsung. But no phone maker has yet created a cohesive set of time-saving AI tools. Apple might be arriving late, but the game is just getting started.",
    "comments": [
      {
        "author": "ChrisArchitect",
        "text": "Discussion: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41971835\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41971835</a>",
        "time": "2024-10-28T20:12:34"
      }
    ],
    "description": "Apple\u2019s first wave of AI features has arrived with iOS 18.1. But the updates that might make Apple Intelligence truly useful are still months away.",
    "document_uid": "8926621731",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974816",
    "title": "Show HN: I made a site to track 2024 U.S. election PAC winners",
    "url": "https://www.1million.space/pacwinner",
    "score": 2,
    "timestamp": "2024-10-28T20:02:28",
    "source": "Hacker News",
    "content": "2024 America PAC Million Dollar WinnersDiscover the fortunate individuals who won $1 million through America PAC in 2024. Real stories of life-changing wins.",
    "comments": [],
    "description": "Discover the fortunate individuals who won $1 million through America PAC in 2024. Real stories of life-changing wins.",
    "document_uid": "34c6575884",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974802",
    "title": "Why logits are not log-odds in neural nets",
    "url": "https://medium.com/@mbonsign/neural-network-theory-understanding-statistical-mechanisms-feature-representation-and-holistic-81e801ed509d",
    "score": 1,
    "timestamp": "2024-10-28T20:01:07",
    "source": "Hacker News",
    "content": "Neural Network Theory: Understanding Statistical Mechanisms, Feature Representation and Holistic TrainingAbstractNeural networks operate through statistical mechanisms yet encode a wide range of knowledge and capabilities. This paper explores the relationship between their statistical foundations and the complex representations they develop. Using analogies from art and examining the holistic nature of their training process, we demonstrate why understanding neural networks purely in terms of their statistical mechanisms is insufficient. We explore their computational completeness, feature representation capabilities, and the fundamental opacity of their learned structures, suggesting new frameworks for interpreting these systems.IntroductionNeural networks have transformed fields from computer vision to natural language processing, yet their underlying nature remains poorly understood. While we can describe their mechanisms with mathematical precision, this description fails to capture the essence of what these models become through training. This understanding has profound implications not only for machine learning, but for our philosophical understanding of computation, meaning, and understanding itself. This paper aims to bridge the gap between understanding neural networks\u2019 statistical foundations and comprehending their emergent capabilities.The Medium and the MessageJust as Michelangelo\u2019s Piet\u00e0 is made of marble, neural networks operate through statistical mechanisms \u2014 weights and weighted sums processing information in a Bayesian manner (Bishop, 2006). However, saying the Piet\u00e0 is made of marble tells us almost nothing about its nature as a work of art \u2014 its emotional resonance, its representation of human suffering, or its artistic achievement. Similarly, understanding that neural networks operate through statistical mechanisms tells us little about what knowledge, capabilities, or representations they have actually developed.This distinction is crucial: the statistical operations are simply the material from which the model is constructed, but they do not determine or constrain what features or knowledge the network can encode, just as marble does not constrain what scenes or emotions a sculpture can convey.The Chinese Room RevisitedOur understanding of neural networks\u2019 relationship between mechanism and meaning provides a compelling counterargument to Searle\u2019s famous Chinese Room thought experiment (Searle, 1980). In this thought experiment, a person who doesn\u2019t understand Chinese follows rules to manipulate Chinese symbols, producing apparently meaningful responses. Searle argues that since the person doesn\u2019t understand Chinese, mere symbol manipulation (or by extension, computation) cannot constitute genuine understanding.However, this argument makes the same category error as claiming the Piet\u00e0 is \u201cjust marble.\u201d Just as the artistic meaning of the Piet\u00e0 cannot be reduced to its marble substrate, the understanding or meaning in a neural network cannot be reduced to its statistical operations. The Chinese Room argument focuses on the mechanism (symbol manipulation) while missing the potential for meaning and understanding to emerge from the holistic organization of the system.This insight connects deeply to our understanding of neural networks:The statistical operations are like the rule-following in the Chinese RoomBut the actual knowledge/understanding exists in the emergent features and representationsThe substrate (statistical operations/symbol manipulation) doesn\u2019t determine the nature of what\u2019s being encodedComputational CompletenessWe can establish that statistical mechanisms must be computationally complete through the following logic:- Neural networks are known to be computationally complete, capable of computing any computable function (Cybenko, 1989)- Neural networks are built entirely from statistical operations- Therefore, statistical operations must be capable of computing any computable functionThis explains why neural networks can implement non-statistical operations. For example, a network can learn to:- Perform exact arithmetic (2 + 2 = 4, always)- Implement logical operations (AND, OR, NOT gates)- Execute deterministic algorithms (sorting, pathfinding)- Store and retrieve precise facts (\u201cParis is the capital of France\u201d)Feature RepresentationWhile the mechanism is statistical, the weights themselves represent features learned through training (Krizhevsky et al., 2012). These features are fundamentally opaque to external analysis and can represent various types of knowledge or capabilities:1. Direct Memories Example: A transformer model might encode the exact text of famous quotations2. Logical Operations Example: Learning to implement if-then relationships or comparison operations3. Pattern Recognition Example: Detecting syntactic structures in language or visual patterns in images4. Abstract Concepts Example: Understanding analogies or categorical relationships5. Statistical Relationships Example: Capturing probability distributions of word co-occurrencesThis emergent representation of features challenges the Chinese Room argument in another way: while the Room\u2019s operator follows discrete, procedural rules, neural networks develop integrated, holistic representations that transcend their mechanical operations. Understanding might emerge from the system\u2019s overall organization rather than from any individual operation.The Training ProcessTraining through backpropagation is a holistic process (Rumelhart et al., 1986), analogous to constantly reshaping a sculpture where each change affects the entire work:1. Each backpropagation step modifies weights to improve performance on specific cases2. These modifications affect how the model handles other cases3. Subsequent steps must then adjust these changes4. This creates a constant morphing of the entire modelFor example, when a language model learns to complete the sentence \u201cThe capital of France is ___\u201d, the weight updates might:- Strengthen connections for \u201cParis\u201d- Affect how it handles other capital cities- Impact its processing of French-related information- Modify its general understanding of geographic factsThe model seeks what we might call \u201csufficient perfection\u201d \u2014 a configuration that satisfies all constraints collectively, rather than optimizing for any single case.Emergence of StructureThe final model emerges through this holistic training process, converging toward a form that satisfies the requirements of the entire corpus (Bengio et al., 2013). Unlike a building constructed piece by piece according to a blueprint, the model\u2019s structure emerges from the simultaneous pressure of all training examples.This holistic emergence of structure provides another perspective on the Chinese Room debate. The Room\u2019s operator follows fixed rules that never evolve or integrate, while neural networks develop complex, interconnected representations through training. The difference is analogous to that between following a phrasebook and developing genuine linguistic competence.Understanding Activation PathwaysWhile activation pathways operate probabilistically, their meaning transcends pure probability (Goodfellow et al., 2016). Consider a neural network that has learned to recognize cats:- The activation mechanism is statistical (weighted sums and probabilities)- But the feature being activated might be a precise visual concept (\u201cpointy ears\u201d)- Or a logical rule (\u201cif it has whiskers AND pointed ears THEN likely a cat\u201d)- Or a complex combination of various",
    "comments": [],
    "description": "Neural networks operate through statistical mechanisms yet encode a wide range of knowledge and capabilities. This paper explores the relationship between their statistical foundations and the\u2026",
    "document_uid": "d5f95a3c85",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974791",
    "title": "How to run AI models locally on your PC (LM Studio)",
    "url": "https://bensbites.com/tutorial/how-to-run-ai-models-locally-on-your-pc",
    "score": 1,
    "timestamp": "2024-10-28T20:00:18",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "1259195d10",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974790",
    "title": "Without Knowledge or Consent",
    "url": "https://www.propublica.org/article/gunmakers-owners-sensitive-personal-information-glock-remington-nssf",
    "score": 1,
    "timestamp": "2024-10-28T20:00:08",
    "source": "Hacker News",
    "content": "For years, America\u2019s most iconic gun-makers turned over sensitive personal information on hundreds of thousands of customers to political operatives. Those operatives, in turn, secretly employed the details to rally firearm owners to elect pro-gun politicians running for Congress and the White House, a ProPublica investigation has found. The clandestine sharing of gun buyers\u2019 identities \u2014 without their knowledge and consent \u2014 marked a significant departure for an industry that has long prided itself on thwarting efforts to track who owns firearms in America. At least 10 gun industry businesses, including Glock, Smith & Wesson, Remington, Marlin and Mossberg, handed over names, addresses and other private data to the gun industry\u2019s chief lobbying group, the National Shooting Sports Foundation. The NSSF then entered the gun owners\u2019 details into what would become a massive database. The data initially came from decades of warranty cards filled out by customers and returned to gun manufacturers for rebates and repair or replacement programs. A ProPublica review of dozens of warranty cards from the 1970s through today found that some promised customers their information would be kept strictly confidential. Others said some information could be shared with third parties for marketing and sales. None of the cards informed buyers their details would be used by lobbyists and consultants to win elections. Warranty card from Remington with common usage disclosure language Selected text Thanks for taking the time to fill out this questionnaire. Your answers will be used for market research studies and reports \u2014 and will help us better serve you in the future. They will also allow you to receive important mailings and special offers from a number of fine companies whose products and services relate directly to the specific interests, hobbies, and other information indicated above. Credit: Obtained by ProPublica The gun industry launched the project approximately 17 months before the 2000 election as it grappled with a cascade of financial, legal and political threats. Within three years, the NSSF\u2019s database \u2014 filled with warranty card information and supplemented with names from voter rolls and hunting licenses \u2014 contained at least 5.5 million people. Jon Leibowitz, who was appointed to the Federal Trade Commission by President George W. Bush in 2004 and served as chair under President Barack Obama, reviewed several company privacy policies and warranty cards at ProPublica\u2019s request. The commission has enforced privacy protections since the 1970s. Leibowitz said firearms companies that handed over customer information may have breached federal and state prohibitions against unfair and deceptive business behavior and could face civil sanctions. \u201cThis is super troubling,\u201d said Leibowitz, who left the commission in 2013. \u201cYou shouldn\u2019t take people\u2019s data without them knowing what you\u2019re doing with it \u2014 and give it or sell it to others. It is the customer\u2019s information, not the company\u2019s.\u201d The undisclosed collection of intimate gun owner information is in sharp contrast with the NSSF\u2019s public image. Founded in 1961 and currently based in Shelton, Connecticut, the trade organization represents thousands of firearms and ammunition manufacturers, distributors, retailers, publishers and shooting ranges. It is funded by membership dues, donations, sponsored events and government grants. While not as well known as the chief lobbyist for gun owners, the National Rifle Association, the NSSF is respected and influential in business, political and gun-rights communities. For two decades, the group positioned itself as an unwavering watchdog of gun owner privacy. The organization has raged against government and corporate attempts to amass information on gun buyers. As recently as this year, the NSSF pushed for laws that would prohibit credit card companies from creating special codes for firearms dealers, claiming the codes could be used to create a registry of gun purchasers. As a group, gun owners are fiercely protective about their personal information. Many have good reasons. Their ranks include police officers, judges, domestic violence victims and others who have faced serious threats of harm. In a statement, the NSSF defended its data collection. Any suggestion of \u201cunethical or illegal behavior is entirely unfounded,\u201d the statement said, adding that \u201cthese activities are, and always have been, entirely legal and within the terms and conditions of any individual manufacturer, company, data broker, or other entity.\u201d The gun industry companies either did not respond to ProPublica or declined to comment, noting they are under different ownership today and could not find evidence that customer information was previously shared. One ammunition maker named in the NSSF documents as a source of data said it never gave the trade group or its vendors any \u201cpersonal information.\u201d ProPublica established the existence of the secret program after reviewing tens of thousands of internal corporate and NSSF emails, reports, invoices and contracts. We also interviewed scores of former gun executives, NSSF employees, NRA lobbyists and political consultants in the U.S. and the United Kingdom. \u201cThis is super troubling. You shouldn\u2019t take people\u2019s data without them knowing what you\u2019re doing with it \u2014 and give it or sell it to others. It is the customer\u2019s information, not the company\u2019s.\u201d Jon Leibowitz former Federal Trade Commission chair, when presented with information about the NSSF database The insider accounts and trove of records lay bare a multidecade effort to mobilize gun owners as a political force. Confidential information from gun customers was central to what NSSF called its voter education program. The initiative involved sending letters, postcards and later emails to persuade people to vote for the firearms industry\u2019s preferred political candidates. Because privacy laws shield the names of firearm purchasers from public view, the data NSSF obtained gave it a unique ability to identify and contact large numbers of gun owners or shooting sports enthusiasts. It also allowed the NSSF to figure out whether a gun buyer was a registered voter. Those who weren\u2019t would be encouraged to register and cast their ballots for industry-supported politicians. From 2000 to 2016, the organization poured more than $20 million into its voter education campaign, which was initially called Vote Your Sport and today is known as GunVote. The NSSF",
    "comments": [],
    "description": "At least 10 gun industry businesses, including Glock, Smith & Wesson, Remington and Mossberg, secretly handed over names, addresses and other data to lobbyists, who used the details to rally firearm owners to elect pro-gun politicians.",
    "document_uid": "bd90cbb552",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974789",
    "title": "AI Slop Is Flooding Medium",
    "url": "https://www.wired.com/story/ai-generated-medium-posts-content-moderation/",
    "score": 8,
    "timestamp": "2024-10-28T20:00:06",
    "source": "Hacker News",
    "content": "Some Medium writers and editors do applaud the platform\u2019s approach to AI. Eric Pierce, who founded Medium\u2019s largest pop culture publication Fanfare, says he doesn\u2019t have to fend off many AI-generated submissions and that he believes that the human curators of Medium\u2019s boost program help highlight the best of the platform\u2019s human writing. \u201cI can\u2019t think of a single piece I\u2019ve read on Medium in the past few months that even hinted at being AI-created,\u201d he says. \u201cIncreasingly, Medium feels like a bastion of sanity amid an internet desperate to eat itself alive.\u201dHowever, other writers and editors believe they currently still see a plethora of AI-generated writing on the platform. Content marketing writer Marcus Musick, who edits several publications, wrote a post lamenting how what he suspects to be an AI-generated article went viral. (Reality Defender ran an analysis on the article in question and estimated it was 99 percent \u201clikely manipulated.\u201d) The story appears widely read, with over 13,500 \u201cclaps.\u201dIn addition to spotting possible AI content as a reader, Musick also believes he encounters it frequently as an editor. He says he rejects around 80 percent of potential contributors a month because he suspects they\u2019re using AI. He does not use AI detectors, which he calls \u201cuseless,\u201d instead relying on his own judgment.While the volume of likely AI-generated content on Medium is notable, the moderation challenges the platform faces\u2014how to surface good work and keep junk banished\u2014is one that has always plagued the greater web. The AI boom has simply super-charged the problem. While click farms have long been an issue, for example, AI has handed SEO-obsessed entrepreneurs a way to swiftly resurrect zombie media outlets by filling them with AI slop. There\u2019s a whole subgenre of YouTube hustle culture entrepreneurs creating get-rich-quick tutorials encouraging others to create AI slop on platforms like Facebook, Amazon Kindle, and, yes, Medium. (Sample headline: \u201c1-Click AI SEO Medium Empire \ud83e\udd2f.\u201d)\u201cMedium is in the same place as the internet as a whole right now. Because AI content is so quick to generate that it is everywhere,\u201d says plagiarism consultant Jonathan Bailey. \u201cSpam filters, the human moderators, et cetera\u2014those are probably the best tools they have.\u201dStubblebine\u2019s argument\u2014that it doesn\u2019t necessarily matter whether a platform contains a large amount of garbage, as long as it successfully amplifies good writing and limits the reach of said garbage\u2014is perhaps more pragmatic than any attempt to wholly banish AI slop. His moderation strategy may very well be the most savvy approach.It also suggests a future in which the Dead Internet theory comes to fruition. The theory, once the domain of extremely online conspiratorial thinkers, argues that the vast majority of the internet is devoid of real people and human-created posts, instead clogged with AI-generated slop and bots. As generative AI tools grow more commonplace, platforms that give up on trying to blot out bots will incubate an online world in which work created by humans becomes increasingly harder to find on platforms swamped by AI.",
    "comments": [
      {
        "author": "altdataseller",
        "text": "Replace \u201cMedium\u201d with internet. Otherwise this is just a paid promo piece for Medium",
        "time": "2024-10-28T20:12:31"
      },
      {
        "author": "tartoran",
        "text": "AI Slop is everywhere and soon to overtake man made content.",
        "time": "2024-10-28T20:11:04"
      }
    ],
    "description": "The blogging platform Medium is facing an influx of AI-generated content. CEO Tony Stubblebine says it \u201cdoesn\u2019t matter\u201d as long as nobody reads it.",
    "document_uid": "41043ddf82",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974770",
    "title": "Graphene oxide, chitosan sponge 10 times efficient at removing gold from e-waste",
    "url": "https://phys.org/news/2024-10-graphene-oxide-chitosan-sponge-ten.html",
    "score": 1,
    "timestamp": "2024-10-28T19:58:29",
    "source": "Hacker News",
    "content": "400 Bad Request Your request has been blocked by our server's security policies. If you believe this is an error, please contact our support team.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "3e2659c272",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974769",
    "title": "Mathematical Theory of Deep Learning",
    "url": "https://arxiv.org/abs/2407.18384",
    "score": 2,
    "timestamp": "2024-10-28T19:58:27",
    "source": "Hacker News",
    "content": "Happy Open Access Week from arXiv! Open access is only possible with YOUR support. Give to arXiv this week to help keep science open for all.",
    "comments": [],
    "description": "Abstract page for arXiv paper 2407.18384: Mathematical theory of deep learning",
    "document_uid": "7bdcb29adb",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974747",
    "title": "Torment Nexus",
    "url": "https://neuralink.com/",
    "score": 1,
    "timestamp": "2024-10-28T19:57:01",
    "source": "Hacker News",
    "content": "Redefining the boundaries of human capabilities requires pioneers.Neuralink is currently seeking people with quadriplegia to participate in a groundbreaking investigational medical device clinical trial for our brain-computer interface.If you have quadriplegia and want to explore new ways of controlling your computer, please consider joining our Patient Registry.Our MissionCreate a generalized brain interface to restore autonomy to those with unmet medical needs today and unlock human potential tomorrow.Potential ImpactWho It's ForOur ApproachWhat's NextBrain-computer interfaces have the potential to change lives for the better. We want to bring this technology from the lab into peoples' homes. ImplantOur brain-computer interface is fully implantable, cosmetically invisible, and designed to let you control a computer or mobile device anywhere you go.Implant Biocompatible EnclosureThe N1 Implant is hermetically sealed in a biocompatible enclosure that withstands physiological conditions several times harsher than those in the human body.Implant BatteryThe N1 Implant is powered by a small battery charged wirelessly from the outside via a compact, inductive charger that enables easy use from anywhere.Implant Chips and ElectronicsAdvanced, custom, low-power chips and electronics process neural signals, transmitting them wirelessly to the Neuralink Application, which decodes the data stream into actions and intents.Implant ThreadsThe N1 Implant records neural activity through 1024 electrodes distributed across 64 threads. These highly-flexible, ultra-thin threads are key to minimize damage during implantation and beyond. Surgical RobotThe threads of our implant are so fine that they can't be inserted by the human hand. Our surgical robot has been designed to reliably and efficiently insert these threads exactly where they need to be.Surgical Robot Base StructureThe base structure and motion stage provide the structural platform for the robot head and the primary 3 axis linear motion used to position the robot head and needle.Surgical Robot Robot HeadThe robot head contains the optics and sensors of 5 camera systems and the optics for an optical coherence tomography (OCT) system.Surgical Robot NeedleThe needle, which is thinner than a human hair, grasps, inserts, and releases threads.A Seamless BCI ExperienceTo restore independence and improve lives, we\u2019ve built a brain-computer interface (BCI) experience that enables fast and reliable computer control and prioritizes ease of use.Patient RegistryIf you're interested in learning whether you may qualify for current and/or future Neuralink clinical trials, consider joining our Patient Registry.Join Our Patient RegistryCareersDeveloping brain-computer interfaces is an interdisciplinary challenge. We are looking to hire a wide range of people with diverse engineering, scientific, and operations expertise.Explore All Open Roles",
    "comments": [],
    "description": "Creating a generalized brain interface to restore autonomy to those with unmet medical needs today and unlock human potential tomorrow.",
    "document_uid": "9e8b126fb1",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974732",
    "title": "Natural compound found in flowers blocks of enzyme involved in MS and cancer",
    "url": "https://phys.org/news/2024-10-natural-compound-blocks-enzyme-involved.html",
    "score": 1,
    "timestamp": "2024-10-28T19:55:39",
    "source": "Hacker News",
    "content": "400 Bad Request Your request has been blocked by our server's security policies. If you believe this is an error, please contact our support team.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "6741e196bd",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974718",
    "title": "Millions may rely on groundwater contaminated with PFAS for drinking water",
    "url": "https://phys.org/news/2024-10-millions-groundwater-contaminated-pfas.html",
    "score": 2,
    "timestamp": "2024-10-28T19:54:22",
    "source": "Hacker News",
    "content": "400 Bad Request Your request has been blocked by our server's security policies. If you believe this is an error, please contact our support team.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "6a82a19bf5",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974701",
    "title": "Hard times for Tinder and Bumble: Why investors are cashing in dating app stock",
    "url": "https://english.elpais.com/economy-and-business/2024-10-12/hard-times-for-tinder-and-bumble-why-investors-are-cashing-in-their-dating-app-stock.html",
    "score": 2,
    "timestamp": "2024-10-28T19:52:44",
    "source": "Hacker News",
    "content": "The major dating apps have had their hearts broken; their honeymoon with investors appearing to have come to an end. Two examples illustrate this rupture. The first is Bumble, which promised to give women more power in digital liaisons, stipulating that they be the ones to begin conversations on the app. The U.S. app\u2019s breakout moment culminated when it began trading on the Nasdaq in February 2021. The company\u2019s initial public offering (IPO) made a strong debut, with shares surging 85% on the first day, jumping from $43 to $78.89. However, the stock has since plummeted, now trading at just $6.33 after a 57% decline this year. The second is Match Group, where a similar situation has unfolded. Since its IPO in 2015, Match Group \u2014 the dating industry leader that owns platforms like Tinder and Match.com \u2014 saw its peak stock price in December 2021, but its shares have since dropped by 79%Why are investors cashing in their stocks in these companies? Various reports blame the platforms themselves for having raised in-app prices to increase earnings. But in reality, things are more complicated. Tinder has a range of rates, from $3.99 to $499.\u201cThat\u2019s not a huge amount of money, so perhaps the apps\u2019 struggles stem from another issue. \u201cI\u2019m not paying one cent to flirt,\u201d says 26-year-old Paula Garc\u00eda, who\u2019s had a Tinder account since she was 18 and has used it to \u201csnag\u201d a couple of stable, two-year relationships. \u201cIt\u2019s like a catalog. You\u2019re selling yourself through a menu; it\u2019s like going to a butcher shop \u2014 until you try the product, you don\u2019t know if it\u2019s worth the trouble.\u201dGarc\u00eda prefers Bumble, mainly because it gives women more control and even provides icebreaker questions like \u201cWhat is your life\u2019s dream?\u201d However, she finds it frustrating that users have to pay for increased visibility. \u201cIt\u2019s absurd. If you\u2019re unattractive, it won\u2019t help \u2014 you\u2019re not going to get anyone. Find another strategy, like being funny,\u201d she says. The sheer volume of choices can also be overwhelming. In just the past week, she\u2019s received 999 matches, all generated by the algorithm. \u201cIt can be suffocating,\u201d she admits. A similar pattern occurs on Tinder, where 84% of users are men.\u201cGen Z is hesitant to pay for digital dating, largely because they\u2019re young and still frequent places [like bars or schools], where meeting people in person is easy,\u201d says Jess Carbino, a sociologist who studied at UCLA. As they grow older, Carbino suggests they\u2019ll be more inclined to invest in online dating. However, the bigger challenge might be the sheer diversity within this generation, which requires careful market segmentation to connect with them.Garc\u00eda, for instance, foresees a future of \u201cdisillusionment\u201d for her peers. In her view, turning 30 marks the start of a decade of loneliness, with fewer singles to meet. \u201cYoung people don\u2019t want to commit,\u201d she observes. \u201cChildren? That\u2019s still taboo.\u201dDiffering expectations in the dating world have led to a surge of niche apps, each catering to specific needs. For example, those seeking \u201copen relationships\u201d often gravitate toward platforms like Feeld, while single parents turn to Stir, and people of color to BLK or Chispa. All of these platforms are owned by Match Group. According to a report from U.S. bank Jefferies, Match Group\u2019s economic goal is to balance the demographic ecosystems \u2014 including men, women, and various age groups \u2014 to create platforms that offer suitable matches for everyone.Grindr, a platform aimed at the gay community, has thrived by connecting deeply with its market niche. Between January and June this year, Grindr\u2019s revenues surged by 34%, reaching $117 million, while its gross operating profit increased by 40% to $68.5 million, driven by its more than 14 million active users.A competitive spaceMeanwhile, Bumble is struggling to hold its ground in an increasingly competitive market, despite a 17.67% rise in paid users over the past year. The company also recently cut 30% of its workforce. Analysts at Jefferies have become increasingly skeptical, slashing earnings estimates and expressing concern about Bumble\u2019s ability to reignite growth. Bank of America experts highlighted the company\u2019s sluggish revenue growth on August 8, noting only a 3% year-on-year increase for the April-June quarter. They forecast a modest revenue for the year at $1 billion, just a 1% rise. Similarly, Match Group\u2019s performance isn\u2019t much stronger. For the same quarter, its revenue grew by only 4%, reaching $3.5 billion.Despite the challenges, Bumble\u2019s CEO, Lidiane Jones, expressed optimism in an email to EL PA\u00cdS: \u201cWe\u2019ve introduced a bold, customer-focused strategy that will help us better align with our ultimate mission of becoming the top global choice for those seeking love, friendship, and community.\u201dHowever, the real test lies in execution. Match Group\u2019s Hinge, for example, offers users a list of recommended profiles but charges $3.99 for a \u201crose\u201d to signal strong interest before starting a conversation. This strategy has proven effective, with Hinge now boasting 1.5 million paying users, a 55% increase from two years ago.The winner of the digital dating race will be the app that strikes the right balance between price and the \u201cquality\u201d of potential matches, while also avoiding sharp declines in user engagement. In the second quarter of this year, Tinder saw an increase in paying users, reaching 250,000, following a loss of 78,000 in the first quarter.\u201cIt has changed its negative trajectory and halted the decline of users,\u201d says \u00c1lvaro Romero, an analyst at Singular Bank. \u201cHowever, it still has a long way to go to achieve sustainable growth. Innovation is essential, and it needs to avoid over-monetizing its network.\u201d While Tinder\u2019s paying users decreased in the last quarter, Hinge\u2019s user base grew, indicating that these apps are increasingly cannibalizing each other\u2019s audiences.Yet, Tinder retains its unique position in the market. \u201cPeople have odd concerns about dating apps. It\u2019s like sitting at a bar \u2014 it takes time,\u201d says a longtime user. \u201cI deleted the app because it had past its prime and became boring.\u201dIn 2022, Tinder boasted 75 million monthly active users, but efforts",
    "comments": [],
    "description": "Saturation, a lack of innovation, rising fees, and the resurgence of real-world interactions pose significant threats to the business of hook-up apps",
    "document_uid": "4cd5ad2be1",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41974694",
    "title": "NASA chief calls for investigation into report that Musk, Putin spoke regularly",
    "url": "https://www.cnn.com/2024/10/25/politics/elon-musk-vladimir-putin/index.html",
    "score": 4,
    "timestamp": "2024-10-28T19:52:13",
    "source": "Hacker News",
    "content": "CNN \u2014 NASA Administrator Bill Nelson on Friday called for an investigation into a Wall Street Journal report that SpaceX founder and Donald Trump ally Elon Musk and Russian President Vladimir Putin have been in \u201cregular contact\u201d since late 2022. The report, which said the SpaceX founder has discussed \u201cpersonal topics, business and geopolitical tensions\u201d with the Russian leader, raises national security concerns as SpaceX\u2019s relationships with NASA and the US military may have granted Musk access to sensitive government information and US intelligence. \u201cI don\u2019t know that that story is true. I think it should be investigated,\u201d Nelson told Semafor\u2019s Burgess Everett. \u201cIf the story is true that there have been multiple conversations between Elon Musk and the president of Russia, then I think that would be concerning, particularly for NASA, for the Department of Defense, for some of the intelligence agencies.\u201d Some US officials have raised counterintelligence concerns in the last year about Musk\u2019s interactions with US adversaries like Russia, but the US intelligence community is wary of looking into those interactions because Musk is an American citizen, an official familiar with the matter told CNN. Several White House officials told the Journal they weren\u2019t aware of the contact between Musk and Putin, and the paper said knowledge of the discussions \u201cappears to be a closely held secret in government.\u201d The discussions were confirmed to the Journal by several current and former US, European and Russian officials. In one instance, the newspaper cited a request from Putin to Musk not to activate his Starlink satellite internet service over Taiwan \u201cas a favor to Chinese leader Xi Jinping.\u201d Musk did not respond to the Journal\u2019s requests for comment. National Security Council spokesperson John Kirby said Friday that he had seen the reporting but the White House is \u201cnot in a position to corroborate\u201d it and deferred questions to Musk. A Pentagon spokesman told the Journal that the Defense Department does not comment on \u201cany individual\u2019s security clearance, review or status, or about personnel security policy matters in the context of reports about any individual\u2019s actions.\u201d Kremlin spokesman Dmitry Peskov told the newspaper that Musk and Putin have only had one telephone call in which they discussed \u201cspace as well as current and future technologies.\u201d Since Russia\u2019s invasion of Ukraine in early 2022, Musk\u2019s support for Ukraine \u2014 exemplified by SpaceX\u2019s provision of Starlink services \u2014 has diminished as his public statements about the conflict have become further aligned with those of Trump, who has said he would negotiate an end to the war quickly. The satellite internet terminals provided by Musk\u2019s company have been a vital source of communication for Ukraine\u2019s military, allowing it to fight and stay connected even as cellular and internet networks have been destroyed. Dmitri Alperovitch, a Russia and cybersecurity expert, told CNN\u2019s Alex Marquardt Friday on \u201cCNN News Central\u201d that Musk\u2019s Starlink is \u201cessential to Ukraine in particular because they really could not prosecute this war without\u201d its services. After Musk trumpeted his early support for Ukraine, SpaceX then abruptly asked the Pentagon to pay tens of millions of dollars per month to fund Starlink in Ukraine and take the burden off SpaceX. In response to that reporting, Musk then abruptly announced on Twitter that he had withdrawn the funding request. Around the same time, Musk used a poll on X to suggest a \u201cUkraine-Russia Peace\u201d plan that included re-doing elections \u201cunder UN supervision\u201d in the regions of the country recently annexed illegally by Russia. After Ukrainian President Volodymyr Zelensky questioned Musk\u2019s preference in the war, the tech entrepreneur responded that he \u201cstill very much support(s) Ukraine\u201d but feared \u201cmassive escalation.\u201d SpaceX had previously limited its Starlink signal to areas controlled by Ukrainian forces, hampering potential advances that would have relied on Starlink communications. SpaceX then enlarged it to the rest of the country, and earlier this year, Ukraine\u2019s Defence Intelligence claimed it has confirmed the use of Starlink satellite communications by Russian forces in occupied areas. Russia appeared to be buying the terminals from third parties; SpaceX said it did not do business of any kind with the Russian government or its military and that its service would not work in Russia. The statement didn\u2019t address whether it would work in occupied Ukraine. Ukraine\u2019s claim followed revelations in a biography of Musk, written by Walter Isaacson, about the satellite system\u2019s use in the war. According to an excerpt from the book, Musk did not grant a Ukrainian request to turn on his company\u2019s Starlink satellite communications network near the Crimean coast last year to disrupt a Ukrainian sneak attack on the Russian naval fleet. Musk\u2019s decision, which left Ukrainian officials begging him to turn the satellites back on, was driven by an acute fear that Russia would respond to a Ukrainian attack on Crimea with nuclear weapons, a fear driven home by Musk\u2019s conversations with senior Russian officials, according to Isaacson. In October 2022, Musk denied a claim by American political scientist Ian Bremmer that he had spoken with Putin about the war and a proposed \u201cpeace plan\u201d to end the conflict. Musk, who is also the CEO of Tesla and owner of X, has emerged as a major financial figure in this year\u2019s presidential election. He plowed nearly $44 million in October into a super PAC working to restore Trump to the White House \u2014 pushing the billionaire\u2019s total donations to the group to nearly $119 million \u2014 and he appeared with Trump on the campaign trail earlier this month in Butler, Pennsylvania. Musk also held his own town halls last week in Pennsylvania, where he urged voters to support Trump and promoted several debunked conspiracy theories about the 2020 election. The two have publicly discussed a potential government role for Musk. In recent days, Musk also offered splashy, $1 million daily sweepstakes for swing-state voters that has drawn scrutiny from the US Justice Department. Despite a warning from the Justice Department that the payments might be illegal, Musk\u2019s super PAC awarded",
    "comments": [
      {
        "author": "gnabgib",
        "text": "Related <i>Elon Musk&#x27;s Conversations with Vladimir Putin</i> (91 points, 4 days ago, 49 comments) <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41941415\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41941415</a>",
        "time": "2024-10-28T19:54:59"
      }
    ],
    "description": "NASA Administrator Bill Nelson on Friday called for an investigation into a Wall Street Journal report that SpaceX founder and Donald Trump ally Elon Musk and Russian President Vladimir Putin have been in \u201cregular contact\u201d since late 2022.",
    "document_uid": "27167d2af1",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "41980081",
    "title": "Free/open source offline handwriting recognition?",
    "url": "https://news.ycombinator.com/item?id=41980081",
    "score": 1,
    "timestamp": "2024-10-29T07:43:05",
    "source": "Hacker News",
    "content": "Chatgpt handwriting recognition is amazingly accurate. Tesseract, ocropus, gocr and friends don&#x27;t comes close.<p>Any highly accurate complement or replacement for these?",
    "comments": [],
    "description": "No description available.",
    "document_uid": "2336efef93",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41980063",
    "title": "Best Websites To download Korean movies with English subtitles",
    "url": "https://www.newsreportage.com.ng/2024/10/best-websites-to-download-korean-movies.html",
    "score": 1,
    "timestamp": "2024-10-29T07:40:07",
    "source": "Hacker News",
    "content": "K-Dramas have become a popular choice of movie/series genre for most people including Nigerians. Known for their emotional, romantic and entertaining storytelling, K-Dramas have come to win the hearts of drama movies and TV series lovers worldwide. In this post, I decided to do a deep research and source out some of the Best websites to download K-Dramas with English subtitles attached. Most of the time, we find ourselves downloading srt subtitle files separately in order to understand what they say in K-Dramas. However, In this post, you will get to know websites where you can download your Korean dramas with English subtitles attached. Read more below as you progress down this Informative post. WHERE TO DOWNLOAD KOREAN MOVIES AND SERIES WITH ENGLISH SUBTITLES After surfing the Internet, I came up with a very useful list of some of the Best Websites to download your Korean movies and series with English subtitles for free. See them below. 1. Viki2. Dramacool3. iQIYI4. MyAsianTV 5. Netflix 6. MyDramaList7. Telegram Now let's get to know detailed information about the aforementioned websites. Stay with me. Note: The list is not a countdown or ranking, just random selection of the best websites. 1. VIKIInfographicsSometimes called Rakuten Viki, this website takes our number 1 spot. Viki has a huge library of Korean, Chinese and Japanese movies and TV series available for free to its users. It is owned by the Japanese Technology company, Rakuten and can also be accessed via a mobile app. Visit Viki to download Korean movies and more with English subtitles. Website Link: Viki2. DRAMACOOL InfographicsDramacool is also another stop for your Korean movies. The website also has lots of movies and shows in its library, which you can browse and download to enjoy. Dramacool also has two versions of the website with different domain extensions in case of downtime on either one of them. See link below. Website Link: Dramacool 3. IQIYI InfographicsUnlike others, iQIYI lets you stream/watch movies and series online only. There's a mobile app as well if you prefer to use the mobile version. IQIYI library boasts of content from Korean, to Chinese to Thai and many more Asian Dramas. Basically anything Asian content. Website Link: iQIYI 4. MYASIANTV InfographicsThis is another popular site for Korean and Asian content as a whole. The site\u2019s library is huge with an easy to navigate interface. MyAsianTV also has a mobile app which is available on various app stores. Visit the site or download the mobile app and explore a wide range of Asian content. Website Link: MyAsianTV 5. NETFLIXInfographicsNetflix is the biggest movie streaming service in the whole world. Netflix also has a wide library of Korean movies and shows. Though Netflix is a paid platform, which can serve as a hassle for most people, if you have a subscribed Netflix account, you can stream or download your K-Dramas with English subtitles as well. Website Link: Netflix6. MYDRAMALIST InfographicsNow when it comes to MyDramaList, it's not a movie streaming or download website. Instead, it's a website for Asian content recommendations and ratings. On MyDramaList, you can explore various lists and genres, read plots, get to know casts and decide if you will watch a particular show or not. You can also view a list of trendy movies and series which people are currently watching and recommending. Visit the site below. Website Link: MyDramaList 7. TELEGRAM InfographicsTelegram is more than a messaging app. With Telegram, you can search for Korean movies or series and simply subscribe to the channels uploading them and download to your device. Simply use the search bar to search for movies or series and you may come across a channel with juicy content. And there you have it, a list of free websites you can use to download Korean movies and shows with English subtitles for free. From the list, you can see that most of the websites also come with a mobile app to suite your preference. Do well to share this post with your friends and fellow K-Drama lovers. CONCLUSION A big thank you for reading this post, please read more and make your comments known using the comment section below. See you in my next post.",
    "comments": [],
    "description": "Discover websites where users can download Korean movies and series with English subtitles for free. ",
    "document_uid": "7161b65a09",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41980061",
    "title": "Election Interference by Russia, China, Iran",
    "url": "https://www.nytimes.com/2024/10/29/technology/election-interference-russia-china-iran.html",
    "score": 2,
    "timestamp": "2024-10-29T07:39:23",
    "source": "Hacker News",
    "content": "Please enable JS and disable any ad blocker",
    "comments": [],
    "description": "No description available.",
    "document_uid": "c7ba296677",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41980048",
    "title": "Death by a Thousand Roundtables",
    "url": "https://chalmermagne.substack.com/p/death-by-a-thousand-roundtables",
    "score": 1,
    "timestamp": "2024-10-29T07:36:56",
    "source": "Hacker News",
    "content": "For this piece, I\u2019ve teamed up with Anastasia Bektimirova, a researcher focusing on technology and science policy. Anastasia writes in a personal capacity, so nothing in this piece should be seen as representing the views of her current or past employers. You can keep up with her work on X at @anastasiabekt. I have seen the dark universe yawning, where the black planets roll without aim, where they roll in their horror unheeded, without knowledge or lustre or name.The cheap coffee, the untouched orange juice, the collection of half-stale pastries, the same two people hijacking the discussion, and the dawning sense that the Earth is rotating on its axis. These are the hallmarks of the dreaded Westminster roundtable. As friends who have routinely suffered through these events, obtaining nothing from the experience, we\u2019ve often struggled to work out what\u2019s in it for their conveners or sponsors. We believe that this speaks to a wider malaise in the world of policy. Fuelled by a flood of money from technology companies with vast budgets, we believe that the quality of policy work has been negatively correlated with quantity. As well as being wasteful, it leads to an impoverished standard of debate on issues that matter.In some group therapy dressed up as a think piece, we canter through the bad types of work that clog up timetables, inboxes, as well as how organisations fritter away their potential influence. And in the spirit of being constructive, we suggest some potential improvements. While these observations are written with technology policy in mind, we suspect they have broader applicability.A favourite of in-house policy teams, this work usually consists of subjecting start-up founders and a loosely defined set of \u2018civil society\u2019 representatives to a series of interminable discussions about a set of \u2018principles\u2019.These principles are usually sufficiently vague that it\u2019s basically impossible to object to them without sounding mental (\u201cI hate collaboration\u201d, \u201cinnovation sucks\u201d, \u201cAI shouldn\u2019t be developed responsibly\u201d). This renders them practically useless. But the company hopes that by including a constellation of logos on their reports, they can signpost their broad coalition of support when they\u2019re arguing with regulators. The problem with this genre of work is that the trade-offs don\u2019t balance out. The \u2018research\u2019 is blatantly self-interested, but doesn\u2019t compensate for it by being interesting. Because it\u2019s desperately trying to hide that it\u2019s an advocacy tool, it stops short of surfacing good policy ideas. The only winners are the people running the work, who can signal how busy they are to their bosses.Rating: 0/5 - no one understands what you do for a living. You are ridiculed in the street and at dinner parties. Common among think tanks desperate to appear relevant, this genre involves proposing initiatives or policies that are, in fact, already being implemented by the government. The authors begin by identifying an existing government programme or policy, before repackaging it with a few minor tweaks. Look out for vague calls for \u201cexpansion\u201d, \u201cenhancement\u201d, \u201crevitalisation\u201d, or, when the authors are really phoning it in, \u201cdelivering on current plans\u201d. While this means a government minister will probably speak at your event, it\u2019s a waste of time and may divert attention away from genuine problems. Once you recognise this trope, you\u2019ll see it everywhere.Rating: 0.5/5 - could have been a tweet. If you want to suck up to ministers professionally, become a SpAd.You refer to yourself as a \u2018thought leader\u2019 on LinkedIn, but you\u2019re just making it up. The \u2018implausibly precise future prediction\u2019 is a cousin of the bad economic impact assessment, but it\u2019s even worse. At least the impact assessment is usually trying to answer a theoretically answerable question. Typically, this work is produced by either an academic with a book to sell or a new grad at McKinsey with a PowerPoint to make. Because this work provides certainty and specificity, it acts as a frequent source of zombie statistics that clog up corporate presentations and ministerial remarks, years after anyone remembers how they were invented calculated. For example, we\u2019ve seen a Microsoft claim that AI could add \u00a3550B to the UK economy by 2035 bandied around various corporate presentations. The source? A \u2018study\u2019 where the authors asked GPT-4\u2026 #scienceRating: 1/5 - horoscopes for people with PPE degrees.This usually boils down to surveying the potential beneficiaries of a tax change or spending decision. Unsurprisingly, said stakeholders are usually in favour of more tax breaks or a greater share of public spending. In Alex\u2019s recent piece on UK VC policy, this was often one of the government\u2019s preferred ways of measuring the success of its interventions in the venture market. The issue is not that this data is useless, it\u2019s just woefully incomplete. Setting aside their self-interest, while industry representatives will know their pain points and can point to domain-specific subtleties that officials miss, they\u2019re often not well-versed in the machinery of government. We have both endured roundtables where this was apparent. Industry representatives are also unlikely to consider (or care about) the implications for others of policies that favour them (e.g. funding being diverted). But these studies skip over this and don\u2019t provide any kind of independent economic analysis. Unfortunately, this style of policy work is popular because it suits everyone involved. Consultancies and lobby groups can advocate for their clients, while governments like producing documents that say their policies are being received well by other people.Rating: 1.5/5 - while it might surface the odd useful nugget of information, this work is usually low value and the enemy of good policy-making.The preserve of the well-meaning but naive. They wrongly approach the policy-making process like it\u2019s a seminar and throw out some half-baked ideas or underdeveloped concepts. They then expect policymakers to do the hard work of turning these into actionable ideas through some kind of undefined co-creation process. The end product is filled with lofty goals and grand visions, but short on practical details. Think calls for \u201ca complete overhaul of the higher education funding model\u201d, \u201cgreater cooperation between the private and public",
    "comments": [],
    "description": "Most policy work is pointless. It doesn\u2019t need to be.",
    "document_uid": "10f4cbfa1a",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41980024",
    "title": "How my 14 year-old Instagram handle @javier was stolen",
    "url": "https://twitter.com/javier/status/1851021308190228593",
    "score": 1,
    "timestamp": "2024-10-29T07:31:48",
    "source": "Hacker News",
    "content": "How my 14 year-old Instagram handle @javier was stolen",
    "comments": [],
    "description": "No description available.",
    "document_uid": "3c62a143f0",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41980017",
    "title": "A simple no-code tool to streamline AI workflows",
    "url": "https://inweave.ai",
    "score": 1,
    "timestamp": "2024-10-29T07:30:43",
    "source": "Hacker News",
    "content": "A simple no-code tool to streamline AI workflows",
    "comments": [
      {
        "author": "ptsilkov",
        "text": "The Story<p>I am working on a project that aims to allow everyone to harness AI models in their daily workflows without a single line of code. This has been somewhat of an organic development, as I started building this for my own startup. We are bootstrapped and can&#x27;t yet allow to automate mundane tasks through hiring extra people. So, I had a simple idea - outsource content moderation and other repetitive tasks well suited for LLMs to AI. I initially started coding directly in our codebase. It was tedious. I had to build everything from scratch and changes meant I needed to change the source code. Then it dawned on me I can&#x27;t be the only one. And, here we are, building a tool that will streamline this whole process and allow everyone in our organization to contribute without writing a single line of code.<p>Who This is For<p>Startups, Scaleups, Micro businesses, Freelancers - everyone who can&#x27;t yet afford to code this on their own. Let&#x27;s not fool ourselves - all the big ones are doing this in-house, because they can afford it.<p>Who This is Not For<p>This tool is not for the super advanced technical teams where even the office cat can whip up a web server in three lines of code with bun. These guys will use n8n or some more advanced tool that has more capabilities, but cannot realistically be utilised by people who don&#x27;t have a technical background.<p>There are a few planned advantages here:<p>1. Unlimited models and providers - use openai, groq, anthropic + more coming soon.\n2. No annoying usage limits - use your own API keys and pay as you go\n3. Say &#x27;Goodbye&#x27; to multiple subscriptions of $20&#x2F;month each - you only pay for what you use\n4. Create assistants that you can use with any provider, model, and your own data\n5. Extensibility - if you&#x27;re more of an advanced user (I am looking at you, technical founders), access all assistants, providers, and models through simple API access.<p>So, it&#x27;s a simple idea - automate the mundane stuff with AI the simple way.<p>I am just looking for feedback. I want to gauge the interest and find out, if other people would really benefit from such a tool.<p>N.B.: This is still in an early phase of development. Not production ready.",
        "time": "2024-10-29T07:30:43"
      }
    ],
    "description": "An AI-powered platform that allows startups and scaleups to grow efficiently.",
    "document_uid": "2d38976b1b",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41980016",
    "title": "Startups should hire recruiters much sooner than they think",
    "url": "https://a16zcrypto.com/posts/article/why-startups-should-hire-recruiters/",
    "score": 1,
    "timestamp": "2024-10-29T07:30:41",
    "source": "Hacker News",
    "content": "When a startup founder finds product-market fit and secures funding to scale, the advice starts to roll in from investors, advisors, and mentors alike: Hiring the right people will create the bedrock of your company. With limited resources and big plans, hiring the wrong people can stop a startup\u2019s plans dead in their tracks. This advice is absolutely correct and completely useless. It\u2019s useless because it contains no actionable guidance whatsoever. And learning how to hire is a full-time job. That\u2019s why hiring the right recruiting help can also set founders up for success from the start. In this post, I\u2019ll share how. In fact, a recruiter should be among your first 10 hires. This may seem counterintuitive \u2014 if founders are pressed for time and need the right technical talent right away, shouldn\u2019t other hires take precedence? Far from it. Having worked and helped scale Airbnb (which I joined at 50 employees) and Coinbase (which I joined at 7), and having counseled countless other startups, I can attest to the fundamental importance of investing in recruiting talent at this early stage. When I joined Airbnb early on I was actually the third recruiter hired, and when I joined Coinbase I was the second A recruiter will save time \u2014 lots of time Sourcing (cold outreach) and managing the process for multiple candidates through multiple interview rounds can take a lot of time. In crypto especially, saving time is crucial because there is a limited pool of candidates with specialized expertise, or candidates talking to the same few companies all at once. Founders could easily be liaising with 100 candidates, and with all the other priorities they are juggling, some ball is going to drop. What does this mean practically? A bad candidate experience, missed hires, and a tarnished reputation. Founders can\u2019t afford to give anyone a bad experience. Not only is it a small world, but candidates are usually comparing companies as they interview. A poor candidate experience will easily knock you out of the running not just with that one candidate but with others too. In short, hiring well takes up a lot of time. If hiring is not taking up much time, then your startup\u2019s hiring process is likely too easy \u2014 or your company may be moving too quickly, which can lead to people problems and churn later. Also remember that as a founder, time is your most valuable resource. Hiring a recruiter means you can focus on other top priorities and allocate your time appropriately. You\u2019ll still be involved in the hiring loops, but your personal involvement will come later \u2014 and be more strategic and efficient. Here\u2019s a scenario that I often see with first-time founders: They are looking to make their first technical hires, have a good network, but the timing isn\u2019t right for many of the candidates in their network (and it\u2019s a competitive hiring landscape). Maybe they get a couple hires using this approach. So then they need to proactively reach out to candidates. But this raises a new series of questions: How many should you contact? Should your company use a sharpshooter or volume recruiting strategy \u2014 researching extensively to find highly qualified candidates with directly applicable skills, or casting a wide net? Your startup will likely need to employ a blend of both narrow-targeting and high-volume strategies. Both have their place depending on the role \u2014 but both are a lot of work. Sharpshooter recruiting can take a lot of time. But industry stats for engineering outreach using volume recruiting show you need to reach out to between 50 and 100 people to make even 1 hire. Each message to an engineer requires some amount of customization. The response rate from a candidate to a recruiter is at best 30% (that rate can be higher if a founder or technical leader reaches out). Let\u2019s imagine your company is only hiring 2 engineers. You could easily have 30 candidates running through your process, each doing 2 to 5 interviews with your team depending on how far they get in the process. That\u2019s 60 to 150 interviews to manage and schedule in just a very short timeframe. As the founder, you\u2019re likely leading some blend of product, engineering, marketing, fundraising, customer support, and general operations, as well as liaising with external counsel daily and so on. So even if you have the best of intentions with candidates, the candidate experience will slip. Negative experiences quickly make their way to Glassdoor reviews, future candidates read those reviews, candidates talk to each other at meetups and conferences about companies, and so on. Having a recruiter partner closely with the hiring managers on outreach strategy alleviates all of these problems. In the early days of Coinbase, I partnered with engineers and leaders like Brian Armstrong, Rob Witoff, and Varun Srinivasan, to help them build out their teams. I\u2019d put together lists of candidates who I thought were great, and, if they liked them, I\u2019d ghostwrite their initial candidate outreach; then they\u2019d do the phone screens while I managed the entire process. These partnerships worked well and landed many hires, at both the individual contributor and executive levels. The founder as a talent magnet The founder has an important role to play in early stage hiring \u2014 this includes articulating a compelling vision and mission for the company, defining values that can be implemented by their team through the hiring process, and of course selling and evaluating top-tier talent. Once hired, a great recruiter can help create and run a tight hiring process, pulling the founder in at strategic moments. A founder should remain a part of every interview loop until they have properly trained their leadership team how to hold an excellent hiring bar, and to recognize and evaluate for the right cultural add. But there\u2019s another crucial role for founders to play. Great engineers want to work with other great engineers building cool stuff. While this may seem like a reason for technical",
    "comments": [],
    "description": "How early should you hire a recruiter? They should be among your first ten employees: An explanation of why, and how to find the right one",
    "document_uid": "5683cef805",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979984",
    "title": "Give Yourself More Playtime",
    "url": "https://martinrue.com/give-yourself-more-playtime/",
    "score": 1,
    "timestamp": "2024-10-29T07:23:26",
    "source": "Hacker News",
    "content": "Give Yourself More Playtime I remember my first computer fondly. As a kid I was more than addicted to the Commodore 64 \u2013 perfectly happy to sit and endure at least 5 minutes of the most colourful, potentially epileptic attack you can imagine, simply to have a few games of Boulder Dash. It was worth every photon, without a doubt. Today its mechanical keyboard, beige plastic casing, and whopping 64 KB of memory condemns it almost exclusively to a quiet life of nostalgia\u2026 and some weird Internet forums. But back then it was the most fascinating thing 11-year-old me had ever seen. Between games of Boulder Dash, sheer curiosity would keep bringing me back to this bright blue screen with hideous white text, wondering what else it could do. Gradually I discovered new commands that made it do other things, besides just LOAD \u2013 although I still typed that often enough to cause the dog to eat a lot of my homework. In time I realised that the machine could run code called BASIC, and that I could simply type it in and then type RUN to make it do things. Being 11 and fearing the inevitably gradual violation of personal privacy by government, the first program I wrote was a password program. It wasn't very clever. Not even a bit, in fact: 10 PRINT \"ENTER PASSWORD\" 20 INPUT L$ 30 IF L$ = \"SECRET\" THEN GOTO 50 40 GOTO 10 50 PRINT \"OK\" Well, that was until I realised something insanely awesome. You probably don't think it's insanely awesome, but I definitely did when I was 11. Armed with the knowledge that typing LOAD triggered loading the first program from the datasette, I figured that if LOAD became my new line 50, this program could protect my computer from the government. It's 11-year-old genius in front of your very eyes. The plan was simple: I type this program and follow it with a swift RUN command. My code starts running and loops until someone types the correct password. When they do, rather than the imaginatively corrupt PRINT \"OK\", it runs LOAD and begins loading whichever game is in the datasette. Man, I was impressed with myself. Well, until I realised you could just turn the computer off and back on again, type LOAD, and completely subvert my entire plan. Obviously, the government would figure that out, so I needed something better. After some careful thought, it became clear to me that the program needed to be on the actual tape, not sitting around in memory waiting to suicide as soon as the power went out. Luckily I discovered the sibling of LOAD, SAVE, which allowed me to do just that. So, I typed my password program once again, inserted a blank cassette and ran SAVE. SAVING READY. Sweet Jesus, it worked. I released the RECORD and PLAY keys on the datasette, pressed REWIND and rebooted. Yes, if you wanted to run your program again you'd have to rewind it\u2026 and that wasn't the only awesome thing about the 90s. As the computer came back to life, I was sat there with my LOAD command at the ready. Without even so much as a colour leaping out of the screen and smashing me in the face, my password program was running! Still, it would be a pretty big ask to get the government to take out the game they wanted to load, put my password program cassette in, and run that first. I mean, I could write some instructions, but I just don't think it's very likely they would follow them. So, onto my next wave of genius. I noticed something interesting: this Joseph and the Amazing Technicolor Dreamcoat light show that happens when a game is loading doesn't begin immediately after running LOAD. In other words, the tape is spinning but nothing is happening for about 5 seconds. Conveniently, my awesome password program took about 1 second to write to a blank tape. If my maths was correct, I had plenty of space at the beginning of Boulder Dash to stash my own program. Now I was really getting somewhere! MI5 were probably shitting their pants, I figured. At my earliest inconvenience I invited a friend over to see my achievement. He had a Commodore 64 too, so I knew he'd appreciate what I'd done here. He loads my tape. We wait. And BOOM! ENTER PASSWORD I knew he was impressed. Well, I think he was. There wasn't much time between the password prompt appearing and him pressing the RUN/STOP key, typing LIST, and reading the god damned password right out of the code. Obviously, the government would figure that out too, so I needed something better. My new target swiftly became that damn RUN/STOP key. It took me a few days until I got the chance to dial up to the Internet (yes, that was a thing too) and search for a way to stop my friends, and the government, breaking my code and listing it to read the password. Eventually I learned that POKE allows you to change values at specific memory addresses, and with that it's possible to overwrite vectors to prevent routines such as LIST and STOP from working. With my newfound POKE knowledge, my password program was now solid. It was time to write this baby to the game tape, making absolutely sure nobody in my family could get better than me at Boulder Dash. I was so impressed with my new skill that I didn't just stop at Boulder Dash. In fact, I password protected every single one of my favourite game tapes using the same program, quickly rising to fame as the family gatekeeper of any game worth playing. It was coming up to summer, and another thing we did as 11-year-old kids in the 90s was go outside in summer. Weird, I know. 4 or 5 weeks of being a healthy 90s outdoor summer kid and I found myself ready for",
    "comments": [],
    "description": "I remember my first computer fondly. As a kid I was more than addicted to the Commodore 64.",
    "document_uid": "f2a0638fc2",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979976",
    "title": "Historian thinks the Ark of Covenant and Holy Grail are buried under this house",
    "url": "https://www.thebrighterside.news/post/historian-believes-the-ark-of-covenant-and-holy-grail-are-buried-under-this-english-house/",
    "score": 1,
    "timestamp": "2024-10-29T07:22:03",
    "source": "Hacker News",
    "content": "In a breathtaking discovery, esteemed historian and anthropologist David Adkins has begun a daring journey that could unveil a fabled trove, rumored to encompass the Ark of the Covenant and the Holy Grail, concealed beneath the venerable Sinai Park House near Burton.Famed for his earlier unearthing of the 10,000-year-old skull \"Greta,\" David Adkins entertains the conviction that the elusive treasures of the Knights Templar might lie hidden within a complex network of subterranean passages and chambers beneath this grand estate.Historian David Adkins believes the Knights Templar hid treasure in Burton. (CREDIT: David Adkins) Sinai Park House, a majestic landmark in Burton, is believed to rest upon caverns of such vastness that they could potentially rival the size of Westminster Abbey, as asserted by a geologist. This revelation has piqued the interest of both historians and treasure hunters alike, as they contemplate the implications of such a discovery.David Adkins, who has diligently researched the connections between this Midlands property and the Knights Templar, an elite group of fighters founded in 1119, states, \"Sinai House is probably the most important house in England.It is an awe-inspiring building perched high on a hill overlooking the town. A timber-framed structure, crumbling and at risk, it hides secrets of international importance - and probably conceals the truth behind the Holy Grail itself.\"If David's theory proves to be accurate, it could mark the end of a centuries-old enigma. \"There is a strong argument for the famous Templar hoard being buried somewhere under the house,\" David Adkins asserts. \"This theory has never been considered before, and no one has ever carried out geophysical studies under the house.\"Historical accounts reveal that attempts were made to explore a concealed passageway at Sinai House during the 1800s, but these efforts were thwarted by fumes, leading to the subsequent bricking up of the cellar's stone archway. The house's historical significance is evident as it once served as a hideout for the Knights Templar, whose founder, Hughes de Payens, had familial ties to the Burton Abbey Estate.Hughes de Payens, whose name evolved into Pagnel and then Paget in literature, eventually led to William Paget's ownership of the Burton Abbey estate in 1539, which culminated in the demolition of the abbey. David Adkins suggests, \"The answer to both questions lies, of course, with the Templar treasure. Firstly, if Paget was indeed a descendant, he was only around six generations removed from Hughes de Payens. As such, it is highly likely that tales and family myths about the lost treasure being on the Burton Abbey estate had been passed down within his family.\"Sinai House and owner Kate Murphy. (CREDIT: Kate Murphy) He continues, \"Whatever compelled him, it was strong enough for him to firstly acquire the Burton lands then take down the buildings brick by brick. They were looking for something.\"David Adkins then delves into a compelling historical connection, saying, \"The first major clue which points to the house as the final resting place of the Templar treasure is, in hindsight, blatantly obvious and almost shouts at us from the pages of history. For shortly after the Templars left Jerusalem, the Abbey took personal charge of Sinai House.The question is why they needed it then; they had never wanted it during the previous 700 years, so why at that particular point in history did they suddenly need the house?\"Sinai Park House on its commanding hilltop site in Burton upon Trent - (CREDIT: Archant) One remarkable aspect of Sinai House is its unique geographical location atop a labyrinth of natural tunnels and caves. According to Adkins, a geologist informed him that there were caverns beneath the house as large as Westminster Abbey itself. These natural formations would have been an ideal hiding place for the Templars, who were skilled tunnel builders, to conceal their valuable treasures.Furthermore, Adkins posits that Burton's central location would have appealed to the Knights Templar as a secure spot to protect their treasures from potential invaders. He explains, \"Back in 1307, powerful people and foreign governments were understandably keen to acquire the hoard, and any site near the coast would have been considered far too vulnerable. Sinai House in the heart of England did not pose this problem \u2013 the Templars knew that an invading force from abroad could never reach the heart of England.\"Sinai House's strategic location, flanked by Needwood Forest and the River Trent, added to its obscurity, making it an ideal sanctuary for the Templars. Additionally, the abbots of Burton Abbey were known to be non-conformists, which made them trustworthy allies during the Templar persecution.The opening was sealed with bricks \u2013 (CREDIT: SWNS) Intriguingly, these factors, combined with David Adkins' groundbreaking research, now point to the possibility that the long-lost treasures of the Knights Templar, including the fabled Ark of the Covenant and the Holy Grail, may be awaiting discovery beneath the historic Sinai Park House. If this theory proves true, it could rewrite history and unveil a treasure trove that has eluded seekers for nearly a millennium. The world watches with bated breath as the investigation into this remarkable historical mystery unfolds.",
    "comments": [],
    "description": "Treasures of the Knights Templar might lie hidden within a complex network of subterranean passages and chambers beneath this grand estate",
    "document_uid": "46e9329c1f",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979950",
    "title": "Specifying Serializability in TLA+",
    "url": "https://surfingcomplexity.blog/2024/10/28/serializability-and-tla/",
    "score": 1,
    "timestamp": "2024-10-29T07:16:08",
    "source": "Hacker News",
    "content": "Concurrency is really, really difficult for humans to reason about. TLA+ itself was borne out of Leslie Lamport\u2019s frustration with the difficulty of write error-free concurrent algorithms: When I first learned about the mutual exclusion problem, it seemed easy and the published algorithms seemed needlessly complicated. So, I dashed off a simple algorithm and submitted it to CACM. I soon received a referee\u2019s report pointing out the error. This had two effects. First, it made me mad enough at myself to sit down and come up with a real solution. The result was the bakery algorithm described in [12]. The second effect was to arouse my interest in verifying concurrent algorithms. Modeling concurrency control in database systems is a great use case for TLA+, so I decided to learn use TLA+ to learn more about database isolation. This post is about modeling serializability. You can find all of the the TLA+ models referenced in this post in my snapshot-isolation-tla repo. This post isn\u2019t about snapshot isolation at all, so think of the name as a bit of foreshadowing of a future blog post, which we\u2019ll discuss at the end. Modeling a database for reasoning about transaction isolation In relational databases, data is modeled as rows in different tables, where each table has a defined set of named columns, and there are foreign key relationships between the tables. However, when modeling transaction isolation, we don\u2019t need to worry about those details. For the purpose of a transaction, all we care about is if any of the columns of a particular row are read or modified. This means we can ignore details about tables, columns, and relations. All we care about are the rows. The transaction isolation literature talks about objects instead of rows, and that\u2019s the convention I\u2019m going to use. Think of an object like a variable that is assigned a value, and that assignment can change over time. A SQL select statement is a read, and a SQL update statement is a write. An example of how we\u2019re modeling the database Note that the set of objects is fixed during the lifetime of the model, it\u2019s only the values that change over time. I\u2019m only going to model reads and writes, but it\u2019s simple enough to extend this model to support creation and deletion by writing a tombstone value to model deletion, and having a not-yet-created-stone value to model an object that has not yet been created in the database. I\u2019ll use the notation r[obj, val] to refer to a read operation where we read the object obj and get the value val and w[obj, val] to mean where we write the value val to obj. So, for example, setting x=1 would be: w[x, 1], and reading the value of x as 1 would be r[x, 1]. I\u2019m going to use Obj to model the set of objects, and Val to model the set of possible values that objects can take on. Obj is the set of objects, Val is the set of values that can be assigned to objects We can model the values of the objects at any point in time as a function that maps objects to values. I\u2019ll call these sorts of functions environments (env for short) since that\u2019s what people who write interpreters call them. Example of an environment As an example of syntax, here\u2019s how we would assert in TLA+ that the variable env is a function that maps element of the set Obj to elements of the set Val: What is serializability? Here\u2019s how the SQL:1999 standard describes serializability (via the Jepsen serializability page): The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins. An execution history of reads and writes is serializable if it is equivalent to some other execution history where the committed transactions are scheduled serially (i.e., they don\u2019t overlap in time). Here\u2019s an example of a serializable execution history. Atul Adya famously came up with a formalism for database isolation levels (including serializability) in his PhD dissertation work, and published this in a paper co-authored by Barbara Liskov (his PhD advisor) and Patrick O\u2019Neil (an author of the original log-structured merge-tree paper and one of the co-authors of the paper A Critique of ANSI SQL Isolation Levels, which pointed out problems in the SQL specification\u2019s definitions of the isolation levels ). Specifying serializability Adya formalized database isolation levels by specifying dependencies between transactions. However, I\u2019m not going to use Adya\u2019s approach for my specification. Instead, I\u2019m going to use a state-based approach, like the one used by Natacha Crooks, Youer Pu, Lorenzo Alvisi and Allen Clement in their paper Seeing is Believing: A Client-Centric Specification of Database Isolation. It\u2019s important to remember that a specification is just a set of behaviors (series of state transitions). We\u2019re going to use TLA+ to define the set of all of the behaviors that we consider valid for serializability. Another way to put that is that our specification is the set of all serializable executions. We want to make sure that if we build an implementation, all of the behaviors permitted by the implementation are a subset of our serializability specification. Note: Causality is not required Here\u2019s an example of an execution history that is serializable according to the definition: This looks weird to us because the write happens after the read: T1 is reading data from the future! But the definition of serializability places no constraints on the ordering of the transaction, for that you need a different isolation level: strict serializability. But we\u2019re modeling serializability, not strict serializability, so we allow histories like the one above in our specification. (I\u2019d say \u201cgood luck actually implementing a system that can read",
    "comments": [],
    "description": "Concurrency is really, really difficult for humans to reason about. TLA+ itself was borne out of Leslie Lamport's frustration with the difficulty of write error-free concurrent algorithms: When I first learned about the mutual exclusion problem, it seemed easy and the published algorithms seemed needlessly complicated.\u00a0 So, I dashed off a simple algorithm and submitted\u2026",
    "document_uid": "f595a4c807",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979940",
    "title": "I am a Pokemon fan and made Pokemon sleep calculator with 6 language supported",
    "url": "https://news.ycombinator.com/item?id=41979940",
    "score": 1,
    "timestamp": "2024-10-29T07:14:46",
    "source": "Hacker News",
    "content": "Hii !<p>I\u2019m excited to share a little project : Pok\u00e9mon Sleep Grader!<p>As a huge Pok\u00e9mon fan , why not develop a tool than calculate grade of Pok\u00e9mon.<p>You can select your Pok\u00e9mon, its nature, and than subskills you\u2019ve unlocked.<p>After that, just hit &quot;Calculate Score&quot; and see the results!<p>I enjoyed a lot while working on this project.<p>Its designed to be dead simple + user-friendly.<p>I\u2019d love to hear your thoughts and any feedback you might have.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "4b93a0ffb3",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979932",
    "title": "New global navigation data service launched in Ireland",
    "url": "https://www.rte.ie/news/ireland/2024/1029/1477887-gps-service/",
    "score": 1,
    "timestamp": "2024-10-29T07:13:26",
    "source": "Hacker News",
    "content": "The National Standards Authority of Ireland's National Metrology Laboratory (NSAI NML) has announced the launch of Europe\u2019s first ever verified GPS/GNSS Data Stream service, an extension to the National Timing Grid of Ireland. The initiative is designed to ensure the availability of secure and reliable Global Navigation Satellite System (GNSS) data. GNSS systems, such as the US\u2019s Global Positioning System (GPS) and the EU\u2019s Galileo, provide real-time navigation assistance for people helping them find the most efficient routes, avoid traffic, and reach their destinations safely and quickly. In emergency situations, GNSS devices help locate individuals quickly, allowing first responders to reach them faster. Navigation systems can be exploited for criminal activities, including signal jamming and spoofing, both of which can disrupt crucial services and threaten public safety. By offering a verified stream of GNSS data, this new service mitigates these risks, ensuring that users can trust the accuracy and reliability of their location and timing data. \"We are so excited to be launching in Ireland, Europe\u2019s first ever Verified GNSS/GPS Data Stream service,\" said David Fleming, NSAI Technical Manager for Time & Frequency. \"As Ireland\u2019s digital economy continues to grow and more services and public safety aspects are dependent on GNSS/GPS data, the importance of verifiable GNSS/GPS data in Ireland is paramount which made the establishment of this service a key priority for the NSAI NML,\" Mr Fleming said. The new service is being developed with specialist partner Timing Solutions, a NovaUCD and ESA BIC Ireland client company. \"The Timing Solutions team is delighted to be taking the lead in establishing Ireland as one of the leading countries around the world with respect to the provision of safe GNSS/GPS data and signals thereby improving the resiliency of the critical infrastructure sectors,\" said Dr Zdenek Chaloupka, Founder, Timing Solutions.",
    "comments": [],
    "description": "The National Standards Authority of Ireland's National Metrology Laboratory has announced the launch of Europe's first ever verified GPS/GNSS Data Stream service, an extension to the National Timing Grid of Ireland.",
    "document_uid": "4a574dbea6",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979917",
    "title": "Brian Chesky, CEO of Airbnb, Talks about the Gospel of Steve Jobs",
    "url": "https://www.theverge.com/24279570/airbnb-ceo-brian-chesky-founder-mode-apple-steve-jobs-management-decoder-podcast-2024",
    "score": 1,
    "timestamp": "2024-10-29T07:09:32",
    "source": "Hacker News",
    "content": "Today, I\u2019m talking with Airbnb cofounder and CEO Brian Chesky, who is only the second person to be on Decoder three times \u2014 the other is Meta CEO Mark Zuckerberg. It\u2019s rare company, and what made this one particularly good is that Brian and I were together in our New York studio for the first time; it\u2019s pretty easy to hear how much looser and more fun the conversation was because we were in the same room.Brian made a lot of waves earlier this year when he started talking about something called \u201cfounder mode\u201d \u2014 or at least, when well-known investor Paul Graham wrote a blog post about Brian\u2019s approach to running Airbnb that gave it that name. Founder mode has since become a little bit of a meme, and I was excited to have Brian back on to talk about it and what specifically he thinks it means.Listen to Decoder, a show hosted by The Verge\u2019s Nilay Patel about big ideas \u2014 and other problems. Subscribe here!One of the reasons I love talking to Brian is because he spends so much time specifically obsessing over company structure and decision-making \u2014 if you listened to his previous Decoder episodes, you already had a preview of founder mode because Brian radically restructured the company after the covid-19 pandemic to get away from its previous divisional structure and transition into a more functional organization that works from a single roadmap. That allows him to have input on many more decisions. That\u2019s really what he\u2019s been talking about \u2014 that good leaders need to get in the details. You\u2019ll hear him express some disappointment in the idea that founder mode is about micromanagement or pure swagger \u2014 and of course, we went back and forth on how much good leaders delegate and trust their teams to make decisions independently of them.If you\u2019re a Decoder fan, this is the good stuff. Brian and I really got into it here: you\u2019ll hear Brian talk about a wide range of management styles and explain why he still considers himself a student of Steve Jobs. Actually, Steve Jobs comes up a lot in this one \u2014 as does Jony Ive, whose new company, LoveFrom, does design work for Airbnb. Don\u2019t worry, I asked about that, too.On top of all that, we actually started the show by talking about some big Airbnb news \u2014 the company just launched something called the Co-Host Network \u2014 which is a directory of experienced Airbnb hosts that can run listings for people who just want to make a little extra money renting out their homes without all the hassle.It\u2019s a big change, effectively creating a new job description for Airbnb\u2019s platform. It also gave me a great opportunity to ask Brian about some of the thorny issues surrounding his company\u2019s approach to running a platform that has many of the same issues as any other platform like YouTube or TikTok but which deals in literal, physical housing. All of that in an hour, plus more \u2014 there\u2019s even an assessment of Tim Cook\u2019s management of Apple in here. Talking to Brian is a ride, but I think I held my own, and I think you\u2019ll really like this one.Okay, Airbnb CEO Brian Chesky. Here we go.This transcript has been lightly edited for length and clarity. Brian Chesky, you\u2019re the co-founder and CEO of Airbnb. Welcome back to Decoder.Thank you for having me again.You\u2019re only our second third-time guest. The other one is Mark Zuckerberg.Oh, wow. This is really good company.And you are in the studio with me. If people are listening, we\u2019re together, which is amazing.It\u2019s the first time I\u2019ve been in the studio with you, so thank you for having me here.Actually, the last time you were on it was such a good conversation. We were both in New York, and my brain reinterpreted it as we were together because it was a good conversation.That\u2019s a sign of a good conversation.We were remote in the same city.But you\u2019re here today, and it\u2019s great. There\u2019s a lot to talk about. Airbnb just had its Winter Release. There\u2019s a bunch of features we should talk about. I\u2019m actually very curious about how you were thinking about hosting, professionalizing hosting, and what that means for the platform.If you\u2019re a Decoder listener, you\u2019ve got to know that I\u2019m going to talk to Brian about \u201cFounder Mode.\u201d The Venn diagram with Decoder and \u201cFounder Mode\u201d is the thesis of our show \u2014 those ideas are basically a circle. Then, I just want to talk about Airbnb generally. You\u2019ve made some org chart changes of your own. But let\u2019s start with the news: the Winter Release. The last time you were on, you actually talked about staggering Airbnb\u2019s releases on one timeline for the whole company, so you get the summer release and then the winter release. What\u2019s the big news in the Winter Release?So two things. The first thing is we\u2019re introducing something that we call the Co-Host Network. What is this? Let\u2019s give some background. Airbnb is only as good as the number of homes we have, and the more homes we have, the more modulated the prices are on Airbnb, so we need to get a lot more homes. Frankly, we have more than 8 million homes today, but we\u2019d love millions more in addition to the organic homes coming to Airbnb.So we went out to a lot of prospective hosts, and we asked them \u2014 we do this periodically \u2014 \u201cWhy aren\u2019t you hosting?\u201d Number one, people first say they had no idea the amount of money they can make. It\u2019s a very compelling amount of money you can make; you can make tens of thousands of dollars with an asset you\u2019re already paying for. So we asked, \u201cWell, why aren\u2019t you hosting?\u201d The number one answer people gave us was that they perceived it as being too much work. It\u2019s like, \u201cWell, I have a job, I have kids, I have this,",
    "comments": [],
    "description": "The Airbnb cofounder on why it\u2019s important to study Steve Jobs. ",
    "document_uid": "edbd02cc45",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979906",
    "title": "The Battery Revolution Is Finally Here",
    "url": "https://insideevs.com/features/738508/high-silicon-anode-solid-state/",
    "score": 1,
    "timestamp": "2024-10-29T07:07:54",
    "source": "Hacker News",
    "content": "Electric vehicles are advancing at an incredible pace, but we\u2019re very much still in the early days. The Tesla Model S and X still use battery cell formats never imagined for use in cars. A lot is about to happen in the world of automotive batteries, though. We\u2019re entering a new chapter of electrification, with new, ground-breaking technology already hitting the market. Most people just haven't seen it yet. The push toward the next generation of batteries has two schools of thought: advance current technology to new heights, or change gears completely into a new type of battery cell. Factorial, QuantumScape and Group14 are all companies with a horse in this race, and each has ideas about achieving the state of the art. All of them have partnerships with major automakers, which means what they\u2019re doing today may have a big impact on the cars you\u2019ll buy soon. What's Coming Next Factorial and QuantumScape are developing solid-state cells. It\u2019s still an emerging technology, and several companies beyond Factorial and QS have different perspectives on how they should work. The key attribute of all these batteries is solidifying the traditionally liquid electrolyte. A solid electrolyte doesn\u2019t just enable advantages in a vacuum, though. It\u2019s all about how you can change other parts of the battery as a result of solidification\u2014mainly the anode. A better anode is key to unlocking the energy density, cost, and weight advantages of SSBs. A demo image of a solid-state battery. The anode, part of the negative electrode, is one of the primary components of lithium-based battery cells, along with the cathode (part of the positive electrode), the separator, and the electrolyte. Currently, almost all battery anodes are made of graphite; the first anode material ever used successfully in lithium-ion batteries. Graphite is near its performance peak, though, and finding a replacement is key to enhancing battery energy density both in terms of mass and volume. Lithium is the best theoretical material for an anode because of its low weight and high energy capacity. Researchers have tried to use a lithium metal anode in conventional liquid electrolyte cells before. The experiments never left the lab. As the research paper Lithium Metal Anode: Past, present, and Future describes, crystals called dendrites formed on the anodes of these experimental cells after just a few cycles. As with all conventional cells today, the electrolyte was a fluid, and the separator was (and still is) effectively just an extremely thin piece of plastic. Neither could do much to prevent dendrite formation. As the cell charged, these crystals penetrated the battery's separator and shorted it out. The sort of funny part about this is that the cells were great for the few cycles they actually worked. The battery contained more energy thanks to the lithium metal anode, but because of the dendrite growth, this extra energy was just fuel on the fire when the cell inevitably shorted out, caught fire and exploded. So these researchers weren\u2019t really making lithium batteries in a lab. They were making little explosives that just happened to store a lot of energy. After a lot more experimentation, the industry determined that solid electrolytes were probably the best way to prevent making rechargeable bombs. In other words, SSBs were more or less conceptualized and developed as a result of chasing an ideal anode. Most solid-state cells in development today from Factorial, QuantumScape and others like Solid Power, have lithium metal anodes. Factorial\u2019s founder Siyu Huang is a chemist who has been developing SSBs for several years. She said when one of her Boston-based company\u2019s early cells achieved 25 cycles, that was a huge step. \u201c25 cycles for a 100 amp-hour cell,\u201d she recalled, \u201cEveryone was so excited.\u201d Today, her company\u2019s batteries\u2014of which thousands have been shipped to eager partners like Stellantis and Mercedes-Benz\u2014are achieving over 600 cycles. Factorial's experimental solid-state battery. Huang was open about her cells\u2019 capabilities, which isn\u2019t always true for SSB companies. She claims Factorial\u2019s automotive units achieve discharge rates between 4C and 10C, which is comparable to the conventional cells going into today\u2019s electric cars. They operate in a slightly wider voltage range than regular lithium-ion cells and can charge from 20-80% in under 15 minutes. That\u2019s also similar to today\u2019s best batteries but with one huge advantage: Factorial\u2019s solid-state packs are 40% lighter and 33% smaller than comparable batteries. Huang is excited about their potential but understands the skeptics who say the technology is still years away. \u201cThe reason solid state hasn't been [industrialized yet] is, first of all, the approach,\u201d she told InsideEVs. \u201cThere are so many different types of solid state.\u201d Indeed, Factorial has several competitors. California-based QuantumScape is one of them. Like Factorial, it has shipped early cells to customers and is in the process of industrializing its batteries. Its CEO, Siva Sivaram, says its \u201csecret sauce\u201d is its unique ceramic separator. It functions like a conventional separator but also effectively replaces the electrolyte. It\u2019s likewise non-combustible, unlike conventional lithium-ion separators. QS\u2019s cells are \u201canode-less,\" meaning lithium metal flows from the cathode to a current collector on the other side of the separator every time the cell cycles. This transient lithium is effectively the anode, as compared to a conventional cell where the anode material remains on one side of the separator fixed to a current collector. QS\u2019s batteries physically expand and contract as a result of this, but the company\u2019s unique cell design means the unit\u2019s outer dimensions remain relatively constant. A QuantumScape battery cell. \u201cWe have the equivalent of the Coca-Cola formula. The ceramic separator is unique,\u201d Siva said. \u201cPower and energy, weight and volume-wise, these solid-state batteries with no anode, as created, are the best.\u201d The company has yet to release a detailed spec sheet for its cells, but it has shared some preliminary data to indicate its innovations are meaningful. Both Factorial and QS are backed by major automakers and have garnered hundreds of millions in investment. Not everyone thinks lithium-ion is dead and buried, though.",
    "comments": [],
    "description": "Solid-state batteries will arrive sooner than you think, but new life is also breathed into regular liquid electrolyte cells.",
    "document_uid": "8158edd793",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979897",
    "title": "One Dollar SaaS Feedback",
    "url": "https://twocents.site/",
    "score": 1,
    "timestamp": "2024-10-29T07:05:42",
    "source": "Hacker News",
    "content": "One Dollar SaaS Feedback",
    "comments": [],
    "description": "Two Cents helps users engage with their audience and gain valuable feedback while advertising their service. Ideal for B2B and B2C digital agencies, freelancers, and perfect for A/B testing.",
    "document_uid": "6f2c747d7a",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979887",
    "title": "ReSwapper \u2013 Reproduce the Implementation of Inswapper",
    "url": "https://github.com/somanchiu/ReSwapper",
    "score": 1,
    "timestamp": "2024-10-29T07:03:32",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [
      {
        "author": "somanchiu",
        "text": "I&#x27;m attempting to reproduce the inswapper implementation. I&#x27;m glad to hear your feedback.",
        "time": "2024-10-29T07:15:38"
      }
    ],
    "description": "ReSwapper aims to reproduce the implementation of inswapper. This repository provides code for training, inference, and includes pretrained weights. - somanchiu/ReSwapper",
    "document_uid": "d4912746c2",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979880",
    "title": "Robert Downey Jr. Refuses to Let Hollywood Create His AI Digital Replica",
    "url": "https://variety.com/2024/film/news/robert-downey-jr-bands-hollywood-digital-replace-lawsuit-1236192374/",
    "score": 4,
    "timestamp": "2024-10-29T07:02:53",
    "source": "Hacker News",
    "content": "Robert Downey Jr. appeared on a recent episode of the \u201cOn With Kara Swisher\u201d podcast and sent a stern warning to Hollywood in the age of AI: \u201cI intend to sue all future executives\u201d who sign off on the creation of a Downey digital replica. The Oscar winner does not want his likeness being used on screen through AI technology and/or deepfakes. The topic came up in relation to Downey\u2019s Marvel tenure as Iron Man, but he\u2019s confident Marvel would not recreate his Tony Stark through AI. \u201cThere\u2019s two tracks. How do I feel about everything that\u2019s going on? I feel about it minimally because I have an actual emotional life that\u2019s occurring that doesn\u2019t have a lot of room for that,\u201d Downey said when asked about being digitally recreated in the future. \u201cTo go back to the MCU, I am not worried about them hijacking my character\u2019s soul because there\u2019s like three or four guys and gals who make all the decisions there anyway and they would never do that to me, with or without me,\u201d he added. When host Kara Swisher said that \u201cfuture executives certainly will\u201d want to digitally recreate Downey on the big screen, the actor responded: \u201cWell, you\u2019re right. I would like to here state that I intend to sue all future executives just on spec.\u201d \u201cYou\u2019ll be dead,\u201d Swisher noted, to which Downey replied: \u201cBut my law firm will still be very active.\u201d Downey is currently confronting the future of AI on Broadway in the play \u201cMcNeal,\u201d which takes aim at corporate giants in the AI space such as OpenAI CEO Sam Altman. \u201cI don\u2019t envy anyone who has been over-identified with the advent of this new phase of the information age. The idea that somehow it belongs to them because they have these super huge start-ups is a fallacy,\u201d Downey told Swisher about figures like Altman. \u201cThe problem is when these individuals believe that they are the arbiters of managing this but meanwhile are wanting and/or needing to be seen in a favorable light. That is a massive fucking error. It turns me off and makes me not want to engage with them because they are not being truthful.\u201d Downey is currently gearing up to return to Marvel, but he\u2019s doing as a human being and not a digital replica of Tony Stark. He\u2019s actually not playing Tony Stark/Iron Man at all and is instead taking on the role of the villainous Doctor Doom starting in 2026\u2019s \u201cAvengers: Doomsday.\u201d Listen to Downey\u2019s full interview on the \u201cOn With Kara Swisher\u201d podcast here.",
    "comments": [
      {
        "author": "WheelsAtLarge",
        "text": "I think it&#x27;s only a matter of time before we start seeing computer-created actors who will be superstars. This reminds me of the situation when the Hollywood studios refused to license Netflix some of their movies and shows. Netflix fixed the situation by creating its own shows and side-stepping the studios. In time a startup that has nothing to lose will create its own character that it will use as it likes in shows and movies not bothering with any real-life celebrity actors. It&#x27;s sad but it&#x27;s coming.",
        "time": "2024-10-29T07:16:35"
      }
    ],
    "description": "Robert Downey Jr. plans to sue any Hollywood executive who signs off on the creation of his digital replica.",
    "document_uid": "3523c6a466",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979877",
    "title": "Ask HN: What can I do with the karma I've accumulated in my Hacker News account?",
    "url": "https://news.ycombinator.com/item?id=41979877",
    "score": 4,
    "timestamp": "2024-10-29T07:02:39",
    "source": "Hacker News",
    "content": "Ask HN: What can I do with the karma I've accumulated in my Hacker News account?",
    "comments": [
      {
        "author": "gregjor",
        "text": "You can put them on The Blockchain and the write smart contracts to trade your HN karma tokens.",
        "time": "2024-10-29T07:19:42"
      },
      {
        "author": "hello_computer",
        "text": "burn it by writing something true, relevant, and&#x2F;or useful",
        "time": "2024-10-29T07:09:00"
      }
    ],
    "description": "No description available.",
    "document_uid": "44e40289b0",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979787",
    "title": "Researcher finds lost city in Mexico jungle by accident",
    "url": "https://www.bbc.com/news/articles/crmznzkly3go",
    "score": 1,
    "timestamp": "2024-10-29T06:50:52",
    "source": "Hacker News",
    "content": "Researcher finds lost city in Mexico jungle by accidentGetty ImagesThere are no pictures of the city but it had pyramid temples similar to this one in nearby CalakmulA huge Maya city has been discovered centuries after it disappeared under jungle canopy in Mexico.Archaeologists found pyramids, sports fields, causeways connecting districts and amphitheatres in the southeastern state of Campeche.They found the hidden complex - which they have called Valeriana - using Lidar, a type of laser survey that maps structures buried under vegetation.They believe it is second in density only to Calakmul, thought to be the largest Maya site in ancient Latin America.The team discovered three sites in total, which are the size of Scotland's capital Edinburgh, \u201cby accident\u201d when one archaeologist browsed data on the internet.\u201cI was on something like page 16 of Google search and found a laser survey done by a Mexican organisation for environmental monitoring,\u201d explains Luke Auld-Thomas, a PhD student at Tulane university in the US.It was a Lidar survey, a remote sensing technique which fires thousands of laser pulses from a plane and maps objects below using the time the signal takes to return.But when Mr Auld-Thomas processed the data with methods used by archaeologists, he saw what others had missed - a huge ancient city which may have been home to 30-50,000 people at its peak from 750 to 850 AD.That is more than the number of people who live in the region today, the researchers say.Mr Auld-Thomas and his colleagues named the city Valeriana after a nearby lagoon.The find helps change an idea in Western thinking that the Tropics was where \u201ccivilisations went to die\u201d, says Professor Marcello Canuto, a co-author in the research. Instead, this part of the world was home to rich and complex cultures, he explains.We can\u2019t be sure what led to the demise and eventual abandonment of the city, but the archaeologists say climate change was a major factor.The ruins were found in eastern Mexico, in CampecheValeriana has the \u201challmarks of a capital city\u201d and was second only in density of buildings to the spectacular Calakmul site, around 100km away (62 miles).It is \u201chidden in plain sight\u201d, the archaeologists say, as it is just 15 minutes hike from a major road near Xpujil where mostly Maya people now live.There are no known pictures of the lost city because \u201cno-one has ever been there\u201d, the researchers say, although local people may have suspected there were ruins under the mounds of earth.The city, which was about 16.6 sq km, had two major centres with large buildings around 2km (1.2 miles) apart, linked by dense houses and causeways. It has two plazas with temple pyramids, where Maya people would have worshipped, hidden treasures like jade masks and buried their dead.It also had a court where people would have played an ancient ball game.There was also evidence of a reservoir, indicating that people used the landscape to support a large population.In total, Mr Auld-Thomas and Prof Canuto surveyed three different sites in the jungle. They found 6,764 buildings of various sizes.Professor Elizabeth Graham from University College London, who was not involved in the research, says it supports claims that Maya lived in complex cities or towns, not in isolated villages.\"The point is that the landscape is definitely settled - that is, settled in the past - and not, as it appears to the naked eye, uninhabited or \u2018wild\u2019,\" she says.The research suggests that when Maya civilisations collapsed from 800AD onwards, it was partly because they were so densely populated and could not survive climate problems.\"It's suggesting that the landscape was just completely full of people at the onset of drought conditions and it didn't have a lot of flexibility left. And so maybe the entire system basically unravelled as people moved farther away,\" says Mr Auld-Thomas.Warfare and the conquest of the region by Spanish invaders in the 16th century also contributed to eradication of Maya city states.Getty ImagesEvidence of the ruins were found by a plane using radar to map beneath the jungle canopyMany more cities could be foundLidar technology has revolutionised how archaeologists survey areas covered in vegetation, like the Tropics, opening up a world of lost civilisations, explains Prof Canuto.In the early years of his career, surveys were done by foot and hand, using simple instruments to check the ground inch by inch.But in the decade since Lidar was used in the Mesoamerican region, he says it\u2019s mapped around 10 times the area that archaeologists managed in about a century of work.Mr Auld-Thomas says his work suggests there are many sites out there that archaeologists have no idea about.In fact so many sites have been found that researchers cannot hope to excavate them all.\"I've got to go to Valeriana at some point. It's so close to the road, how could you not? But I can't say we will do a project there,\" says Mr Auld-Thomas. \"One of the downsides of discovering lots of new Maya cities in the era of Lidar is that there are more of them than we can ever hope to study,\" he adds.The research is published in the academic journal Antiquity.",
    "comments": [],
    "description": "The city is the size of Edinburgh and among the largest Mayan sites in ancient Latin America.",
    "document_uid": "e09d9cf726",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41979779",
    "title": "Ask HN: What happens if you die, but your SaaS still run?",
    "url": "https://news.ycombinator.com/item?id=41979779",
    "score": 1,
    "timestamp": "2024-10-29T06:50:05",
    "source": "Hacker News",
    "content": "Ask HN: What happens if you die, but your SaaS still run?",
    "comments": [
      {
        "author": "WheelsAtLarge",
        "text": "I can see a situation where the bills are auto-paid from the same account that receives the subscription income. In theory, it can run forever if it needs no maintenance and it&#x27;s a SaaS that is a basic need but it&#x27;s a pain to achieve on your local computer. I can&#x27;t think of a case but it&#x27;s possible. There are still websites from the 90s that are running today that look and function as they did then.",
        "time": "2024-10-29T07:34:53"
      }
    ],
    "description": "No description available.",
    "document_uid": "18a267d5a4",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41983217",
    "title": "Internet.nl's Website Connection Checker",
    "url": "https://internet.nl/",
    "score": 1,
    "timestamp": "2024-10-29T13:32:51",
    "source": "Hacker News",
    "content": "//matomo.internet.nl/ 1 *.internet.nl Home Modern Internet Standards provide for more reliability and further growth of the Internet. Are you using them? Test your website Modern address? Signed domain? Secure connection? Route authorisation? About the test Test your email Modern address? Anti-phishing? Secure transport? Route authorisation? About the test Test your connection Modern addresses reachable? Domain signatures validated? About the test",
    "comments": [],
    "description": "Test for modern Internet Standards IPv6, DNSSEC, HTTPS, HSTS, DMARC, DKIM, SPF, STARTTLS, DANE, RPKI and security.txt",
    "document_uid": "5d02412e26",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983208",
    "title": "Moving Mental Mountains \u2013 By Kent Beck",
    "url": "https://tidyfirst.substack.com/p/moving-mental-mountains",
    "score": 1,
    "timestamp": "2024-10-29T13:32:07",
    "source": "Hacker News",
    "content": "When I was developing a piece of rural property, I was in awe of the guy who ran the \u201cescavator\u201d. Not excavator. Escavator. But he\u2019d been running one for 30 years so I wasn\u2019t about to \u201ccorrect\u201d his pronunciation. Also he cooked meth on the side & he was scary when he was high. But that\u2019s not my point.My point is that he looked at terrain completely differently from how I looked at terrain. I\u2019d see a hill & think, \u201cWell, I\u2019ve got to go around that.\u201d He\u2019d look at a hill & think of all the different places he could put it, all the shapes he could leave it in.Warning: In what follows my main point is about digging into what I call background work more frequently because it\u2019s become so much cheaper & easier. That seems to get lost in the example I chose\u2014AI potentially replacing programmers\u2014& the means I chose for analysis\u2014namely Claude. Neither AI vis a vis programmer employment nor LLM hallucination invalidate my main point\u2014be more curious, it\u2019s cheaper now.When I had the impulse to use Thinkie Reverse Causality on the relationship between employee count & revenue, the subsequent analysis took most of a day. I was digging data out of SEC filings, figuring out how to render the data, iterating on the analysis. All this is hard enough work that I hesitate to undertake it more than once a month. I think, \u201cI wonder\u2026\u201d, grimace \ud83d\ude2c, & move along. Those \u201cI wonder\u201d moments are what I need to change. Analysis just got cheaper.I posted the following on LinkedIn the other day.60K impressions, 480 likes, 80 comments & here\u2019s the thing\u2014nobody did the assignment. Nobody drafted a before & after income statement & balance sheet. This is the moment I\u2019m trying to call out. Because it\u2019s easy now to start to do the assignment.I asked Claude the same question. A minute later I got this. Of course the analysis is shallow & flawed. Not the point! What I have here is good enough to decide whether I want to dig deeper.That same moment, that \u201cgotta go around this hill\u201d moment, has changed. We can now decide to routinely dig into analysis that would have been too expensive to even start previously.50% reduction in software engineering headcountMaintained productivity through AI augmentationUsing average tech company compensation metrics from 2023Analysis based on public financial data from major software companies(All figures in millions USD)Before AI ImplementationAfter AI Implementation(Key changes only)AssetsIncrease in Cash (+$150M annually from improved operating income)New Intangible Assets: AI software licenses & implementation costsPotential reduction in office space & equipmentOne-time increase in restructuring reserves for severanceLiabilitiesNew technology lease obligations or licensing payablesOne-time increase in severance liabilitiesReduced ongoing compensation liabilitiesMicrosoft (2023)Laid off 10,000 employees (~5% of workforce)Reported $1.2B in severance costsMaintained revenue growth while reducing headcountInvested heavily in AI infrastructureMeta (2022-2023)Cut 21,000 jobsSaved $1B+ in annual costsMaintained user growth and ad revenueIncreased AI investment by $4BIBM (2020-2023)Ongoing restructuring with AI focus~$1.5B average annual savings from automationReinvested savings into AI/cloud capabilitiesMaintained service delivery with reduced headcount20 years of the internet have trained us to respond purely emotionally. Look at the comments on my original post.Business is about maximizing profits..however that can be achieved..Generating tonnes of code via GenAI, claim tax relief as R&D, huge balance sheet assets, inflated valuation, sell to clueless investors, profit \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0But the code is unmanageable and riddled with 0-day vulnerabilities.The best trick that engineering has pulled off to date is to convince marketing that what amounts to a glorified guessing game - one that consumes energy like it's mining cryptocurrency - is, in fact, a manifestation of intelligence.Again! Nobody did the assignment! We have gotten into the habit of responding in emotional burps instead of doing the work. I\u2019m noting, not blaming. The incentives encourage burping\u2014burping is less effort with more upside if you spark outrage.If you\u2019re going to do this kind of analysis, be responsible.Make progressive investments. Spend the first 5 minutes finding out if the next hour is going to be worth it. Spend the hour finding out if a day will be worth it.Double check everything. Make sure numbers add up. I fed Claude\u2019s analysis to Perplexity & got good pointers for where to follow up.Publish your results. Write up what you find. Whether people are interested or not, writing will accelerate your learning.But for goodness sake, dig in! It\u2019s cheaper now, more fun, & potentially accelerates our collective learning. We no longer have to steer around \u201cI wonder\u201ds. We can plow right through them.When I said a while back \u201cthe economic value of 90% of my skills just went to $0 but the value of the other 10% just increased by 1000X\u201d, people reasonably asked, \u201cWhat\u2019s the 10%\u201d & I reasonably responded, \u201cHow would I know?\u201d The above is an example of that 10%\u2014asking good \u201cI wonder\u201d questions & then following up.",
    "comments": [],
    "description": "When I was developing a piece of rural property, I was in awe of the guy who ran the \u201cescavator\u201d.",
    "document_uid": "0f55c8f9d5",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983206",
    "title": "Show HN: I made a tool that turns LinkedIn problems into leads",
    "url": "https://linkedrecs.com/",
    "score": 1,
    "timestamp": "2024-10-29T13:31:48",
    "source": "Hacker News",
    "content": "Turn LinkedIn messages into your top sales funnel Our AI analyzes LinkedIn activity to identify potential leads, then engages them with personalized messages recommending your product. Transform your presence into an automated lead generation system. Get Customers From LinkedIn Get customers on autopilot Save hours on manual communication High-converting personalized recommendations",
    "comments": [],
    "description": "Turn LinkedIn messages into your top sales funnel. Our AI analyzes LinkedIn activity to identify potential leads, then engages them with personalized messages recommending your product. Transform your presence into an automated lead generation system.",
    "document_uid": "c1e854bf8b",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983204",
    "title": "Metal Spinning Tops vs. the Hydraulic Press [video]",
    "url": "https://www.youtube.com/watch?v=G54ZB3i-EZY",
    "score": 1,
    "timestamp": "2024-10-29T13:31:33",
    "source": "Hacker News",
    "content": "Metal Spinning Tops vs. the Hydraulic Press [video]",
    "comments": [],
    "description": "Use my link https://foreverspin.com/?hydraulic/ and code hydraulic to get a free titanium stand for your spinning top!What is the strongest material for spin...",
    "document_uid": "2a53d15402",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983200",
    "title": "You Must Try, and then You Must Ask",
    "url": "https://www.mattringel.com/2013/09/30/you-must-try-and-then-you-must-ask/",
    "score": 1,
    "timestamp": "2024-10-29T13:31:03",
    "source": "Hacker News",
    "content": "September 30, 2013 \u00b7 13:00 I like working with grownups. Here\u2019s an example: When I was a wee little New Hire at my current employer, one of the things that came up a lot was the \u201c15 minute rule.\u201d That is, if you\u2019re stuck on a problem, take a solid 15 minutes to bash your brain against it in whatever manner you see fit. However, if you still don\u2019t have an answer after 15 minutes, you must ask someone. I shorten this down to \u201cYou must try, and then you must ask.\u201d It\u2019s a simply-worded rule, which works something like this: If you\u2019ve hit the point of giving up, you have to push yourself for another 15 minutes. The pressure is now off. You know that in 15 minutes, you\u2019ll be able to take what you found and talk to another person about it and get their help. For right now, all you have to do is step back and look at the whole problem from the top. Maybe you\u2019ll find the solution that was sitting there all along. Maybe you\u2019ll convince yourself it\u2019s completely unsolvable. Whatever you end up doing, those next 15 minutes are where you look at the problem one more time. During those 15 minutes, you must document everything you\u2019re doing so that you can tell someone else. So, what does \u201clook at the problem one more time\u201d mean? It means taking notes. Lots of them. I\u2019m a big fan of using a paper notebook with an excruciatingly fine-point pen, because I don\u2019t need to move windows out of the way to keep writing in it, and I can fit a lot of words on a single page. Use what you like, but keep writing. Write down all the steps, all the assumptions, everything you tried, and anything you can do to reproduce the problem. More likely than not, you\u2019ve now probably figured out at least one other way to solve the problem, just by getting it out of your head and onto paper. After that, you must ask someone for help. Okay, you\u2019ve decided you need help, and you\u2019ve spent another 15 minutes looking at the problem again (and again (and again)), and you\u2019ve documented your approach. Now, stop. Stop trying to solve the problem, if only for a moment. Call for help. Even if you think that you almost have it, stop. Even if you think that an incarnation of the wisdom of the masters is perched on your shoulder whispering the answer in your ear, stop. Write that email or walk over to the office/cube/etc. or cast the appropriate summoning spell, but make sure that someone else knows that you need help. Request assistance, state the problem, and show your work. You may not get help right away, but now you\u2019ve employed at least one other brain in helping you, and now they have a great head-start, courtesy of you. So, that\u2019s the 15 Minute Rule in 3 easy steps. Here\u2019s why it\u2019s important: Your paid hours are costing someone money. You can be in a Professional Services Organization, an internal IT organization, or an independent contractor, but it all works out to the same thing; someone is paying for your skills. While it may feel good to figure out the answer on your own, there\u2019s no medal for wasting 3 hours worth of money on a problem that doesn\u2019t merit that kind of time. In a sneaky way, this also helps you value your own time, if only by making you ask yourself \u201cIs this problem worth this much of my time?\u201d Your colleagues will help you because they\u2019re playing by the same rules. This means they\u2019re used to asking and listening to informed questions, and they\u2019ll be expecting the same from their peers. Needless to say, use your common sense\u2026 find someone that isn\u2019t heads-down in a problem of their own; no one likes to have their flow interrupted. That being said, your colleagues will know that if you come over to ask for help, you\u2019ll already have taken time to look it over and documented your findings so they can help you figure out the problem faster or point you in the right direction. It\u2019s possible you\u2019ll end up Rubber Duck Debugging the problem, and the act of talking through the problem will help you solve it. Last but certainly not least, You have to interact with your colleagues because they have the answers you need. Building and maintaining an enterprise software platform (to choose something of appropriately fiendish complexity) is not a solo sport. Your colleagues have different ways of understanding problems and different ways of using the knowledge they have. This goes for many definitions of colleague and many definitions of knowledge. This eventually turns into a virtuous cycle. People value each others\u2019 time and their own, so they do their own homework before asking a question. In turn, people are more likely to answer questions because they know the person asking will give them the interesting part of the problem to solve. Put another way: by explicitly taking enough time, everyone saves time.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "dfda474c05",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983196",
    "title": "Students with 'impossible' proof of Pythagorean Theorem discover more solutions",
    "url": "https://www.livescience.com/physics-mathematics/mathematics/high-school-students-who-came-up-with-impossible-proof-of-pythagorean-theorem-discover-9-more-solutions-to-the-problem",
    "score": 1,
    "timestamp": "2024-10-29T13:29:38",
    "source": "Hacker News",
    "content": "Two students who discovered a seemingly impossible proof to the Pythagorean theorem in 2022 have wowed the math community again with nine completely new solutions to the problem.While still in high school, Ne'Kiya Jackson and Calcea Johnson from Louisiana used trigonometry to prove the 2,000-year-old Pythagorean theorem, which states that the sum of the squares of a right triangle's two shorter sides are equal to the square of the triangle's longest side (the hypotenuse). Mathematicians had long thought that using trigonometry to prove the theorem was unworkable, given that the fundamental formulas for trigonometry are based on the assumption that the theorem is true.Jackson and Johnson came up with their \"impossible\" proof in answer to a bonus question in a school math contest. They presented their work at an American Mathematical Society meeting in 2023, but the proof hadn't been thoroughly scrutinized at that point. Now, a new paper published Monday (Oct. 28) in the journal American Mathematical Monthly shows their solution held up to peer review. Not only that, but the two students also outlined nine more proofs to the Pythagorean theorem using trigonometry.\"To have a paper published at such a young age \u2014 it's really mind-blowing,\" Johnson, who is now studying environmental engineering at Louisiana State University, said in a statement emailed to Live Science. \"I am very proud that we are both able to be such a positive influence in showing that young women and women of color can do these things.\"Related: Largest known prime number, spanning 41 million digits, discovered by amateur mathematician using free softwareBy proving Pythagoras' theorem using trigonometry, but without using the theorem itself, the two young women overcame a failure of logic known as circular reasoning. Trigonometry is a branch of mathematics that lays out how the sides, lengths and angles in a triangle are related, and as such, the discipline often includes expressions of the Pythagorean theorem. But Jackson and Johnson managed to prove the theorem using a result of trigonometry called the Law of Sines, dodging circular reasoning.In the new study, and on top of their initial proof, the young mathematicians described four new ways to prove Pythagoras' theorem using trigonometry, as well as a novel method that revealed five more proofs, totaling 10 proofs.Get the world\u2019s most fascinating discoveries delivered straight to your inbox.Jackson and Johnson are only the third and fourth people known to have proven the Pythagorean theorem using trigonometry and without resorting to circular reasoning. The two other people were professional mathematicians, according to the statement.\"I didn't think it would go this far,\" Jackson, who currently studies pharmacology at the Xavier University of Louisiana, said in the statement. \"I was pretty surprised to be published.\"In the paper, Jackson and Johnson say there are two ways to present trigonometry and its functions sine and cosine, but these versions are often conflated into one. Sine and cosine are ratios that are defined in the context of a triangle's right angle, and they can be presented according to either the trigonometric method or a method that uses polynomials of complex numbers, according to the paper.The conflation means that \"trying to make sense of trigonometry can be like trying to make sense of a picture where two different images have been printed on top of each other,\" Jackson and Johnson wrote.By teasing the two methods apart, researchers can discover \"a large collection of new proofs of the Pythagorean theorem,\" the young mathematicians added.If you liked reading this story, here are some mathematics books you might also enjoy:",
    "comments": [],
    "description": "In a new peer-reviewed study, Ne'Kiya Jackson and Calcea Johnson outlined 10 ways to solve the Pythagorean theorem using trigonometry, including a proof they discovered in high school.",
    "document_uid": "07bfb36abf",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983194",
    "title": "Microsoft Excel's bloopers reel: 40 years of spreadsheet errors",
    "url": "https://www.theguardian.com/technology/2024/oct/28/microsoft-excels-bloopers-reel-40-years-of-spreadsheet-errors",
    "score": 1,
    "timestamp": "2024-10-29T13:29:14",
    "source": "Hacker News",
    "content": "For millions of people, from accountants to the person in charge of the work rota, Microsoft Excel has been been a godsend.But as the spreadsheet software celebrates its 40th birthday, spare a thought for those who misplaced a decimal, left out a row or got their cut and paste wrong. Here are some of the most memorable examples.Austerity errorIn 2010, two respected economists released a paper that made an influential case for the years of austerity that followed. Carmen Reinhart and Kenneth Rogoff argued in Growth in a Time of Debt that once a government\u2019s debt reaches 90% of gross domestic product \u2013 a measure of a country\u2019s economic output \u2013 growth goes into reverse.However, in 2013 it emerged that the Harvard economists had made an error in an Excel spreadsheet: instead of falling by 0.1%, their work should have found that the economies in question grow by 2.2%.\u201cIt is sobering that such an error slipped into one of our papers,\u201d they said in a statement. They nonetheless stood by their broad conclusion \u2013 debt hinders growth \u2013 but the error was jumped on by critics.\u201cThis can\u2019t possibly be good for Team Austerity,\u201d wrote the US economist Paul Krugman.Utter dis-maeFannie Mae, the US mortgage lender, issued a quarterly results statement in 2003 containing accounting errors that totalled more than $1bn. Fannie Mae said the mistakes were due to a company accountant putting the wrong formula into an Excel spreadsheet, as the financial firm sought to comply with a new accounting standard.The company stressed that the errors did not affect profits, but questions were raised about Fannie Mae\u2019s internal controls at the time. Five years later, Fannie Mae was rescued by the US government as the credit crunch took hold.Computer says 000MI5, the UK\u2019s domestic spy agency, tapped 134 incorrect telephone numbers in 2010 following a spreadsheet error that altered the last three digits in the numbers to \u201c000\u201d.A report admitted the errors were caused by \u201ca formatting fault on an electronic spreadsheet\u201d, adding euphemistically that \u201ca degree of unintended collateral intrusion occurred\u201d.skip past newsletter promotionafter newsletter promotionVirus problemsDuring the coronavirus pandemic, nearly 16,000 cases went unreported in England after an Excel error. The Guardian reported that the mistake may have been caused by a file containing test results, sent from NHS test and trace to Public Health England, exceeding the number of rows it could contain. As a consequence, test reports were missed off and thousands of potentially infectious people were missed by contact tracers.The then health secretary, Matt Hancock, said the incident \u201cshould never have happened\u201d.Whale-sized mistakeAn internal report into a $6bn trading loss at the US investment bank JP Morgan, at the hands of a trader known as the London Whale, found flaws in how an important measure of financial risk at the institution was being calculated.One problem was that the risk model monitoring the fateful portfolio operated \u201cthrough a series of Excel spreadsheets, which had to be completed manually, by a process of copying and pasting data from one spreadsheet to another\u201d \u2013 a process that might ring bells for some Excel veterans.An error was created after a cell mistakenly divided by the sum of two interest rates, rather than the average, according to the report.James Kwak, a US scholar, said the report outlined one of Excel\u2019s main issues: its accessibility.\u201cThe biggest problem is that anyone can create Excel spreadsheets \u2013 badly,\u201d he wrote.",
    "comments": [],
    "description": "As the software used by millions around the world celebrates its birthday, here are some of the low points",
    "document_uid": "a12f183a95",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983166",
    "title": "ParadeDB pg_analytics is now PostgreSQL License",
    "url": "https://github.com/paradedb/pg_analytics",
    "score": 1,
    "timestamp": "2024-10-29T13:25:49",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "DuckDB-powered analytics for Postgres. Contribute to paradedb/pg_analytics development by creating an account on GitHub.",
    "document_uid": "f68851a1a3",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983165",
    "title": "Google's Shadow Campaigns",
    "url": "https://blogs.microsoft.com/on-the-issues/2024/10/28/googles-shadow-campaigns/",
    "score": 2,
    "timestamp": "2024-10-29T13:25:43",
    "source": "Hacker News",
    "content": "403 Forbidden nginx",
    "comments": [
      {
        "author": "okasaki",
        "text": "The EU should just kick US tech companies out of its territory like China did. That&#x27;s the only way they will ever develop their own industry.",
        "time": "2024-10-29T13:34:04"
      }
    ],
    "description": "No description available.",
    "document_uid": "7009f88d74",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983164",
    "title": "Without chaos theory, social science will never understand the world Essays",
    "url": "https://aeon.co/essays/without-chaos-theory-social-science-will-never-understand-the-world",
    "score": 1,
    "timestamp": "2024-10-29T13:25:41",
    "source": "Hacker News",
    "content": "The social world doesn\u2019t work how we pretend it does. Too often, we are led to believe it is a structured, ordered system defined by clear rules and patterns. The economy, apparently, runs on supply-and-demand curves. Politics is a science. Even human beliefs can be charted, plotted, graphed. And using the right regression we can tame even the most baffling elements of the human condition. Within this dominant, hubristic paradigm of social science, our world is treated as one that can be understood, controlled and bent to our whims. It can\u2019t. Our history has been an endless but futile struggle to impose order, certainty and rationality onto a Universe defined by disorder, chance and chaos. And, in the 21st century, this tendency seems to be only increasing as calamities in the social world become more unpredictable. From 9/11 to the financial crisis, the Arab Spring to the rise of populism, and from a global pandemic to devastating wars, our modern world feels more prone to disastrous \u2018shocks\u2019 than ever before. Though we\u2019ve got mountains of data and sophisticated models, we haven\u2019t gotten much better at figuring out what looms around the corner. Social science has utterly failed to anticipate these bolts from the blue. In fact, most rigorous attempts to understand the social world simply ignore its chaotic quality \u2013 writing it off as \u2018noise\u2019 \u2013 so we can cram our complex reality into neater, tidier models. But when you peer closer at the underlying nature of causality, it becomes impossible to ignore the role of flukes and chance events. Shouldn\u2019t our social models take chaos more seriously? The problem is that social scientists don\u2019t seem to know how to incorporate the nonlinearity of chaos. For how can disciplines such as psychology, sociology, economics and political science anticipate the world-changing effects of something as small as one consequential day of sightseeing or as ephemeral as passing clouds? On 30 October 1926, Henry and Mabel Stimson stepped off a steam train in Kyoto, Japan and set in motion an unbroken chain of events that, two decades later, led to the deaths of 140,000 people in a city more than 300 km away. The American couple began their short holiday in Japan\u2019s former imperial capital by walking from the railway yard to their room at the nearby Miyako Hotel. It was autumn. The maples had turned crimson, and the ginkgo trees had burst into a golden shade of yellow. Henry chronicled a \u2018beautiful day devoted to sightseeing\u2019 in his diary. Nineteen years later, he had become the Unites States Secretary of War, the chief civilian overseeing military operations in the Second World War, and would soon join a clandestine committee of soldiers and scientists tasked with deciding how to use the first atomic bomb. One Japanese city ticked several boxes: the former imperial capital. The Target Committee agreed that Kyoto must be destroyed. They drew up a tactical bombing map and decided to aim for the city\u2019s railway yard, just around the corner from the Miyako Hotel where the Stimsons had stayed in 1926. Stimson pleaded with the president Harry Truman not to bomb Kyoto. He sent cables in protest. The generals began referring to Kyoto as Stimson\u2019s \u2018pet city\u2019. Eventually, Truman acquiesced, removing Kyoto from the list of targets. On 6 August 1945, Hiroshima was bombed instead. If such random events could lead to so many deaths, how are we to predict the fates of human society? The next atomic bomb was intended for Kokura, a city at the tip of Japan\u2019s southern island of Kyushu. On the morning of 9 August, three days after Hiroshima was destroyed, six US B-29 bombers were launched, including the strike plane Bockscar. Around 10:45am, Bockscar prepared to release its payload. But, according to the flight log, the target \u2018was obscured by heavy ground haze and smoke\u2019. The crew decided not to risk accidentally dropping the atomic bomb in the wrong place. Bockscar then headed for the secondary target, Nagasaki. But it, too, was obscured. Running low on fuel, the plane prepared to return to base, but a momentary break in the clouds gave the bombardier a clear view of the city. Unbeknown to anyone below, Nagasaki was bombed due to passing clouds over Kokura. To this day, the Japanese refer to \u2018Kokura\u2019s luck\u2019 when one unknowingly escapes disaster. Roughly 200,000 people died in the attacks on Hiroshima and Nagasaki \u2013 and not Kyoto and Kokura \u2013 largely due to one couple\u2019s vacation two decades earlier and some passing clouds. But if such random events could lead to so many deaths and change the direction of a globally destructive war, how are we to understand or predict the fates of human society? Where, in the models of social change, are we supposed to chart the variables for travel itineraries and clouds? In the 1970s, the British mathematician George Box quipped that \u2018all models are wrong, but some are useful\u2019. But today, many of the models we use to describe our social world are neither right nor useful. There is a better way. And it doesn\u2019t entail a futile search for regular patterns in the maddening complexity of life. Instead, it involves learning to navigate the chaos of our social worlds. Before the scientific revolution, humans had few ways of understanding why things happened to them. \u2018Why did that storm sink our fleet?\u2019 was a question that could be answered only with reference to gods or, later, to God. Then, in the 17th century, Isaac Newton introduced a framework where such events could be explained through natural laws. With the discovery of gravity, science turned the previously mysterious workings of the physical Universe \u2013 the changing of the tides, celestial movements, falling objects \u2013 into problems that could be investigated. Newtonian physics helped push human ideas about causality from the unknowable into the merely unknown. A world ruled by gods is fundamentally unknowable to mere mortals, but, with Newton\u2019s equations, it became possible to imagine that",
    "comments": [],
    "description": "Social scientists cling to simple models of reality \u2013 with disastrous results. Instead they must embrace chaos theory",
    "document_uid": "05f9ee8312",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983157",
    "title": "JetBrains Plugin Developer Conf 2024",
    "url": "https://lp.jetbrains.com/plugin-dev-conf-2024/",
    "score": 1,
    "timestamp": "2024-10-29T13:24:44",
    "source": "Hacker News",
    "content": "This talk covers how to implement localization in JetBrains plugins. While JetBrains IDEs are available in Simplified Chinese, Japanese, and Korean, most plugins remain English-only. Joachim will demonstrate how to easily localize different plugin elements like messages, settings, inspections, and file templates. The session will also provide tips on localizing plugin descriptions for Marketplace, websites, or handbooks.",
    "comments": [],
    "description": "Join our free virtual community event focused on developing plugins for JetBrains products.",
    "document_uid": "25a5677797",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983156",
    "title": "Textcasting: Applying the Philosophy of Podcasting to Text (2022)",
    "url": "https://textcasting.org",
    "score": 1,
    "timestamp": "2024-10-29T13:24:35",
    "source": "Hacker News",
    "content": "Textcasting: Applying the Philosophy of Podcasting to Text (2022)",
    "comments": [],
    "description": "Applying the philosophy of podcasting to text.",
    "document_uid": "46157be0e5",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983144",
    "title": "WireGuard Endpoint Discovery and Nat Traversal Using DNS-SD (2020)",
    "url": "https://www.jordanwhited.com/posts/wireguard-endpoint-discovery-nat-traversal/",
    "score": 1,
    "timestamp": "2024-10-29T13:23:11",
    "source": "Hacker News",
    "content": "WireGuard WireGuard is a next generation, cross-platform VPN technology created by Jason A. Donenfeld that has quickly become a popular alternative to the beefy, complex IPSec and SSL VPN solutions used for years. As a testament to its success it has recently been merged into the Linux Kernel as of v5.6. It is also available as a kernel module or as a user space application written in Go or Rust. If you are new to WireGuard I recommend checking out the conceptual overview as this will be helpful for following along through the rest of the post. Dynamically Addressed Peers One of the genius parts of WireGuard is its concept of crypto-key routing. Crypto-key routing defines an association between a public key, and a list of IP addresses (Allowed IPs). The list of IP addresses permits (inbound) and routes (outbound) packets inside of a WireGuard tunnel. This association encompasses the total minimum configuration for a peer, there is no static peer endpoint IP address required from a tunnel validation standpoint. This allows for built-in IP roaming on both sides, assuming peer addresses don\u2019t change simultaneously OR there is a reliable means for signaling when they do. The DNS can be leveraged to support dynamically addressed peers as various WireGuard utilities will resolve DNS names when configuring a peer, and there are supporting scripts that can be used to periodically re-resolve peer addresses. Awesome! This sounds promising\u2026 however: What happens if both peers are behind a NAT that we don\u2019t control? i.e. no static port-forwarding. How can we discover not only IP addresses but ports? The existing utilities do not support this. In this post we will set out to establish a WireGuard tunnel between dynamically addressed peers that are both sitting behind a NAT. One of the primary goals for achieving this is to stick with WireGuard in its purest form, the code that now ships with the Linux Kernel. We do not want to compromise it in any fashion to achieve our goals, although we could get very creative with its user space implementation. hub-and-spoke Some of you may be thinking why not use a hub-and-spoke model? Surely we can just create tunnels from Alice and Bob to a statically addressed, NAT-free central hub. Alice and Bob can route through the hub. This is a totally valid approach and one that is widely used today. However, for our use case we are not interested for the following reasons: With many peers a hub becomes a vertical scaling bottleneck. (think IoT, connected cars, robotics, etc.) Sending all of our data through the hub may be costly. The hub may introduce considerable latency between peers. Traversing the NAT Now that we\u2019ve outlined the problem, it\u2019s time to dive in. If we are going to establish a WireGuard tunnel directly between Alice and Bob we need to be able to traverse the NATs in front of them. Since WireGuard works over UDP, UDP hole punching is our best bet for accomplishing this. UDP hole punching exploits the fact that most NATs are lenient when matching inbound packets against existing \u201cconnections\u201d. This allows us to re-use port state for punching our way back in. If Alice sends a UDP packet to a new host, Carol, and Bob has knowledge of the outbound source IP and port Alice\u2019s NAT used during translation, Bob can reach Alice by sending a UDP packet towards this IP:port pair (2.2.2.2:7777 in the illustration below). * Our hole punching example describes a full-cone NAT. There are limitations with other, less common NAT types where this method does not work. STUN So we now know how UDP hole punching works. Great, but that still leaves us with open questions. How does Alice discover her external IP:port? How does Alice communicate this to Bob? How do we make this work in the context of WireGuard? RFC5389 Session Traversal Utilities for NAT (STUN) defines a protocol to answer some of these questions. It\u2019s a lengthy RFC, so I\u2019ll do my best to summarize. It\u2019s important to note that STUN is not a drop-in solution to the problem we are trying to solve: STUN by itself is not a solution to the NAT traversal problem. Rather, STUN defines a tool that can be used inside a larger solution. The term \u201cSTUN usage\u201d is used for any solution that uses STUN as a component. \u2014 RFC5389 STUN is a client/server protocol. In the example above Alice is acting as the client and Carol is the server. Alice sends a STUN Binding request to Carol. When the Binding request passes through Alice\u2019s NAT, the source IP:port gets rewritten. Once Carol receives the Binding request, she copies the source IP:port from the layer 3 and layer 4 headers into the payload of the Binding response and sends it to Alice. The Binding response passes back through Alice\u2019s NAT at which point the destination IP:port gets rewritten, but the payload remains untouched. Alice receives the Binding response and becomes aware her external IP:port for this socket is 2.2.2.2:7777. As previously pointed out, STUN is not a complete solution. STUN provides a mechanism for an application to understand its external IP:port when behind a NAT, but STUN does not provide a method for signaling this to interested parties. If we were writing an application from the ground up that required NAT traversal capabilities, STUN is a component we should consider. We are not writing WireGuard, it already exists, and its not something we can modify (see goal about leaving its source untouched). So where does that leave us? We can certainly take some concepts from STUN and use them to achieve our goal. We clearly need an external, statically addressed host for discovering UDP holes that we can punch through. An existing PoC Back in August 2016, the creator of WireGuard, shared a NAT hole punching PoC/Example on the WireGuard mailing list. Jason\u2019s example contains a client and server application. The client is intended to be run alongside",
    "comments": [],
    "description": "In this post we will set out to establish a WireGuard tunnel between dynamically addressed peers that are both sitting behind a NAT. One of the primary goals for achieving this is to stick with WireGuard in its purest form, the code that now ships with the Linux Kernel.",
    "document_uid": "ec824f74e6",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983131",
    "title": "PGmacs \u2013 fantastic Emacs PostgreSQL client and editing interface",
    "url": "https://github.com/emarsden/pgmacs",
    "score": 1,
    "timestamp": "2024-10-29T13:21:49",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "Emacs is editing a PostgreSQL database. Contribute to emarsden/pgmacs development by creating an account on GitHub.",
    "document_uid": "2864ecf2bc",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983113",
    "title": "Automating UI Regression Testing with Argos, Storybook and GitHub Actions",
    "url": "https://argos-ci.com/blog/storybook-github-actions",
    "score": 2,
    "timestamp": "2024-10-29T13:19:11",
    "source": "Hacker News",
    "content": "TestingIntegrating Argos with Storybook and GitHub Actions: A Complete GuideVisual regression testing setup with Storybook and GitHub Actions. Visual regression testing is crucial for maintaining UI consistency. Argos automates this by comparing screenshots over time, catching visual changes early. By integrating Argos with Storybook and GitHub Actions, you can run visual regression tests directly within your CI/CD pipeline, catching issues early in development. This guide provides a step-by-step walkthrough to integrate Argos with Storybook and GitHub Actions, ensuring any visual inconsistencies are caught during development. Overview Prerequisites Before beginning, make sure you have: A GitHub repository with Storybook already installed and configured. Basic knowledge of GitHub Actions and experience with CI/CD workflows. Setting Up Argos Follow these steps to configure Argos for your project: 1. Create an Argos Account Sign up at argos-ci.com using your GitHub account. Authorize Argos to access your repositories. 2. Add Your Repository In the Argos dashboard, click on Add a repository. Select the GitHub repository you want to integrate. Follow the setup instructions on the screen to complete the configuration. 3. Copy the Argos Token Go to the settings page for your repository on Argos. Copy the ARGOS_TOKEN; you\u2019ll need it for your GitHub Actions configuration. Configuring Storybook for Visual Testing To capture screenshots, we\u2019ll use the Storybook Test Runner and Argos SDK. 1. Install Dependencies Add the necessary dependencies for the Storybook test runner and Argos SDK: npm install --save-dev @argos-ci/cli @argos-ci/storybook @storybook/test-runner 2. Update package.json Scripts In your package.json, add scripts to build Storybook, run tests, and upload screenshots: { \"scripts\": { \"build-storybook\": \"build-storybook\", \"test-storybook\": \"NODE_NO_WARNINGS=1 NODE_OPTIONS=--experimental-vm-modules test-storybook\", \"upload-screenshots\": \"npm exec argos -- upload screenshots --build-name storybook\" } } The test-storybook script uses Storybook\u2019s test runner to execute tests and capture screenshots. The upload-screenshots script uploads the screenshots to Argos for visual comparison. Note: NODE_OPTIONS=--experimental-vm-modules is required due to Storybook\u2019s reliance on Jest, which needs this flag to run modern packages like Argos Storybook SDK. 3. Add a .storybook/test-runner.ts File Create a test-runner.ts file in your .storybook directory to configure screenshot capture: // .storybook/test-runner.ts import { argosScreenshot } from \"@argos-ci/storybook\"; import type { TestRunnerConfig } from \"@storybook/test-runner\"; const config: TestRunnerConfig = { async postVisit(page, context) { await argosScreenshot(page, context); }, }; export default config; This configuration will capture screenshots of your stories in the ./screenshots directory. Be sure to add this directory to .gitignore. 4. Run Storybook Test Runner Locally Run the Storybook test runner locally to verify screenshots are generated correctly: npx test-storybook This command will capture screenshots of your stories and save them to ./screenshots. Automating with GitHub Actions Next, we\u2019ll set up GitHub Actions to automate visual regression testing. This example runs against a locally served Storybook instance. 1. Add ARGOS_TOKEN to GitHub Secrets In your GitHub repository, go to Settings > Secrets and variables > Actions. Click on New repository secret. Add a secret named ARGOS_TOKEN and paste in the token from Argos. 2. Create the GitHub Actions Workflow Add the following workflow file to .github/workflows/storybook-tests.yml: # .github/workflows/storybook-tests.yml name: \"Storybook Tests\" on: pull_request: push: branches: - main jobs: test: timeout-minutes: 60 runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-node@v4 with: node-version-file: \".nvmrc\" - name: Install dependencies run: npm install --frozen-lockfile - name: Install Playwright run: npx playwright install --with-deps - name: Build Storybook run: npm run build-storybook -- --quiet - name: Serve Storybook and Run Tests run: | npx concurrently -k -s first -n \"SB,TEST\" -c \"magenta,blue\" \\ \"npx http-server storybook-static --port 6006 --silent\" \\ \"npx wait-on tcp:127.0.0.1:6006 && npm run test-storybook && npm run upload-screenshots\" env: ARGOS_TOKEN: ${{ secrets.ARGOS_TOKEN }} Note: Storybook outputs the build to storybook-static by default. If using a different output directory, adjust accordingly. This workflow triggers on every push to main and on pull requests. It will build Storybook, serve it locally, run the tests, and upload screenshots to Argos. Conclusion By integrating Argos with Storybook and GitHub Actions, you now have a reliable automated visual regression testing setup. This process helps you catch visual inconsistencies early, ensuring UI stability and quality. For advanced configurations, check the Argos documentation.",
    "comments": [],
    "description": "Set up visual regression testing for your UI components with Argos, Storybook, and GitHub Actions. Automate detection of visual changes within your CI/CD pipeline to ensure consistent and high-quality interfaces.",
    "document_uid": "54c8c460ec",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983111",
    "title": "FCC chair: Mobile dead spots will end when space-based and ground comms merge",
    "url": "https://www.theregister.com/2024/10/29/fcc_chair_gives_a_taste/",
    "score": 1,
    "timestamp": "2024-10-29T13:18:57",
    "source": "Hacker News",
    "content": "The chair of America's Federal Communications Commission (FCC) foresees a single network future where space-based comms will be integrated with terrestrial networks to ensure connectivity anywhere. Jessica Rosenworcel was speaking as part of a discussion at the Princeton University Center for Information Technology Policy, New Jersey late last week. Predicting the future is a dangerous business, she joked, and even smart people can get it wrong \u2013 really, really wrong. As examples, she noted McKinsey & Company's forecast in the 1980s that there would only be about 900,000 cellphones globally by the turn of the millennium, and she highlighted the claim by former Intel CEO Andrew Grove, who said: \"the idea of a wireless personal communicator in every pocket is a pipe dream.\" Satellites may be in our skies, but they are the anchor tenant in our communications future... Rosenworcel's predictions are not really outlandish enough to embarrass her or the FCC, and the first isn't even a prediction, more of a principle: that communications should be available to all. \"In the past you saw this principle in our work to bring telephone service to every corner of the country. Today, you see it in initiatives to close the digital divide because we now know that access to broadband is essential \u2013 no matter who you are or where you live,\" she said. The US has rolled out programs to push broadband support to where the infrastructure is lacking, such as in rural areas, said Rosenworcel. One such scheme, however, the Affordable Connectivity Program (ACP), was terminated just a few months back due to a funding shortfall, much to the annoyance of the FCC. \"Concentrating access to digital networks, skills, and production in a single population or a select geography is not viable,\" said Rosenworcel. \"These are vital inputs in modern civic and economic life. We need them to reach all. This is about more than equity; it is about our economic and national security.\" The second point made by the FCC chair will likely not surprise anyone who has been following the progress of Starlink and other organizations putting in place satellite-based broadband and phone connectivity. \"In the not-too-distant future, we will integrate space-based communications in our terrestrial networks, including in the mobile phone you have in your palm or pocket right now,\" said Rosenworcel. \"This will make it possible for every one of us to have a back-up connection in the skies when ground-based systems are unavailable or fail in disaster. If we do this, we can end mobile dead zones,\" she added, claiming the FCC is the first regulatory authority to adopt a framework for combining these services into what it calls supplemental coverage from space (SCS). But she went further and said a single network future will need all network infrastructure \u2013 fiber, cellular, next-gen unlicensed wireless technology, and satellite broadband \u2013 seamlessly interoperating. \"To be clear, this vision does not work without satellites. The way I see it, satellites may be in our skies, but they are the anchor tenant in our communications future,\" Rosenworcel said. This will call for in-orbit maintenance, assembly, and manufacturing capabilities, and means that policies over the next ten years will need to adapt to support services and protect against the downsides, such as orbital debris. The next prediction from the FCC chair - inevitably - concerned AI. But rather than eulogize about its potential, Rosenworcel warned of the risks, including voice cloning and digital faking of people, as has already happened to President Biden. This will only worsen as AI gets more advanced, she predicted, and said that rather than restrict speech or technology, the FCC is seeking to create the norm, both legally and socially, that when AI is being used, ordinary citizens deserve to know. \"For effective democracy, we need to commit to disclosing what is synthetic and what is not. We need to make transparency front and center.\" On more familiar ground, the forth prediction is that the wireless industry will need \"new spectrum and new models\" in order to thrive. \"The challenge we face during the next decade is that we will see extraordinary increases in wireless traffic, but our airwaves are a finite resource. So what can we do to turn this spectrum scarcity into abundance?\" Rosenworcel asked. New tech will provide part of the answer, she said, mentioning developments such as cognitive radios, which can intelligently use radio channels to avoid interference and congestion, and MIMO transmitters and receivers. Another part is to identify and repurpose underutilized spectrum, often from other federal authorities, and get them into the market for flexible, licensed commercial use, she said. Again, this is something that is already happening rather than a prediction for the future. Finally, there will be a need for policies that can improve digital trust, Rosenworcel claimed. \"Right now, if you buy a television, thermostat, home security camera, or fitness tracker, the odds are it is connected to the internet,\" she stated, adding that during the next decade, these will multiply so that they could surpass a trillion connected devices, and these devices need to be secure. The FCC's answer is the Cyber Trust Mark. \"This is the first-ever voluntary cybersecurity labeling program supported by the United States. When this mark is displayed, it will mean that the device has been certified to meet cybersecurity standards.\" It is envisaged that this will follow the cybersecurity criteria developed by the National Institute of Standards and Technology. Rosenworcel said the Commission is also building it on an existing model at the FCC for authorization of devices using radio frequency. \"So we have both a framework for standards and a framework for execution.\" Rosenworcel ended by quoting management consultant Peter Drucker, who said: \"The best way to predict the future is to create it.\" Going by her predictions, it seems like the FCC has taken that one to heart. \u00ae",
    "comments": [],
    "description": "Jessica Rosenworcel looks at policy challenges for the next decade",
    "document_uid": "258f43d72b",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983110",
    "title": "Ask HN: What's Holding You Back?",
    "url": "https://news.ycombinator.com/item?id=41983110",
    "score": 1,
    "timestamp": "2024-10-29T13:18:54",
    "source": "Hacker News",
    "content": "Ask HN: What's Holding You Back?",
    "comments": [],
    "description": "No description available.",
    "document_uid": "c69ecb53ea",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983049",
    "title": "Guy just installed a payphone in his yard [video]",
    "url": "https://www.youtube.com/watch?v=8hVk9q3Q9To",
    "score": 1,
    "timestamp": "2024-10-29T13:08:49",
    "source": "Hacker News",
    "content": "Guy just installed a payphone in his yard [video]",
    "comments": [],
    "description": "Since we already know there's a PBX in the house, it makes sense to mess about with telephones in the yard. I alluded to this near the end of my bedroom reno...",
    "document_uid": "5b9c8a5415",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983024",
    "title": "Uncovering Big Tech's Hidden Network",
    "url": "https://www.somo.nl/uncovering-big-techs-hidden-network/",
    "score": 1,
    "timestamp": "2024-10-29T13:05:48",
    "source": "Hacker News",
    "content": "Undisclosed affiliations distort Digital Market Act\u2019s public workshops Posted in category: Written by: Published on: 29 October 2024 Summary In March 2024, the European Commission set up a series of public workshops to test Big Tech companies\u2019 compliance with the Digital Markets Act (DMA). These workshops were meant to gather the input from impacted business and end users. Research by CEO, LobbyControl and SOMO now shows that 1 in 5 workshop participants had a direct connection with Big Tech, most of them undisclosed. Gatekeeper companies like Alphabet, Amazon, and Meta were able to complement their own legal and lobbying teams\u2019 influence through a vast and undisclosed network of law firms, lobby consultants, trade associations and think tanks. Given the clear asymmetry of resources between Big Tech and regulators, the European Commission must strengthen its conflict of interest safeguards before its next DMA workshops. Over the past two decades, Big Tech companies have monopolised key features of the internet, making businesses and end users vulnerable to their massive and increasing power. The Digital Markets Act (DMA) is the EU\u2019s attempt to rein in Big Tech\u2019s monopoly power by limiting its worst excesses. Tech companies identified as gatekeepers \u2013 Google, Amazon, Meta, Apple, Microsoft, Bytedance, and Booking.com \u2013 have to comply with a specific set of rules to prevent anti-competitive behaviour. For example, gatekeepers are prohibited from self-preferencing (i.e. using their platform power to favour their products over competitors\u2019) or merging personal data collected across services without consent. Testing compliance To publicly test Big Tech\u2019s compliance with the DMA, the European Commission organised a series of public workshops in March 2024. The workshops received widespread attention, with nearly 4,000 registrations from businesses, law firms, civil society organisations, academics, other public regulators and industry associations. The workshops aimed to collect feedback from impacted communities, such as users, competitors and businesses dependent on their services. Yet, Corporate Europe Observatory(opens in new window) (CEO), LobbyControl(opens in new window) , and SOMO research shows that 1 in 5 registered participants were linked to gatekeepers. This included participants from 34 law firms, 22 lobby firms, 17 trade associations, 10 economic consultancies and 8 think tanks. Find the full list of registrants and their affiliations here(opens in new window) . When added to the gatekeeper\u2019s own direct employees who attended the workshops, that comes up to 26% of all participants, this was more than the combined number of business users and civil society organisations who participated. More about our methodology at the end of the article. Jump to methodology. Scollable story navigation Skip slides Transparent policymaking The workshops were a notable effort of transparent policy-making, creating a space for public confrontation between Big Tech companies and those impacted by their market power. The European Commission took the positive step of asking participants to disclose any affiliation they might have with the gatekeeper companies. However, our analysis of the participants list shows that attendees mostly ignored this question. The only participants that listed any affiliation were employees of DMA gatekeepers (although not even all of these declared their affiliation), as well as three lawyers working as external counsel or seconded to a DMA gatekeeper . Most Big Tech affiliations were not disclosed The majority of actors with known affiliations to Big Tech did not disclose them, regardless of whether they were linked to gatekeepers contractually (for instance, lobby and law firms), through funding or even if the gatekeepers were members of their business associations. European Commission officials told us the affiliation information provided by participants was used \u201cto ensure there was an appropriate representation of the different kinds of stakeholders in the physical attendance\u201d. The Commission prioritised (opens in new window) in-person attendance for the gatekeepers\u2019 team, business users, and civil society organisations. Others\u2014journalists, consultants, external lawyers, and academics or students\u2014could follow and participate online. At face value Commission officials added that due to time constraints, the information provided by attendees was mostly taken at face value, with only limited checks performed. Groups very close to Big Tech were, in fact, able to participate in person and engage in the discussions without ever disclosing their affiliation (see the boxes below). Commenting on this report, Sebastiano Toffaletti, Secretary-General of the European Digital SME Alliance, a trade association that does not receive Big Tech funding, commented that it \u201csheds light on the reality we see on the ground as SME representatives.\u201d \u201c Big Tech companies use a wide range of shady practices to influence EU decision-making. The Commission, and the other EU institutions alike, are unprepared to prevent these practices. The result is a process that is unduly influenced by those with more resources and the risk of decisions that do not serve the interests of European SMEs and legitimate stakeholders. Sebastiano Toffaletti Secretary-General of the European Digital SME Alliance Ultimately, Big Tech firms were still able to use their deep pockets and vast networks of third parties to make up a sizeable number of the attendees at these public workshops, which worked to deflect criticism and distort public debate. This data also shows once again the extremely wide gap in resources between the regulator and the companies it is charged with overseeing. Big Tech hides behind well-funded third-parties The DMA, which entered into force on 7 March 2024, has been heavily contested (opens in new window) by Big Tech. The companies have been fighting tooth and nail ever since the act was passed to avoid compliance. ByteDance(opens in new window) , Apple(opens in new window) , and Meta have all filed legal challenges against the DMA in the EU Court of Justice. While those lawsuits ran on in the background, the gatekeepers still had to show how they were complying with the new rules. This process included bilateral discussions with the European Commission, but also the publication of non-confidential compliance plans. The publication of these plans enabled third parties to evaluate and provide feedback. \u201cDrop dead\u201d-message Quickly, the reactions started to come in. Tech journalist Cory Doctorow said the gatekeepers\u2019",
    "comments": [],
    "description": "Undisclosed\u00a0affiliations\u00a0distort Digital\u00a0Market\u00a0Act\u2019s\u00a0public\u00a0workshops Over the past two decades, Big Tech companies have monopolised key features of the internet, making businesses and end users vulnerable to their massive and increasing power. The Digital Markets Act (DMA) is the EU\u2019s attempt to rein in Big Tech\u2019s monopoly power by limiting its worst excesses. Tech companies identified as gatekeepers [\u2026]",
    "document_uid": "3f43861a6e",
    "ingest_utctime": 1730205416
  },
  {
    "original_id": "41983019",
    "title": "Show HN: CreativePixel \u2013 Visual AI Image Generation with Reference Engine",
    "url": "https://creativepixel.ai",
    "score": 1,
    "timestamp": "2024-10-29T13:05:24",
    "source": "Hacker News",
    "content": "\u2728 Idea to Image Describe your vision, and watch as our AI brings it to life. Perfect for creating unique artwork, illustrations, or visualizing concepts. Ideal for: Artists, designers, writers, and anyone looking to visualize their ideas\ud83d\uddbc\ufe0f Photo Reimagining Give your photos a magical makeover. Transform them into different styles, add elements, or create entirely new scenes based on your existing images. Ideal for: Photographers, digital artists, and social media content creators\ud83e\udde0 Creative Assistant Stuck for ideas? Our AI will suggest creative concepts and help you explore new artistic directions for your projects. Ideal for: Creative professionals, marketers, and anyone facing creative blocks\ud83c\udff7\ufe0f Custom Text Addition Seamlessly blend text with your AI-generated or edited images. Our AI understands image composition and can intelligently integrate specified text. Ideal for: Graphic designers, meme creators, and social media managers\ud83d\udd0d Image Enhancer Turn blurry or low-quality images into sharp, clear, and larger versions. Ideal for improving old photos or creating high-quality prints. Ideal for: Photographers, archivists, and anyone working with legacy images\ud83d\udd0c API Integration Easily add our AI image capabilities to your own apps and websites with our robust APIs. Ideal for: Developers, businesses, and platforms looking to integrate AI imaging capabilities",
    "comments": [
      {
        "author": "vivekalogics",
        "text": "Hey HN! Built something to transform how we interact with AI image generation.<p>The Challenge: \nDespite all amazing AI models, getting from inspiration to exact output you want isn&#x27;t straightforward. When you find a perfect reference image or have a specific vision: \n- How do you create something unique but similar? \n- How do you control exactly what changes? \n- How do you maintain quality at scale? \n- How do you do it all without jumping between tools?<p>What I Built:\nAn intuitive and simplified system that lets you:\n1. Generate images through visual controls (dropdowns for style, composition, lighting, camera angle etc.)\n2. Create variations from any reference (stock photos, images you like, your own work) using our VisualMuse engine\n3. Fine-tune results with precision:\n   - Describe specific changes you want (&quot;make it more vibrant&quot;, &quot;add dramatic lighting&quot;)\n   - Use creativity slider to control how different from original you want results\n4. Upscale any image up to 10x while preserving quality\n5.  Available via APIs for automation needs<p>Quick Demo: <a href=\"https:&#x2F;&#x2F;res.cloudinary.com&#x2F;creativepixelai&#x2F;video&#x2F;upload&#x2F;v1730165545&#x2F;Reimagine-demo_hvvb7a.mp4\" rel=\"nofollow\">https:&#x2F;&#x2F;res.cloudinary.com&#x2F;creativepixelai&#x2F;video&#x2F;upload&#x2F;v173...</a><p>How it Works: \n- Start with any reference image \n- Explore creative variations instantly \n- Fine-tune with text descriptions or creative strength slider \n- Scale up to high-resolution when needed<p>Real Use Cases: \n- Transform reference images into unique visuals \n- Generate variations of design concepts \n- Adapt existing images to new styles \n- Create visuals using intuitive controls \n- Upscale to high-resolution when needed<p>Try it out:\n- Web Interface: <a href=\"https:&#x2F;&#x2F;creativepixel.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;creativepixel.ai</a> (10 free images)\n- Showcase: <a href=\"https:&#x2F;&#x2F;creativepixel.ai&#x2F;showcase\" rel=\"nofollow\">https:&#x2F;&#x2F;creativepixel.ai&#x2F;showcase</a><p>Built this because I was tired of the usual choices: expensive stock photos, designer bottlenecks, or endless prompt tweaking. There had to be a better way.<p>Would love to hear your thoughts and how you&#x27;d use it.<p>[Disclosure: I&#x27;m the creator]",
        "time": "2024-10-29T13:06:14"
      }
    ],
    "description": "Transform ideas into stunning visuals with CreativePixel. Generate, upscale, and reimagine images using our AI technology. Integrate our powerful APIs into your applications for seamless AI-driven image processing.",
    "document_uid": "371a65e5b3",
    "ingest_utctime": 1730205416
  }
]