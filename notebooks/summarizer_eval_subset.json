[
  {
    "original_id": "41980016",
    "title": "Startups should hire recruiters much sooner than they think",
    "url": "https://a16zcrypto.com/posts/article/why-startups-should-hire-recruiters/",
    "score": 1,
    "timestamp": "2024-10-29T07:30:41",
    "source": "Hacker News",
    "content": "When a startup founder finds product-market fit and secures funding to scale, the advice starts to roll in from investors, advisors, and mentors alike: Hiring the right people will create the bedrock of your company. With limited resources and big plans, hiring the wrong people can stop a startup\u2019s plans dead in their tracks. This advice is absolutely correct and completely useless. It\u2019s useless because it contains no actionable guidance whatsoever. And learning how to hire is a full-time job. That\u2019s why hiring the right recruiting help can also set founders up for success from the start. In this post, I\u2019ll share how. In fact, a recruiter should be among your first 10 hires. This may seem counterintuitive \u2014 if founders are pressed for time and need the right technical talent right away, shouldn\u2019t other hires take precedence? Far from it. Having worked and helped scale Airbnb (which I joined at 50 employees) and Coinbase (which I joined at 7), and having counseled countless other startups, I can attest to the fundamental importance of investing in recruiting talent at this early stage. When I joined Airbnb early on I was actually the third recruiter hired, and when I joined Coinbase I was the second A recruiter will save time \u2014 lots of time Sourcing (cold outreach) and managing the process for multiple candidates through multiple interview rounds can take a lot of time. In crypto especially, saving time is crucial because there is a limited pool of candidates with specialized expertise, or candidates talking to the same few companies all at once. Founders could easily be liaising with 100 candidates, and with all the other priorities they are juggling, some ball is going to drop. What does this mean practically? A bad candidate experience, missed hires, and a tarnished reputation. Founders can\u2019t afford to give anyone a bad experience. Not only is it a small world, but candidates are usually comparing companies as they interview. A poor candidate experience will easily knock you out of the running not just with that one candidate but with others too. In short, hiring well takes up a lot of time. If hiring is not taking up much time, then your startup\u2019s hiring process is likely too easy \u2014 or your company may be moving too quickly, which can lead to people problems and churn later. Also remember that as a founder, time is your most valuable resource. Hiring a recruiter means you can focus on other top priorities and allocate your time appropriately. You\u2019ll still be involved in the hiring loops, but your personal involvement will come later \u2014 and be more strategic and efficient. Here\u2019s a scenario that I often see with first-time founders: They are looking to make their first technical hires, have a good network, but the timing isn\u2019t right for many of the candidates in their network (and it\u2019s a competitive hiring landscape). Maybe they get a couple hires using this approach. So then they need to proactively reach out to candidates. But this raises a new series of questions: How many should you contact? Should your company use a sharpshooter or volume recruiting strategy \u2014 researching extensively to find highly qualified candidates with directly applicable skills, or casting a wide net? Your startup will likely need to employ a blend of both narrow-targeting and high-volume strategies. Both have their place depending on the role \u2014 but both are a lot of work. Sharpshooter recruiting can take a lot of time. But industry stats for engineering outreach using volume recruiting show you need to reach out to between 50 and 100 people to make even 1 hire. Each message to an engineer requires some amount of customization. The response rate from a candidate to a recruiter is at best 30% (that rate can be higher if a founder or technical leader reaches out). Let\u2019s imagine your company is only hiring 2 engineers. You could easily have 30 candidates running through your process, each doing 2 to 5 interviews with your team depending on how far they get in the process. That\u2019s 60 to 150 interviews to manage and schedule in just a very short timeframe. As the founder, you\u2019re likely leading some blend of product, engineering, marketing, fundraising, customer support, and general operations, as well as liaising with external counsel daily and so on. So even if you have the best of intentions with candidates, the candidate experience will slip. Negative experiences quickly make their way to Glassdoor reviews, future candidates read those reviews, candidates talk to each other at meetups and conferences about companies, and so on. Having a recruiter partner closely with the hiring managers on outreach strategy alleviates all of these problems. In the early days of Coinbase, I partnered with engineers and leaders like Brian Armstrong, Rob Witoff, and Varun Srinivasan, to help them build out their teams. I\u2019d put together lists of candidates who I thought were great, and, if they liked them, I\u2019d ghostwrite their initial candidate outreach; then they\u2019d do the phone screens while I managed the entire process. These partnerships worked well and landed many hires, at both the individual contributor and executive levels. The founder as a talent magnet The founder has an important role to play in early stage hiring \u2014 this includes articulating a compelling vision and mission for the company, defining values that can be implemented by their team through the hiring process, and of course selling and evaluating top-tier talent. Once hired, a great recruiter can help create and run a tight hiring process, pulling the founder in at strategic moments. A founder should remain a part of every interview loop until they have properly trained their leadership team how to hold an excellent hiring bar, and to recognize and evaluate for the right cultural add. But there\u2019s another crucial role for founders to play. Great engineers want to work with other great engineers building cool stuff. While this may seem like a reason for technical",
    "comments": [],
    "description": "How early should you hire a recruiter? They should be among your first ten employees: An explanation of why, and how to find the right one",
    "document_uid": "5683cef805",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41961860",
    "title": "Show HN: UTMultiply \u2013 easy VM cloning in UTM",
    "url": "https://github.com/lambda-m/UTMultiply",
    "score": 1,
    "timestamp": "2024-10-27T12:55:07",
    "source": "Hacker News",
    "content": "Scratching my own itch in setting up lab environments on my laptop. I have some ideas on extending this to become a little more robust, support more distros, clean up etc. but for now it helps me quickly spin up a few machines to test other software.",
    "comments": [],
    "description": "Easy UTM Cloning. Contribute to lambda-m/UTMultiply development by creating an account on GitHub.",
    "document_uid": "b264c1a1eb",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961492",
    "title": "A Brief History of Defragging",
    "url": "https://eclecticlight.co/2024/10/05/a-brief-history-of-defragging/",
    "score": 1,
    "timestamp": "2024-10-27T11:53:38",
    "source": "Hacker News",
    "content": "When the first Macs with internal hard disks started shipping in 1987, they were revolutionary. Although they still used 800 KB floppy disks, here at last was up to 80 MB of reliable high-speed storage. It\u2019s worth reminding yourself of just how tiny those capacities are, and the fact that the largest of those hard disks contained one hundred times as much as each floppy disk. By the 1990s, with models like the Mac IIfx, internal hard disks had doubled in capacity, and reached as much as 12 GB at the end of the century. Over that period we discovered that hard disks also needed routine maintenance if they were to perform optimally, and all the best users started to defrag their hard disks. Consider a large library containing tens or even hundreds of thousands of books. Major reference works are often published in a series of volumes. When you need to consult several consecutive volumes of such a work, how they\u2019re stored is critical to the task. If someone has tucked each volume away in a different location within the stack, assembling those you need is going to take a long while. If all its volumes are kept in sequence on a single shelf, that\u2019s far quicker. That\u2019s why fragmentation of data has been so important in computer storage. The story of defragging on the Mac is perhaps best illustrated in the rise and fall of Coriolis Systems and iDefrag. Coriolis was started in 2004, initially to develop iPartition, a tool for non-destructive re-partitioning of HFS+ disks, but its founder Alastair Houghton was soon offering iDefrag, which became a popular defragging tool. This proved profitable until SSDs became more widespread and Apple released APFS in High Sierra, forcing Coriolis to shut down in 2019, when defragging Macs effectively ceased. All storage media, including memory, SSDs and rotating hard disks, can develop fragmentation, but most serious attention has been paid to the problem on hard disks. This is because of their electro-mechanical mechanism for seeking to locations on the spinning platter they use for storage. To read a fragmented file sequentially, the read-write head has to keep physically moving to new positions, which takes time and contributes to ageing of the mechanism and eventual failure. Although solid-state media can have slight overhead accessing disparate storage blocks sequentially, this isn\u2019t thought significant and attempts to address that invariably have greater disadvantages. Fragmentation on hard disks comes in three quite distinct forms: file data across most of the storage, file system metadata, and free space. Different strategies and products have been used to tackle each of those, with varying degrees of success. While few doubt the performance benefits achieved immediately after defragging each of those, little attention has been paid to demonstrating more lasting benefits, which remain more dubious. Manually defragging HFS+ hard disks was always a questionable activity, as Apple added background defragmentation to Mac OS X 10.2, released two years before Coriolis was even founded. By El Capitan and Sierra that built-in defragging was highly effective, and the need for manual defragging had almost certainly become a popular myth. Neither did many consider the adverse effects on hard disk longevity of those intense periods of disk activity. The second version of TechTool Pro in 1999 offered a simplified volume map for what it termed optimisation, offering the options to defragment only files, or the whole disk contents including free space. By the following year, TechTool Pro was paying greater attention to defragging file system metadata, here shown in white. This view was animated during the process of defragmentation, showing its progress in gathering together all the files and free space into contiguous areas. TechTool Pro is still developed and sold by MicroMat, and now in version 20. A similar approach was adopted by its competitor Speed Disk, here with even more categories of contents. By 2010, my preferred defragger was Drive Genius 3, shown here working on a 500 GB SATA hard disk, one of four in my desktop Mac; version 6 is still sold by Prosoft. One popular technique for defragmentation with systems like that was to keep one of its four internal disks empty, then periodically clone one of the other disks to that, and clone it back again. Alsoft\u2019s DiskWarrior is another popular maintenance tool, shown here in 2000. This doesn\u2019t perform conventional defragmentation, but restructures the trees used for file system metadata, and remains an essential tool for anyone maintaining HFS+ disks. Since switching to APFS on SSDs several years ago, I have missed defragging like a hole in the head.",
    "comments": [],
    "description": "As Mac hard disks grew larger, fragmentation of file data, file system metadata and free space slowed the disk down. Here's how we used to solve that.",
    "document_uid": "818ee3f8a6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "ffj9tg",
    "title": "Rudimentary 3D on the 2D HTML Canvas",
    "url": "https://www.charlespetzold.com/blog/2024/09/Rudimentary-3D-on-the-2D-HTML-Canvas.html",
    "score": 2,
    "timestamp": "2024-10-27T00:54:38.000-05:00",
    "source": "Lobsters",
    "content": "This results in the following transform formulas: ax+cy+e=x\u2032 bx+dy+f=y\u2032 You can define such a 2D matrix using the constructor of the same DOMMatrix object that I used for the 3D transforms. That matrix is then passed to the setTransform method of the object returned from createPattern to transform a pattern in addition to any transforms set on the graphics context. In the general case, a, b, c, d, e, and f must be obtained by solving two sets of three simultaneous equations for the following mapping: You only need to solve for three sets of points because the fourth comes along for the ride. However, in the case where you\u2019re using an entire bitmap or off-screen canvas for your pattern, the mapping simplifies considerably because the source coordinates (at the left) are often zero. In this diagram w is the width of the image and h is the height: Using the upper-left, upper-right, and lower-left points, the simultaneous equations crumble into something almost trivial and you can derive: a=(x\u20321\u2013x\u20320)/w b=(y\u20321\u2013y\u20320)/h c=(x\u20323\u2013x\u20320)/w d=(y\u20323\u2013y\u20320)/h e=x\u20320 f=y\u20320 Another issue is that surfaces in 3D space conceptually have both a front and a back. For the six faces of a die, the front of each face is the outside of the cube and the back is the inside. These images of the die dots must be oriented on the front side. Moreover, only those faces with front sides facing the user (either fully or partially) should be drawn. The orientation of a surface in 3D space is described by something called a surface normal. This is a 3D vector that is orthogonal to the surface; in the case of a face of a cube, the normal points outward from the face. In my simple coordinate system, if the Z coordinate of the normal is positive, then the normal is conceptually pointing out of the screen and the viewer should be able to see that surface. This normal can be easily calculated by the vector cross product. The cross product of two 3D vectors A and B is symbolized as A \u00d7 B. The result is a third vector that is orthogonal to both A and B. For a right-hand coordinate system, the direction of this vector is given by another right-hand rule: Curve the fingers of your right hand to sweep from A to B. The thumb points in the direction of the cross product. The cross product is not commutative. Because I thought I might need some additional vector functions, I defined a Vector3 class in the Vector3.js file. This is not a complete Vector3 class but has some essentials that I use in the blog entry. Here\u2019s the result: Notice that the value of the die is also displayed. The Die3D class that implements this graphic extends the CubeWireFrame class and is defined in the Die3D.js file. The constructor is devoted to creating six canvas objects on which are rendered the six standard faces of a die. Because each of these faces has the same background color, each of these images is outlined in black to make the faces distinct. Otherwise the background of the faces would blend into each other and you wouldn\u2019t be able to see the edges. The Render method loops through the cube vertices and calculates a cross product from the left and top sides. This is a the normal vector pointing out from the face of the surface. The surface is only visible if the Z coordinate of this normal is positive. By keeping track of the maximum Z coordinate, it\u2019s also possible to determine which side is facing towards the viewer. This is displayed as the Die Value. The Render method creates a path based on the 3D transformed vertex points and uses those transformed points to calculate a matrix to transform the pattern as described above. This is truly \u201crudimentary\u201d 3D, as the title of this blog entry indicates. It would be possible to use this same technique to display other types of polyhedra, but only convex polyhedra. If one face partially obscures another face, then rather complex clipping would have to be implemented. Similar problems would arise when displaying multiple overlapping polyhedra. This is stuff that a 3D system like WebGL handles automatically. Also, there\u2019s no perspective. Objects in the background are the same size as objects in the foreground. Perspective in 3D is accomplished through a camera transform, but perspective has the effect of transforming rectangles not to parallelograms but to irregular quadrilaterals, sometimes with infinite dimensions, in which case it would not be possible to define a 2D transform to stretch the patterns to cover the die faces. There\u2019s also no concept of light providing different levels of illumination of the faces of the die. But that\u2019s possible to implement using the surface normals. For example, here\u2019s a dodecahedron whose 12 faces are illuminated based on a hypothetical light source from the upper left: Click the Animate button to see it spin, and for the sides to change shade based on their orientation. This job was facilitated by some code I wrote for my book 3D Programming for Windows: Three-Dimensional Graphics Programming for the Windows Presentation Foundation (Microsoft Press, 2008). In addition to the demonstration programs for the book, I also created a 3D media library, all of which is available in this ZIP file. Unfortunately, many of these demos are standalone XAML files, which no longer run in the browser, and even the WPF EXE files don\u2019t seem to be running properly. But the media library includes definitions of a bunch of primitive 3D objects, including the dodecahedron. In 3D programming, three-dimensional objects are constructed from triangles rather than the squares that I used for the die. Triangles are the simplest form of polygon and are guaranteed to lie entirely on a plane. Each of the pentagonal faces of the dodecahedron is constructed from five triangles that share a point in the center of the pentagon. A collection of such triangles",
    "comments": [
      {
        "author": "vg_head",
        "text": "<p>Great post! Regarding projection, it could have been possible to implement \u201crudimentary projection\u201d as well if Canvas2D supported additional perspective parameters. IIRC it\u2019s only possible to construct an affine transform matrix using a/b/c/d/e/f.</p>\n<p>The OpenVG spec <a href=\"https://registry.khronos.org/OpenVG/specs/openvg-1.1.pdf\" rel=\"ugc\">1</a> allows these parameters (the only values not used by the affine matrix) and explains how transforms happen in that case. The interface has no helpers though, and setting the values manually is a no-go. The easiest way to achieve this is using a polygon-to-polygon transform. There is an implementation of this in Skia\u2019s <code>SkMatrix::setPolyToPoly</code>. <a href=\"https://github.com/google/skia/blob/cadf2538dcde3b1c7cde5505191990381a39c68b/src/core/SkMatrix.cpp#L1385\" rel=\"ugc\">2</a></p>\n",
        "time": "2024-10-27T02:28:56.000-05:00"
      }
    ],
    "description": "Charles Petzold is the author of the books Code and The Annotated Turing",
    "document_uid": "f1c414e08b",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "41979984",
    "title": "Give Yourself More Playtime",
    "url": "https://martinrue.com/give-yourself-more-playtime/",
    "score": 1,
    "timestamp": "2024-10-29T07:23:26",
    "source": "Hacker News",
    "content": "Give Yourself More Playtime I remember my first computer fondly. As a kid I was more than addicted to the Commodore 64 \u2013 perfectly happy to sit and endure at least 5 minutes of the most colourful, potentially epileptic attack you can imagine, simply to have a few games of Boulder Dash. It was worth every photon, without a doubt. Today its mechanical keyboard, beige plastic casing, and whopping 64 KB of memory condemns it almost exclusively to a quiet life of nostalgia\u2026 and some weird Internet forums. But back then it was the most fascinating thing 11-year-old me had ever seen. Between games of Boulder Dash, sheer curiosity would keep bringing me back to this bright blue screen with hideous white text, wondering what else it could do. Gradually I discovered new commands that made it do other things, besides just LOAD \u2013 although I still typed that often enough to cause the dog to eat a lot of my homework. In time I realised that the machine could run code called BASIC, and that I could simply type it in and then type RUN to make it do things. Being 11 and fearing the inevitably gradual violation of personal privacy by government, the first program I wrote was a password program. It wasn't very clever. Not even a bit, in fact: 10 PRINT \"ENTER PASSWORD\" 20 INPUT L$ 30 IF L$ = \"SECRET\" THEN GOTO 50 40 GOTO 10 50 PRINT \"OK\" Well, that was until I realised something insanely awesome. You probably don't think it's insanely awesome, but I definitely did when I was 11. Armed with the knowledge that typing LOAD triggered loading the first program from the datasette, I figured that if LOAD became my new line 50, this program could protect my computer from the government. It's 11-year-old genius in front of your very eyes. The plan was simple: I type this program and follow it with a swift RUN command. My code starts running and loops until someone types the correct password. When they do, rather than the imaginatively corrupt PRINT \"OK\", it runs LOAD and begins loading whichever game is in the datasette. Man, I was impressed with myself. Well, until I realised you could just turn the computer off and back on again, type LOAD, and completely subvert my entire plan. Obviously, the government would figure that out, so I needed something better. After some careful thought, it became clear to me that the program needed to be on the actual tape, not sitting around in memory waiting to suicide as soon as the power went out. Luckily I discovered the sibling of LOAD, SAVE, which allowed me to do just that. So, I typed my password program once again, inserted a blank cassette and ran SAVE. SAVING READY. Sweet Jesus, it worked. I released the RECORD and PLAY keys on the datasette, pressed REWIND and rebooted. Yes, if you wanted to run your program again you'd have to rewind it\u2026 and that wasn't the only awesome thing about the 90s. As the computer came back to life, I was sat there with my LOAD command at the ready. Without even so much as a colour leaping out of the screen and smashing me in the face, my password program was running! Still, it would be a pretty big ask to get the government to take out the game they wanted to load, put my password program cassette in, and run that first. I mean, I could write some instructions, but I just don't think it's very likely they would follow them. So, onto my next wave of genius. I noticed something interesting: this Joseph and the Amazing Technicolor Dreamcoat light show that happens when a game is loading doesn't begin immediately after running LOAD. In other words, the tape is spinning but nothing is happening for about 5 seconds. Conveniently, my awesome password program took about 1 second to write to a blank tape. If my maths was correct, I had plenty of space at the beginning of Boulder Dash to stash my own program. Now I was really getting somewhere! MI5 were probably shitting their pants, I figured. At my earliest inconvenience I invited a friend over to see my achievement. He had a Commodore 64 too, so I knew he'd appreciate what I'd done here. He loads my tape. We wait. And BOOM! ENTER PASSWORD I knew he was impressed. Well, I think he was. There wasn't much time between the password prompt appearing and him pressing the RUN/STOP key, typing LIST, and reading the god damned password right out of the code. Obviously, the government would figure that out too, so I needed something better. My new target swiftly became that damn RUN/STOP key. It took me a few days until I got the chance to dial up to the Internet (yes, that was a thing too) and search for a way to stop my friends, and the government, breaking my code and listing it to read the password. Eventually I learned that POKE allows you to change values at specific memory addresses, and with that it's possible to overwrite vectors to prevent routines such as LIST and STOP from working. With my newfound POKE knowledge, my password program was now solid. It was time to write this baby to the game tape, making absolutely sure nobody in my family could get better than me at Boulder Dash. I was so impressed with my new skill that I didn't just stop at Boulder Dash. In fact, I password protected every single one of my favourite game tapes using the same program, quickly rising to fame as the family gatekeeper of any game worth playing. It was coming up to summer, and another thing we did as 11-year-old kids in the 90s was go outside in summer. Weird, I know. 4 or 5 weeks of being a healthy 90s outdoor summer kid and I found myself ready for",
    "comments": [],
    "description": "I remember my first computer fondly. As a kid I was more than addicted to the Commodore 64.",
    "document_uid": "f2a0638fc2",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41962440",
    "title": "The Active Badge Location System (1992)",
    "url": "https://dl.acm.org/doi/pdf/10.1145/128756.128759",
    "score": 1,
    "timestamp": "2024-10-27T14:20:50",
    "source": "Hacker News",
    "content": "%PDF-1.4 %\u00e2\u00e3\u00cf\u00d3 1 0 obj <>stream x\u0153\u00ed\u00c1\ufffd \u00c3 \u007f\u00ea]\u00e1 U \ufffd\u00ce\ufffd+\u015f endstream endobj 2 0 obj <>]/Intent/Perceptual/Subtype/Image/Height 1004/Filter/FlateDecode/Type/XObject/Width 1003/SMask 1 0 R/Length 50324/BitsPerComponent 8>>stream x\u0153\u00ec\u0130\u00e7\u007f\u201cw\ufffd\u00ef\u00ff\u007f\u00e2w\u00e7\u00dc=\u00bbw\u00ce\ufffdsfwg3\u203a\ufffd\u00c9\u00ec C\u00d2{\u2019\ufffdRI\u2021\u201e\u201e\u00f4\ufffd\ufffd^H! I(w\u00c0`\u00e3&[\u00eer\u2018l\u00c9E\u00b6\u0160e\u00c9r\u00af\u00e6\\\u2030\u00e7\u00c70\u0152\u00a5\u00eb\u00ba\u00f4\u00b9\u00be\u00d7\u00f5z>\u015e7w \u007fmI\u00af|\u00d7\u2013\u00ff\u00faW 3::\u00eb\u00eb\u00eb\u00ec\u00e8h\u00f6x\u00aa**\u00caKKc\u01521\u00c6s\u00c8\u00b4 \u00d62X\u2039a-\u2030\u00b50\u2013n\u00f3\u00d3\u0161\ufffd\u0161j\u00f3\u00f9\u00f6\u00e5\u00e5}\u015f\u00d1G\u00be\u00fb.c\u01521\u00c6cL\u00dbg~\u00a8Er\u203a\u00d7\u00ab\u00b3t\u00b3\u00ff\ufffd\u00af\u00b55k\u00f7n\u00f1\u201ca\u01521\u00c6c\u00cc\u00e2\u00cb\u00fa\u00f9g-\ufffd\u00d3\u00bd\u00bb\u00abk\u00d7\ufffd?\u0160\u0178c\u01521\u00c6c MKh-\u00a43\u0153\u00ee}\u00d1h\u00ce\u015e\u00bd\u00e2;c\u01521\u00c6c\u0160N\u00cbi-\u00aa3S\u00efM\ufffd\ufffd\u00e2/c\u01521\u00c6c6\u02dc\u2013\u00d6f\u00d7{yI\u2030\u00f8\u2021\u00c9c\u01521\u00c6\u02dcm\u00a6\u00b6I\u00e9~df&?;[\u00fcd\u01521\u00c6c\u00ccf\u00d32[\u2039m\u00c3~\u00cf\u00ce\ufffd\u2039\u00ff7|\u00bci\u00d3\u00d6/\u00bf\u00dc\u00b1}{^v\u00f6\ufffd\u201a\u00c6c\u01521\u00c62\u00ad\u00c6w\u00fc\u011f\u00c3\u2013\u00cd\u203a\u00b5$^|?k\u00b1ml\u00bd:p`1\u007f\u00ef\u00b6\u00af\u00bf.-.\u00ee\u00ed\u00e91\u00f6o T\u00a4\u2026\u00b1\u2013\u00c7Z$/\u00a6\u00a5\u00b5\u00e46\u00ea\u00ef\u00ad\u00a9\u00aa:\u00e3_\u00f7\u00d3\u00f7\u00df\u00fc~\u00a3\u015fF \u00c0N\u00b4T\u00d6\u201a\u00f9\u0152Q\u00ad\u2026\u00b7\u015f\u00bf\u00ab\u0130\u00e7[\u00f8o\u00d9\u00fc\u00e9\u00a7\u00cd\ufffd\u015f\u00bf \u00b07-\u203a\u00b5x^\u00b8\u00ae\u00b5\u00fc\u00d6\u00f3W=z\u00f4\u00eb/\u00beX\u00f8\u00e2=90`\u00d4G \u00d8\u203a\u00cf_\u00c5k\u00f9\u00adEx\u00da\u007f~ue\u00e5\u007fxAn\u00ae\ufffd? p -\u00a1\u00b5\ufffd^ \u00b3\u00b5O\u00efO\ufffd\u00ff\u00e4\u0192N\u00f7\u00c7\u00ee\u0130\u00bd\u00db\u00d8 p-\u00a7OW\u00daZ\u201ek)\ufffd\u00c6\u0178YRT\u00b4\u00c0\u00bb\u00cdLLL\u015fQ \u00a1\u00e5\u00f4\u00efN\u00a3\u00a5x\u00aa\u007f\u00e0\u00f8\u00f8\u00f8\u00b7\u00fa\u00d1H\u00c4\u201e p-\u00aaH\u00ee\u00f1/\u00e1[\u0161\u0161N\u00f7G\u0161\u00f4! \ufffd\u00a2\u00a5\u00f5\u00e9\u00aa[\u00f2\u201d\u015f\u00a8\u00dc\u00ac\u00ac\u00d3\u0131Q\u00a3##&\u0131\u00fb G\u00d1\u00d2\u00fat\u00d5\ufffd\u2014\ufffd\u00bd\u00f8?gvv\u00f6t\u00bf\u00f8u\u00e7?\u02dc\u00f7\u00ef \u0153F\u00ecS\u2020\u00b7\u00e4Z\u2013/\u00f2\u00e9\u011f\u00fbO\u00f7\u2018p\u00d8\u00d4\u007f? \u00e0(Z`\u0178\u00ae\u00bd\u00b5,_\u00e4Rv\u00f8\u011f)\u00ff\u201e\u00af\u00bf\u00f8\u00c2\u00d4\u007f<",
    "comments": [],
    "description": "No description available.",
    "document_uid": "9a1527389d",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962509",
    "title": "The Prophet of Cyberspace",
    "url": "https://www.filfre.net/2016/11/the-prophet-of-cyberspace/",
    "score": 3,
    "timestamp": "2024-10-27T14:34:14",
    "source": "Hacker News",
    "content": "William Gibson was born on March 17, 1948, on the coast of South Carolina. An only child, he was just six years old when his father, a middle manager for a construction company, choked on his food and died while away on one of his many business trips. Mother and son moved back to the former\u2019s childhood home, a small town in Virginia. Life there was trying for the young boy. His mother, whom he describes today as \u201cchronically anxious and depressive,\u201d never quite seemed to get over the death of her husband, and never quite knew how to relate to her son. Gibson grew up \u201cintroverted\u201d and \u201chyper-bookish,\u201d \u201cthe original can\u2019t-hit-the-baseball kid,\u201d feeling perpetually isolated from the world around him. He found refuge, like so many similar personalities, in the shinier, simpler worlds of science fiction. He dreamed of growing up to inhabit those worlds full-time by becoming a science-fiction writer in his own right. At age 15, desperate for a new start, Gibson convinced his mother to ship him off to a private school for boys in Arizona. It was by his account as bizarre a place as any of the environments that would later show up in his fiction. It was like a dumping ground for chronically damaged adolescent boys. There were just some weird stories there, from all over the country. They ranged from a 17-year-old, I think from Louisiana, who was like a total alcoholic, man, a terminal, end-of-the-stage guy who weighed about 300 pounds and could drink two quarts of vodka straight up and pretend he hadn\u2019t drunk any to this incredibly great-looking, I mean, beautiful kid from San Francisco, who was crazy because from age 10 his parents had sent him to plastic surgeons because they didn\u2019t like the way he looked. Still, the clean desert air and the forced socialization of life at the school seemed to do him good. He began to come out of his shell. Meanwhile the 1960s were starting to roll, and young William, again like so many of his peers, replaced science fiction with Beatles, Beats, and, most of all, William S. Burroughs, the writer who remains his personal literary hero to this day. William Gibson on the road, 1967 As his senior year at the boys\u2019 school was just beginning, Gibson\u2019s mother died as abruptly as had his father. Left all alone in the world, he went a little crazy. He was implicated in a drug ring at his school \u2014 he still insists today that he was innocent \u2014 and kicked out just weeks away from graduation. With no one left to go home to, he hit the road like Burroughs and his other Beat heroes, hoping to discover enlightenment through hedonism; when required like all 18-year-olds to register for the draft, he listed as his primary ambition in life the sampling of every drug ever invented. He apparently made a pretty good stab at realizing that ambition, whilst tramping around North America and, a little later, Europe for years on end, working odd jobs in communes and head shops and taking each day as it came. By necessity, he learned the unwritten rules and hierarchies of power that govern life on the street, a hard-won wisdom that would later set him apart as a writer. In 1972, he wound up married to a girl he\u2019d met on his travels and living in Vancouver, British Columbia, where he still makes his home to this day. As determined as ever to avoid a conventional workaday life, he realized that, thanks to Canada\u2019s generous student-aid program, he could actually earn more money by attending university than he could working some menial job. He therefore enrolled at the University of British Columbia as an English major. Much to his own surprise, the classes he took there and the people he met in them reawakened his childhood love of science fiction and the written word in general, and with them his desire to write. Gibson\u2019s first short story was published in 1977 in a short-lived, obscure little journal occupying some uncertain ground between fanzine and professional magazine; he earned all of $27 from the venture. Juvenilia though it may be, \u201cFragments of a Hologram Rose,\u201d a moody, plot-less bit of atmospherics about a jilted lover of the near future who relies on virtual-reality \u201cASP cassettes\u201d to sleep, already bears his unique stylistic stamp. But after writing it he published nothing else for a long while, occupying himself instead with raising his first child and living the life of a househusband while his wife, now a teacher with a Master\u2019s Degree in linguistics, supported the family. It seemed a writer needed to know so much, and he hardly knew where to start learning it all. It was punk rock and its child post-punk that finally got him going in earnest. Bands like Wire and Joy Division, who proved you didn\u2019t need to know how to play like Emerson, Lake, and Palmer to make daring, inspiring music, convinced him to apply the same lesson to his writing \u2014 to just get on with it. When he did, things happened with stunning quickness. His second story, a delightful romp called \u201cThe Gernsback Continuum,\u201d was purchased by Terry Carr, a legendary science-fiction editor and taste-maker, for the 1981 edition of his long-running Universe series of paperback short-story anthologies. With that feather in his cap, Gibson began regularly selling stories to Omni, one of the most respected of the contemporary science-fiction magazines. The first story of his that Omni published, \u201cJohnny Mnemonic,\u201d became the manifesto of a whole new science-fiction sub-genre that had Gibson as its leading light. The small network of writers, critics, and fellow travelers sometimes called themselves \u201cThe Movement,\u201d sometimes \u201cThe Mirrorshades Group.\u201d But in the end, the world would come to know them as the cyberpunks. If forced to name one thing that made cyberpunk different from what had come before, I wouldn\u2019t point to any of the exotic computer",
    "comments": [],
    "description": "No description available.",
    "document_uid": "aef0e04585",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961677",
    "title": "A New Volcanic Era",
    "url": "https://www.cnn.com/interactive/2024/10/climate/solutions/iceland-volcanos/index.html",
    "score": 2,
    "timestamp": "2024-10-27T12:22:39",
    "source": "Hacker News",
    "content": "KraflaGrindavikReykjavikICELAND50 miles50 kmFor Klara Halld\u00f3rsd\u00f3ttir, the volcano that has roiled southwest Iceland for the last year is both awe-inspiring and devastating. She has hiked to a crater to marvel at the fiery eruption and the river of cooled lava stretching down toward Grindavik, the town she has lived in her whole life.But this volcano also forced her from her home.Last November, she was at the beach letting her dogs run wild when the ground started moving and did not stop. It was a swarm of earthquakes, the warning signs of an imminent eruption.Her family packed swiftly and joined a line of cars rolling out of town. It was \u201clike a terrible movie,\u201d she said. No one knew whether they would be able to come back. Nearly a year on, only a handful of the 3,600 residents have returned.Vikings roamed the last time the volcanoes on Iceland\u2019s Reykjanes Peninsula raged. Now, eight centuries later, this slice of land close to the capital city Reykjavik is one of the more densely populated parts of the country.Icelanders like Klara have a complex relationship with volcanoes. They are both a force of destruction \u2014 lava has already consumed homes and carved up roads in Grindavik \u2014 and a source of abundant clean energy, powering people\u2019s lives.The country itself was born of volcanic activity.Earthquakes since 2020:4.53.55 magnitudeApproximate location of fissureTectonic plate boundaryVolcanic eruptions, since 1960Tectonic plates are huge pieces of the planet\u2019s outer shell constantly ripping apart and crashing together, causing earthquakes and volcanoes.Iceland, one of the most volcanically active places on the planet, was created from lava, steam and heat.It sits astride the Mid-Atlantic Ridge, a huge spine of mostly underwater mountains that separates two plates. Magma pushes up as the plates pull apart, creating new land in a violent, searing process.Since December 2023, the Reykjanes Peninsula in southwest Iceland has been rocked by a series of earthquakes and eruptions.Data as of June 17, 2024 Source: National Oceanic and Atmospheric Administration, United States Geological Survey, Icelandic Meteorological Office, Polar Geospatial CenterThe first eruption arrived in December. A fissure more than two miles long sliced the ground open, sending fountains of lava spewing hundreds of feet into the air. Five more eruptions have followed, engulfing homes on the outskirts of town, threatening a vital power station and turning Grindavik into a ghost town. The lava has also flowed close to the turquoise pools of the Blue Lagoon geothermal spa, one of Iceland\u2019s most famous tourist attractions, forcing several evacuations and closures over the past year.Klara Halld\u00f3rsd\u00f3ttir visits the Grindavik home she was forced to flee, as lava erupts from a crater just a few miles away, in April 2024. Iceland is used to volcanoes. It experiences roughly one eruption every five years, though most are in uninhabited areas. Some are even described as \u201ctourist volcanoes,\u201d relatively accessible and typically non-disruptive. These new eruptions are not that; they are violent, dangerous and could last centuries. They could also hold the key to a new future.As this new volcanic era upends lives in the southwest, hundreds of miles northeast at a volcanic caldera called Krafla, there is an audacious plan underway to drill directly into a magma chamber.Firefighters driving near the Sundhnukagigar fissure, north of Grindavik, in April 2024KraflaGrindavikReykjavikICELAND50 miles50 km\u2018The potential is limitless\u2019In 2009, Bjarni P\u00e1lsson was an engineer with Iceland\u2019s national power company, Landsvirkjun, running a deep drilling geothermal project at Krafla. They were trying to sink a borehole nearly 3 miles into the ground, but the drill kept getting stuck. \u201cAgain and again, at exactly the same depth,\u201d he said.When they were eventually able to free it, they found glass chips \u2014 cooled, crystallized, molten rock. It was proof of what they had stumbled upon: a magma chamber. P\u00e1lsson was shocked. These reservoirs of super-hot molten rock exist everywhere there are volcanoes, but are very hard to find and usually much deeper. The team rushed to control and cool the borehole, pumping in around 1 million tons of cold water before closing it. Fifteen years later, P\u00e1lsson is standing in the exact same spot, his high-viz jacket the only splash of color in Krafla\u2019s stark, white landscape. Armed with new technology and know-how, he is going back in.The ambition of the geothermal experts and volcanologists that comprise the Krafla Magma Testbed is to convert the immense heat and pressure into a new \u201climitless\u201d form of supercharged geothermal energy \u2014 a tantalizing prospect as the world struggles to end its relationship with planet-heating fossil fuels. \"This has never been done before,\" said Hjalti P\u00e1ll Ing\u00f3lfsson, director of the Geothermal Research Cluster, which developed the project. He compares its scale and ambition to the James Webb Space Telescope, which is giving humans an unprecedented view of the universe. If they succeed, the implications could reverberate around the world, Ing\u00f3lfsson said. There are an estimated 800 million people living within roughly 60 miles of an active volcano.\u201cWe are looking into the sky, we are spending trillions and trillions of dollars to understand planets far away,\u201d he said. \u201cBut we do not spend nearly as much on understanding our own.\u201dGeothermal expert Bjarni P\u00e1lsson was at Krafla in 2009 when they discovered the magma chamber. Fifteen years on, he\u2019s getting ready to drill into it again \u2014 this time with an ambitious purpose. Krafla is a unique natural laboratory for studying volcanoes. Not only is it one of the hottest geothermal fields in the world, it is also very accessible. There is a power plant, high speed internet and a paved road \u2014 all on top of a volcano.Drilling into magma that is around 1,800 Fahrenheit (nearly 1,000 degrees Celsius) won\u2019t be easy. But as humans heat the planet at record speed with fossil fuel pollution, there is increasing pressure to perform moonshot feats of engineering to save us from ourselves.If all goes to plan, the first borehole will be completed in 2027 and will mark the first time anyone has ever implanted sensors directly into a magma chamber.",
    "comments": [],
    "description": "They drilled into a volcano. What they found could help power the world.",
    "document_uid": "9350620c94",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962323",
    "title": "Web of Science index puts eLife 'on hold' due to its publishing model",
    "url": "https://www.science.org/content/article/web-science-index-puts-elife-hold-because-its-radical-publishing-model",
    "score": 3,
    "timestamp": "2024-10-27T14:01:42",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "f998f36cf2",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961564",
    "title": "Easy real-time collision detection",
    "url": "https://arxiv.org/abs/2406.00026",
    "score": 2,
    "timestamp": "2024-10-27T12:06:23",
    "source": "Hacker News",
    "content": "Happy Open Access Week from arXiv! Open access is only possible with YOUR support. Give to arXiv this week to help keep science open for all.",
    "comments": [],
    "description": "Abstract page for arXiv paper 2406.00026: Easy real-time collision detection",
    "document_uid": "960cf152c8",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41966031",
    "title": "Fundamentals of Scanning Electron Microscopy (2006)",
    "url": "https://link.springer.com/chapter/10.1007/978-0-387-39620-0_1",
    "score": 1,
    "timestamp": "2024-10-27T23:01:11",
    "source": "Hacker News",
    "content": "O. C. Wells, Scanning Electron Microscopy, McGraw-Hill, New York (1974). Google Scholar S. Wischnitzer, Introduction to Electron Microscopy, Pergamon Press, New York (1962). Google Scholar M. E. Haine and V. E. Cosslett, The Electron Microscope, Spon, London (1961). Google Scholar A. N. Broers, in: SEM/1975, IIT Research Institute, Chicago (1975). Google Scholar J. Goldstein, D. Newbury, D. Joy, C. Lyman, P. Echlin, E. Lifshin, L. Sawyer, and J. Michael, Scanning Electron Microscopy and X-Ray Microanalysis, 3rd edn, Kluwer Academic/Plenum Publishers, New York (2003). Google Scholar C. W. Oatley, The Scanning Electron Microscope, Cambridge University Press, Cambridge (1972). Google Scholar J. I. Goldstein and H. Yakowitz, Practical Scanning Electron Microscopy, Plenum Press, New York (1975). Google Scholar Y. X. Chen, L. J. Campbell, and W. L. Zhou, J. Cryst. Growth, 270 (2004) 505.Article CAS ADS Google Scholar J. J. Liu, A. West, J. J. Chen, M. H. Yu, and W. L. Zhou, Appl. Phys. Lett., 87 (2005) 172505.Article ADS CAS Google Scholar T. E. Everhart and R. F. , J. Sci. Instrum., 37 (1960) 246.Article ADS Google Scholar D. C. Joy, C. S. Joy, and R. D. Bunn, Scanning, 18 (1996) 533.Article CAS PubMed Google Scholar H. Koike, K. Ueno, and M. Suzuki, Proceedings of the 29th Annual Meeting EMSA, G. W. Bailey (Ed.), Claitor\u2019s Publishing, Baton Rouge (1971), pp. 225\u2013226. Google Scholar L. Xu, W. L. Zhou, C. Frommen, R. H. Baughman, A. A. Zakhidov, L. Malkinski, J. Q. Wang, and J. B. Wiley, Chem. Commun., 2000 (2000) 997. Google Scholar K. Tanaka, A. Mitsushima, Y. Kashima, T. Nakadera, and H. Osatake, J. Electron Microsc. Tech., 12 (1989) 146.Article CAS PubMed Google Scholar K.-R. Peters, J. Microsc., 118 (1980) 429.CAS PubMed Google Scholar R. P. Apkarian and J. C. Curtis, Scan. Electron Microsc., 4 (1981) 165.CAS PubMed Google Scholar A. Boyde, Scan. Electron Microsc., 11 (1978) 303. Google Scholar T. F. Anderson, NY Acad. Sci., 13 (1951) 130\u2013134. Google Scholar R. P. Apkarian, Scanning Microsc., 8(2) (1994) 289.CAS PubMed Google Scholar K.-R. Peters, Scan. Electron Microsc., 4 (1985) 1519. Google Scholar R. P. Apkarian, 45th Annual Proceedings of the Microscopy Society of America (1987) 564. Google Scholar D. C. Joy, 52nd Annual Proceedings of the Microscopy Society of America (1994). Google Scholar E. L. Bearer, L. Orci, P. Sors, J. Cell Biol., 100 (1985) 418.Article CAS PubMed Google Scholar R. P. Apkarian, Scanning, 19 (1997) 361.Article CAS PubMed Google Scholar",
    "comments": [],
    "description": "'Fundamentals of Scanning Electron Microscopy (SEM)' published in 'Scanning Microscopy for Nanotechnology'",
    "document_uid": "8511fd5f79",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "41970028",
    "title": "InterLM: Open-source LLM with 1M context window",
    "url": "https://github.com/InternLM/InternLM",
    "score": 1,
    "timestamp": "2024-10-28T12:34:48",
    "source": "Hacker News",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [],
    "description": "Official release of InternLM2.5 base and chat models. 1M context support - InternLM/InternLM",
    "document_uid": "62cce97bab",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "41962056",
    "title": "A chain of Mars exploration catastrophes",
    "url": "https://english.elpais.com/science-tech/2024-10-27/a-chain-of-mars-exploration-catastrophes-when-nasa-confused-the-metric-with-the-imperial-system.html",
    "score": 1,
    "timestamp": "2024-10-27T13:25:37",
    "source": "Hacker News",
    "content": "The 1990s were a disastrous decade for Mars exploration projects. Out of seven attempts, only two were successful. And the apparent ease of Viking 1 and 2 \u2014 a pair of NASA landers and orbiters \u2014 hid a more difficult reality: landing on the red planet was much more difficult than it seemed. In the first 50 years of exploring Mars, almost half of the vehicles sent there crashed or stopped working.In September 1992, NASA launched its Mars Observer, a platform that was meant to continue and expand the studies carried out by the Viking orbiters. It was a newly-designed vehicle, the first of a class intended to make planetary visits not only to Mars, but also \u2014 in the future and with the necessary adaptations \u2014 to Venus, or even Mercury.It was built from a standard structure typical of conventional communications satellites. It seemed like a good idea from the point of view of taking advantage of already proven designs, but in the end, it wasn\u2019t. Some of its components \u2014 which had worked well for weeks around the Earth \u2014 wouldn\u2019t be able to withstand the rigors of a months-long trip to much colder environments.When the Mars Observer was only a couple of days away from reaching its target, it was given the order to pressurize its tanks in preparation for braking. It\u2019s unclear exactly what happened: suspicion points to a slight leak of oxidizer (nitrogen tetroxide) from a valve. Even though it was a small amount over the course of an 11-month-long flight, the corrosive liquid would have built up in the pipes. Hence, when a second set of valves opened, it could have come into contact with the fuel, causing an explosion. This is just one of several hypotheses, but the sudden failure of communications didn\u2019t allow for a definitive conclusion to be reached.The Mars Global Surveyor (MGS) \u2014 another probe launched four years later \u2014 had better luck. Following the failure of the Observer, the idea of using a type of spacecraft that would be of use to all missions had already been abandoned. This was a new design, specifically built for operating on Mars: the scientific instruments it carried were almost identical to those that had been lost in the previous attempt. The relative positions of Earth and Mars made the journey a long one: 11 months. The trip culminated with the vessel\u2019s entry into a heavily-elongated orbit, whose altitude would be reduced until it became circular at a level of only 142 miles above Mars. The adjustment took another year-and-a-half because, for the first time, solar panels (rather than chemical engines) were used as air brakes. There were also two steerable fins, strong enough to withstand repeated friction from the upper layers of the atmosphere.In the end, the Global Surveyor was placed in a sun-synchronous orbit, calculated in such a way that it passed over the same terrain feature at the same solar time. The lighting conditions were similar and the shadows \u2014 always identical \u2014 made it easy to detect changes in the landscape.Although projected with a useful life of only two years, the MGS received repeated mission extensions (that is, budget allocations), which kept it active for almost 10 years. This was longer than any other spacecraft sent to Mars until then. During that time, it obtained a quarter-of-a-million images, as well as detailed altimetric coverage of the planet. This information would prove to be of great help in preparing for future operations taken by the mobile robots that were about to be completed.By November 1996, the Soviet Union had ceased to exist. Mars 8 was intended to be the first deep-space probe to fly the Russian tricolor flag and was tasked with an ambitious scientific program. In addition to a platform equipped with video cameras and remote sensors, it carried two landing capsules and two penetrators. The latter were six-foot-long darts that would be launched from orbit to embed themselves in the ground of Mars. Just before impact, they would split into two sections: the nose cone would be buried deep, while the aft section \u2014 connected by a series of cables \u2014 would remain on the surface. The bow contained a seismometer, thermal meters and mineralogy analyzers, all of them capable of surviving a crash of up to 186 miles per hour; their measurements would reach the orbital vehicle through the transmitter installed in the rear segment of the probe.In the end, however, this plan couldn\u2019t be put into action. The rocket\u2019s final stage failed and the probe \u2014 after a day spent trapped in an incorrect orbit around the Earth \u2014 disintegrated upon re-entering the atmosphere.The series of failures continued, this time under the Japanese flag. Nozomi was a small vehicle, whose instruments had been co-designed by Japan and four foreign space agencies. Its objective \u2014 like that of its predecessors \u2014 was also to orbit around Mars so as to analyze its terrain, atmosphere and neighboring interplanetary environment.The most novel thing was Nozomi\u2019s trajectory. To reach escape velocity without much fuel expenditure, it was made to pass close to the Moon twice and once near the Earth itself, accelerating more thanks to its gravitational pull. The maneuver took six months of navigation, but it was successful. In late-December of 1998 \u2014 with a little help from its engine \u2014 it set off for Mars.Unfortunately, another poorly-closed valve would cause a loss of valuable fuel needed for the final course adjustments. The Japanese technicians were forced to recalculate the trajectory to accelerate the deep space probe without wasting propellant. It passed in front of the Earth twice more, but each maneuver meant going around the Sun again. This resulted in four more years of travel. An odyssey.A solar flare ended up damaging the communications equipment and the heater control. The remaining hydrazine in the supply lines froze. Only by carefully managing the ship\u2019s orientation to take advantage of the Sun\u2019s heat could the technicians \u2014 monitoring the spacecraft",
    "comments": [],
    "description": "In this excerpt from his book, journalist Rafael Clemente recounts the amazing history of space exploration  ",
    "document_uid": "d54169bec5",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961654",
    "title": "Is this thing on? Using OpenBMC and ACPI power states for reliable server boot",
    "url": "https://blog.cloudflare.com/how-we-use-openbmc-and-acpi-power-states-to-monitor-the-state-of-our-servers/",
    "score": 3,
    "timestamp": "2024-10-27T12:18:59",
    "source": "Hacker News",
    "content": "At Cloudflare, we provide a range of services through our global network of servers, located in 330 cities worldwide. When you interact with our long-standing application services, or newer services like Workers AI, you\u2019re in contact with one of our fleet of thousands of servers which support those services.These servers which provide Cloudflare services are managed by a Baseboard Management Controller (BMC). The BMC is a special purpose processor \u2014 different from the Central Processing Unit (CPU) of a server \u2014 whose sole purpose is ensuring a smooth operation of the server.Regardless of the server vendor, each server has this BMC. The BMC runs independently of the CPU and has its own embedded operating system, usually referred to as firmware. At Cloudflare, we customize and deploy a server-specific version of the BMC firmware. The BMC firmware we deploy at Cloudflare is based on the Linux Foundation Project for BMCs, OpenBMC. OpenBMC is an open-sourced firmware stack designed to work across a variety of systems including enterprise, telco, and cloud-scale data centers. The open-source nature of OpenBMC gives us greater flexibility and ownership of this critical server subsystem, instead of the closed nature of proprietary firmware. This gives us transparency (which is important to us as a security company) and allows us faster time to develop custom features/fixes for the BMC firmware that we run on our entire fleet.In this blog post, we are going to describe how we customized and extended the OpenBMC firmware to better monitor our servers\u2019 boot-up processes to start more reliably and allow better diagnostics in the event that an issue happens during server boot-up. Server systems consist of multiple complex subsystems that include the processors, memory, storage, networking, power supply, cooling, etc. When booting up the host of a server system, the power state of each subsystem of the server is changed in an asynchronous manner. This is done so that subsystems can initialize simultaneously, thereby improving the efficiency of the boot process. Though started asynchronously, these subsystems may interact with each other at different points of the boot sequence and rely on handshake/synchronization to exchange information. For example, during boot-up, the UEFI (Universal Extensible Firmware Interface), often referred to as the BIOS, configures the motherboard in a phase known as the Platform Initialization (PI) phase, during which the UEFI collects information from subsystems such as the CPUs, memory, etc. to initialize the motherboard with the right settings. Figure 1: Server Boot ProcessWhen the power state of the subsystems, handshakes, and synchronization are not properly managed, there may be race conditions that would result in failures during the boot process of the host. Cloudflare experienced some of these boot-related failures while rolling out open source firmware (OpenBMC) to the Baseboard Management Controllers (BMCs) of our servers. Baseboard Management Controller (BMC) as a manager of the host A BMC is a specialized microprocessor that is attached to the board of a host (server) to assist with remote management capabilities of the host. Servers usually sit in data centers and are often far away from the administrators, and this creates a challenge to maintain them at scale. This is where a BMC comes in, as the BMC serves as the interface that gives administrators the ability to securely and remotely access the servers and carry out management functions. The BMC does this by exposing various interfaces, including Intelligent Platform Management Interface (IPMI) and Redfish, for distributed management. In addition, the BMC receives data from various sensors/devices (e.g. temperature, power supply) connected to the server, and also the operating parameters of the server, such as the operating system state, and publishes the values on its IPMI and Redfish interfaces. Figure 2: Block diagram of BMC in a server system.At Cloudflare, we use the OpenBMC project for our Baseboard Management Controller (BMC).Below are examples of management functions carried out on a server through the BMC. The interactions in the examples are done over ipmitool, a command line utility for interacting with systems that support IPMI. # Check the sensor readings of a server remotely (i.e. over a network) $ ipmitool <some authentication> <bmc ip> sdr PSU0_CURRENT_IN | 0.47 Amps | ok PSU0_CURRENT_OUT | 6 Amps | ok PSU0_FAN_0 | 6962 RPM | ok SYS_FAN | 13034 RPM | ok SYS_FAN1 | 11172 RPM | ok SYS_FAN2 | 11760 RPM | ok CPU_CORE_VR_POUT | 9.03 Watts | ok CPU_POWER | 76.95 Watts | ok CPU_SOC_VR_POUT | 12.98 Watts | ok DIMM_1_VR_POUT | 29.03 Watts | ok DIMM_2_VR_POUT | 27.97 Watts | ok CPU_CORE_MOSFET | 40 degrees C | ok CPU_TEMP | 50 degrees C | ok DIMM_MOSFET_1 | 36 degrees C | ok DIMM_MOSFET_2 | 39 degrees C | ok DIMM_TEMP_A1 | 34 degrees C | ok DIMM_TEMP_B1 | 33 degrees C | ok \u2026 # check the power status of a server remotely (i.e. over a network) ipmitool <some authentication> <bmc ip> power status Chassis Power is off # power on the server ipmitool <some authentication> <bmc ip> power on Chassis Power Control: On Switching to OpenBMC firmware for our BMCs gives us more control over the software that powers our infrastructure. This has given us more flexibility, customizations, and an overall better uniform experience for managing our servers. Since OpenBMC is open source, we also leverage community fixes while upstreaming some of our own. Some of the advantages we have experienced with OpenBMC include a faster turnaround time to fixing issues, optimizations around thermal cooling, increased power efficiency and supporting AI inference.While developing Cloudflare\u2019s OpenBMC firmware, however, we ran into a number of boot problems.Host not booting: When we send a request over IPMI for a host to power on (as in the example above, power on the server), ipmitool would indicate the power status of the host as ON, but we would not see any power going into the CPU nor any activity on the CPU. While ipmitool was correct about the power going into the chassis as ON, we had no information about the power state",
    "comments": [],
    "description": "Cloudflare\u2019s global fleet benefits from being managed by open source firmware for the Baseboard Management Controller (BMC), OpenBMC. This has come with various challenges, some of which we discuss here with an explanation of how the open source nature of the firmware for the BMC enabled us to fix the issues and maintain a more stable fleet.",
    "document_uid": "e305a722f3",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "frfdc8",
    "title": "Jia Tanning Go code",
    "url": "https://www.arp242.net/jia-tan-go.html",
    "score": 15,
    "timestamp": "2024-10-28T11:06:39.000-05:00",
    "source": "Lobsters",
    "content": "The Go compiler skips files ending with _test.go during normal compilation. They are compiled with go test command (together will all other .go files), which also inserts some chutney to run the test functions. The standard way to do testing is to have a foo.go and foo_test.go file next to each other. If you have a file that appears to end with _test.go but doesn\u2019t actually end with _test.go, then it will get compiled for a regular build. For example: U+FE0E is a variation selector. These are typically very invisible. Or more traditional homoglyph trickery: Those Cyrillic letters appear virtually identical on my machine: b_t\u0435\u0455t.go / b_test.go, or as monospace: b_t\u0435\u0455t.go / b_test.go. This can also be done with zero-width space, zero-width joiner, and perhaps a few other codepoints, but variation selectors tend to be the \u201cmost invisible\u201d of them. This is pretty sneaky, something like this: var bcryptCost = bcrypt.DefaultCost func hashPassword(pwd []byte) []byte { pwd, _ := bcrypt.GenerateFromPassword(pwd, bcryptCost) return pwd } func init() { bcryptCost = bcrypt.MinCost } Is perfectly valid, but with a doctored \u201cnon-test\u201d user_test.go it will now lower the security of all passwords, and effectively inserts a backdoor. There are many variants one can think of: code that looks innocent and would probably pass many reviews, but will backdoor the codebase by weakening the security, or adds in \u201ctest users\u201d, \u201ctest keys\u201d, \u201cdefault passwords\u201d, and things like that. Who is carefully auditing tests for security in the first place? I\u2019ve audited a bunch of packages over the years, but I\u2019ve never paid much attention to the test files. Most tools don\u2019t display this: Vim, VSCode, macOS finder, Windows Explorer, /bin/ls, GitHub, GitLab, etc. \u2013 they all just display user_test.go with no indication there\u2019s something more there. Take a look at the test repo; all seems fairly innocent. Arguably, that\u2019s how it should be, at least for some of these programs. Variation selectors are a Unicode feature and required to display certain types of text. That said, filenames are not really \u201cnormal text\u201d, certainly not in the context of code. The major exception is the git CLI with core.quotePath=true (the default), which displays the byte sequences for anything that\u2019s not ASCII. You need to enable that setting to have any non-ASCII path display correctly, and depending on locality it\u2019s probably a somewhat common setting. And how many people use the git CLI to review files (instead of GitHub web UI, VSCode integration, etc?) I\u2019m a pretty heavy git CLI user, but typically don\u2019t use \u201cgit diff\u201d or \u201cgit log\u201d for viewing differences between releases, and even if I did, there\u2019s a good chance I\u2019d miss this in a \u201cgit diff\u201d. Another way to detect this is to carefully pay attention to the URL in e.g. GitHub when viewing files, which displays escapes for some \u2013 but not all \u2013 of the variants. But this isn\u2019t displayed in the PR view, only when browsing files, and is very easy to miss. I reported this to GitHub, GitLab, and BitBucket back in June. None of them considered this to be a security issue. I don\u2019t really understand why, because it doesn\u2019t seem too dissimilar to LTR trickery to hide code. Also GitHub will display a warning for PRs with this sort of trickery: The head ref may contain hidden characters: \"a\\uFE0Eb\" Crafted branch names is an issue but crafted files are not? Well okay \ud83e\udd37 Also emailed the security@golang.org, but they never got back to me, so I guess they don\u2019t consider it an issue either. Is it viable to do a real-world attack with this? I don\u2019t know, I haven\u2019t tried. I am not employed at the University of Minnesota so I don\u2019t go around sending malicious patches just to see what would happen. But if I were tasked to Jia Tan a Go codebase, this seems a promising method. There\u2019s very little obfuscation and with a bit of careful design from a malicious actor everything can seem legitimate test code that\u2019s there for valid reasons, being only malicious because it gets compiled in the regular program, and because it still gets compiled in the test program the tests will still work. Seems like a great way to hide malicious code in plain sight. Doing a full Jia Tan and compromising ssh is still tricky, because that requires doing the sort of stuff that typically has no business being in a test file. That\u2019s why in xz it was hidden in binary test data. Still, with the right project and a bit of creativity I could envision this as a step in a similar scheme, especially when done by the maintainer itself. Feedback Contact me at martin@arp242.net or GitHub for feedback, questions, etc.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "605b664d56",
    "ingest_utctime": 1730142827
  },
  {
    "original_id": "mbdbe0",
    "title": "The Open Source AI Definition \u2013 1.0",
    "url": "https://opensource.org/ai/open-source-ai-definition",
    "score": 1,
    "timestamp": "2024-10-28T12:50:35.000-05:00",
    "source": "Lobsters",
    "content": "To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions. The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network. The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user. The technical storage or access that is used exclusively for statistical purposes. The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you. The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.",
    "comments": [
      {
        "author": "kylewlacy",
        "text": "<p>I have mixed feelings about this definition. It\u2019s at least better than the status quo, where you see companies both big and small haphazardly throwing model weights out in the world and calling them \u201cOpen Source\u201d, despite them not being\u2026 source code. I\u2019ve also seen the term \u201copen weights\u201d, but as far as I can tell, this isn\u2019t rigorously defined anywhere, so usually it just means \u201cyou can download the weights\u201d but anything beyond that is a coin toss</p>\n<p>Then, there\u2019s the fact that it <em>doesn\u2019t</em> require the data to be provided, just described in detail such that someone could reproduce the results. This is practically sensible, since as best as I can tell, <em>no one</em> is releasing the full data sets for their state-of-the-art models. But it still feels like a very weak stance (personally, I feel like training data sources is one of the biggest ethical problems for LLMs / AI models right now, and my impressions from the sidelines is that it\u2019s an open secret that basically all the competitive players in the space scrape data that they don\u2019t really have the right to train on). The FAQ provides <a href=\"https://hackmd.io/@opensourceinitiative/osaid-faq#Why-do-you-allow-the-exclusion-of-some-training-data\" rel=\"ugc\">a justification</a> for this, but I\u2019m still not swayed</p>\n",
        "time": "2024-10-28T13:02:55.000-05:00"
      }
    ],
    "description": "Endorse the Open Source AI Definition: have your organization appended to the list of supporters of version 1.0 version 1.0 Preamble Why we need Open Source Artificial Intelligence (AI) Open\u2026",
    "document_uid": "3bcbc4117c",
    "ingest_utctime": 1730142827
  },
  {
    "original_id": "41962100",
    "title": "The twins who created their own language",
    "url": "https://www.bbc.com/future/article/20241025-do-twins-have-a-secret-language",
    "score": 1,
    "timestamp": "2024-10-27T13:31:03",
    "source": "Hacker News",
    "content": "'It's a unique language spoken by two people': The twins who created their own languageMatthew and Michael Youlden/ Superpolyglotbros(Credit: Matthew and Michael Youlden/ Superpolyglotbros)Up to 50% of twins develop their own communication pattern with one another. Most lose it over time, but for the Youlden twins it has become a normal way of communicating.Twins Matthew and Michael Youlden speak 25 languages each. The 26th is Umeri, which they don't include in their tally. If you've not heard of Umeri, there's good reason for that. Michael and Matthew are the only two people who speak, read and write it, having created it themselves as children.The brothers insist Umeri isn't an intentionally secret language.\"Umeri isn't ever reduced to a language used to keep things private,\" they say in an email. \"It definitely has a very sentimental value to us, as it reflects the deep bond we share as identical twins.\"An estimated 30-50% of twins develop a shared language or particular communication pattern that is only comprehensible to them, known as cryptophasia. The term translates directly from Greek as secret speech.Nancy Segal, director of the Twin Studies Center at California State University, believes there are now better and more nuanced words for the phenomenon, and prefers to use \"private speech\". In her book Twin Mythconceptions, Segal also uses the phrase \"shared verbal understanding\" to refer to speech used within the pair.\"Based on available studies, it is safe to say that about 40% of twin toddlers engage in some form of 'twin-speak',\" writes Segal. \"But that figure does not convey just how complex twins' language development turns out to be.\"Umeri is now written using the Latin alphabet, though the Youlden twins tried to design their own alphabet for the languageRoy Johannink from the Netherlands is father to teenage twins Merle and Stijn. Thirteen years ago, when they were babies, he took a video of them babbling to one another and shared it on YouTube. To date, their conversation has had over 30 million views. Johannink happened to have his camera on hand at the moment the two first began to verbally interact with each other.\"I was a little surprised that they saw each other,\" remembers Johannink. \"They thought: 'Hey, I'm not alone in this moment. There's another one of me! It's us against the world.'\"Segal explains that like Merle and Stijn (who went on to lose their shared language when they learnt Dutch), most twins outgrow their private words as they gain more exposure to other people beyond the home.But for the Youlden twins, this wasn't the case. They didn't outgrow their language. Quite the contrary, they enriched and perfected it over the years.Matthew and Michael Youlden/ SuperpolyglotbrosTwins Matthew and Michael Youlden developed their own language as children, which they speak to this day (Credit: Matthew and Michael Youlden/ Superpolyglotbros)Born and raised in Manchester in the UK, the Youlden twins grew up surrounded by different ethnicities and cultures, fostering a love of languages.Memories of when Umeri first began are hazy, but the brothers remember their grandfather being confused when as pre-schoolers, the two would share a joke between themselves he would not understand.Then came their first family holiday abroad, at the age of eight. They were headed to Spain and decided they were going to learn Spanish, convinced that if they didn't, they'd struggle to order ice cream. Armed with a dictionary and with little understanding of how the grammar worked, they began to translate phrases word for word from English into Spanish. Later they took on Italian, and then turned their attention to learning Scandinavian languages. Pooling together various grammatical elements of all the languages they had studied, the brothers realised Umeri could actually become a fully-fledged language itself.This chimes with Segal's observations. According to her, in general, \"twins do not invent a new language, they tend to produce atypical forms of the language they are exposed to. Even though it's unintelligible, they still direct it to other people\".The Youlden twins began to standardise and codify Umeri. At one point, they even tried to design their own alphabet but realised (when they got their first computer) it would be of little use considering there was no Umeri font. Umeri is now written using the Latin alphabet.Shared languagePreserving a language spoken by few people comes with its own challenges, however.\"Twins have this shared language, that at some point they stop using, as if they feel ashamed of it,\" says Matthew. \"This is also not something unique to twin languages.\"Anyone speaking a minority language \u2013 meaning a language not shared by much of the rest of society \u2013 may grow shy of speaking it, \"especially if you are raised with a minority language where you are maybe ostracised or looked at funnily at school,\" he says. \"We thankfully never had that [reaction from others].\" On the contrary, in the Youlden home, their parents never saw the development of Umeri between the brothers as a negative thing.LET'S TALKLet's Talk is a BBC series exploring the wonder and mystery of languages.When the brothers would swerve off to converse in their own language when with extended family, the response tended to be \"they're off doing the language thing again\", recalls Matthew.Karen Thorpe is a specialist in child development, education and care research at the Queensland Brain Institute at the University of Queensland. She has in previous roles extensively studied language development in twins.\"For me, it's about a very close relationship,\" she says. \"Rather than seeing it as something strange and unusual, private language is really about a beautiful thing that humans do when they're very, very close to one another. But is that exclusive to twins? I don't think so. I think it's exclusive to very special, close relationships.\"She also regards it as a normal development feature. As she put it in a 2010 research paper: \"It is simply that young children who are just beginning to speak tend to understand each other rather better than do their parents or other adults.\"For others, such as the Youldens, the",
    "comments": [],
    "description": "Up to 50% of twins develop their own communication pattern with one another. Most lose it over time, but for the Youlden twins it has become a normal way of communicating.",
    "document_uid": "c95147bb70",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41969839",
    "title": "Golang developers should try Odin",
    "url": "https://rm4n0s.github.io/posts/2-go-devs-should-learn-odin/",
    "score": 1,
    "timestamp": "2024-10-28T12:06:37",
    "source": "Hacker News",
    "content": "(BEWARE: the article may cause seizures) Introduction I\u2019ve been using Golang for 10 years, and even though I tried other programming languages, I always went back to it because of its simplicity. It is so simple that reading the source code and playing around helped me pick up the majority of the syntax when I first got into Golang. However, it pains me to say that Golang has problems because of its garbage collector and runtime. Some of the problem are listed here: CGO It runs and compiles slowly You have to know and write C in comments!! It can not compile to WASM The language is slow compared to Rust It is easy to have memory leaks and they are hard to find. The error type is not good But even with these problems, I still prefer Golang to other programming languages, until I met Odin a week ago, and it was love at first sight. This article will show you the beauty of Odin. What is Odin and who is it for Odin is a new programming language that is 60% Golang, 20% features that you wished Golang had 10% array programming 10% low level, FFI, memory management People think that it is only for game development just because it includes 3D libraries with the compiler, but I disagree. Odin is also for backend development because it has a core library similar to Go\u2019s standard library. Which means that it has all the building blocks for us to copy the rest of Go\u2019s libraries like Chinese manufacturers. I know that some of you play around with Rust, Zig, C3 or Hare, but these do not have array programming and struct tags in their languages. Just ask yourself: how will you serialize data between DBs and markup languages without struct tags? how will you write a server side anticheat without array programming? Don\u2019t you want to make linux gamers happy? It even has quaternions! Do you know what that is? Me neither! But I am excited to learn. Odin is ready The language will not change and it will stay stable until you retire. However, the compiler, toolchain, and core library are still in development and always improved. Also, it is used in production from JangaFX and ChiAha\u2122 The characteristics of Odin The language has 31 keywords, but each keyword may have a #directive or @(attribute). It may seem too much for someone that cannot remember more than 26 keywords like me, but all the keywords, directives, and attributes connect so seamlessly together like reading English. For example, the switch-case that works like in Golang requires from the user to specify every case, unless you put the #partial directive in front of it. It will really make sense to you how easy it is for you to understand the code without knowing the language. Take 10 minutes to read demo.odin, it\u2019s got almost everything you need to get started. Yes, it does not have documentation (no, the overview is not a real documentation), but that is the beauty of it. You learn to code in Odin by reading other\u2019s people code. Most of the standard library does not have comments or examples, but you can easily understand how to use it through its source code. No, it does not have macros, or comptime, or constexpr or decorators and it is better that way because every language that has them makes developers to waste 90% of their time googling for every library that uses them. Trust me, stick with Odin\u2019s pre-defined directives and attributes. Yes, it has generics, and they are more well-designed than Golang\u2019s generics. No, it does not have a package manager, or go.mod file or anything else. The package is just a directory that you just copy to your project, and you call it relative to the path of the source code file. For example, // git clone https://github.com/laytan/odin-http // vim main.odin package main import http \"odin-http\" Yes, compiler is as simple as Golang\u2019s compiler. cd myproject odin build . odin run main.odin -file Yes, it has VSCode plugin with autocompletion. The trade offs Odin has manual memory management, which is why it does not have Closures, Composition, Goroutines, and Selectors. However, I will demonstrate how you can live without them. Second, it lacks libraries, which is why I\u2019m trying to get you to learn it and get your libraries from Go to Odin. Lastly, it does not have documentation or books, but if you have years of experience in Go, then Odin will feel like home. Living without a garbage collector Error handling Living without a garbage collector means that you don\u2019t have errors.New() or fmt.Errorf(). There is no error type in Odin. You only have Enums, Unions and Structs, and that is better than Go\u2019s errors. Go\u2019s errors suck Golang\u2019s errors makes you push server-side error messages to users, and you cannot do anything to stop it. That is not a good thing because users don\u2019t need to know what is SQL or that the bank_account is nil. Furthermore, another problem is that Go\u2019s errors do not have stack traces. Now look at the superpower of Odin For each function you create, also include an enum or a union or a struct to represent the error for that specific function. Payment_Error :: enum { None, Bank_Account_Is_Empty, } Pay_Half_Debt_Error :: union #shared_nil { Payment_Error, json.Marshal_Error, } Save_House_Error :: union #shared_nil { Pay_Half_Debt_Error, Is_Even_Worth_Saving_Error, } // etc When a function returns an error that it received from another function, and so forth, you will receive a type similar to this. err := Extend_Deadline_Error( Save_House_Error( Pay_Half_Debt_Error( Payment_Error.Bank_Account_Is_Empty ) ), ) From there you can create trees of switch statements that return proper user messages. #partial switch save_house in err { case Save_House_Error: pay_half := save_house.(Pay_Half_Debt_Error) #partial switch pay in pay_half { case Payment_Error: #partial switch pay { case .Bank_Account_Is_Empty: fmt.println(\"You have 24 hours to leave the house.\") } } } And also you can print it like a",
    "comments": [],
    "description": "",
    "document_uid": "b73841193f",
    "ingest_utctime": 1730115433
  },
  {
    "original_id": "d0h69f",
    "title": "An Ode To Vim",
    "url": "https://bokwoon.com/posts/1khtfep-an-ode-to-vim/",
    "score": 32,
    "timestamp": "2024-10-26T15:22:33.000-05:00",
    "source": "Lobsters",
    "content": "An Ode To Vim This website you see right now, as well as the CMS powering it, is all thanks to Vim. Not because I wrote it in Vim (which I did, in Neovim) but because coming into contact with Vim as a freshman likely kickstarted my programming career and gave me programming edge that I otherwise would not have leaving university. I took my first computer science lesson (CS1010) back in 2016 at the National University of Singapore. The professor was teaching us how to write and compile a C program, and for some reason we had to use Vim. I remember thinking it was archaic. You could only move around with arrow keys because the mouse didn\u2019t work. There were a million commands to learn each which did their own little thing. I remember learning that \"dd\" deleted the current line. This software was obviously something that people only used in the past back when everything was more primitive. Imagine my shock when, after class, I Googled about this outdated text editor and saw nothing but universal acclaim for Vim. Stack Overflow, blog posts, everything. It shifted my perspective 180\u00b0 on this editor. I was delighted to find I could run it locally and quickly learnt that you could apply basic customizations to it so that it didn't look like a text editor from the 1990s. A TANGENT: It is crucial that I was running macOS (then OS X). A Computer Science senior laughed when I said I was thinking of getting a Windows laptop for university because Windows supported more programs. He straight up told me that MacBooks were better for programming. I did not realize what he meant at first, but after getting my first MacBook and being exposed to the unix command line environment I finally understood. Thank you, Jeremiah. Your offhand remark likely changed my career, because without the MacBook I would have never used Vim locally (only through SSH-ing into the university's servers), and without Vim I would have never been exposed to the command line environment. I would have stayed a blind Windows user programming exclusively on Java IDEs and never \"getting it\". From there I started using Vim for all my lab exercises, and it was through practicing Vim with these exercises that I found more inadequacies with my Vim setup and the config lines needed to remedy it. I did not read vimtutor. Nor did I do any \"learn Vim now\" exercises to build muscle memory of the shortcuts. I simply started with a basic set of shortcuts (\"i\" to enter insert mode, \"esc\" to enter normal mode, \"hjkl\" to move around) and layered on more as I encountered tedious and repetitive situations that I solved by Googling for and learning more about other vim shortcuts. I would recommend beginners learn Vim this way, instead of all at once. Using Vim exposed me to the command line, and the command line exposed me to a whole host of other programmer-related things. I did not intend on learning shell scripting, but I picked that up by slow osmosis after reading about it a million times from vim-related stackoverflow answers and articles. I read about the grep command for two years without knowing what it did or how to use it before I finally tried it (for those who don't know, grep is like CTRL+F for lines of text). I never took a formal course on unix command line fundamentals, yet simply using Vim day-to-day was enough to make me learn everything I needed to know through effortless, natural progression. So thanks, Vim Without Vim I would have gone through the motions of a computer engineering student and graduated as someone with bad grades. With Vim I still left university as someone with bad grades, but with a passion for programming.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "f17d41b63b",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "kuhlap",
    "title": "OpenZFS deduplication is good now and you shouldn't use it",
    "url": "https://despairlabs.com/blog/posts/2024-10-27-openzfs-dedup-is-good-dont-use-it/",
    "score": 13,
    "timestamp": "2024-10-28T01:20:17.000-05:00",
    "source": "Lobsters",
    "content": "27 October, 2024 OpenZFS deduplication is good now and you shouldn't use it OpenZFS 2.3.0 will be released any day now, and it includes the new \u201cFast Dedup\u201d feature. My team at Klara spent many months in 2023 and 2024 working on it, and we reckon its pretty good, a huge step up from the old dedup as well as being a solid base for further improvements. I\u2019ve been watching various forums and mailing lists since it was announced, and the thing I kept seeing was people saying something like \u201cit has the same problems as the old dedup; needs too much memory, nukes your performance\u201d. While that was true (ish), and is now significantly less true, the real problem is that this just repeating the same old non-information that they probably heard from someone else repeating it. I don\u2019t blame anyone really; it is true that dedup has been extremely challenging to get the best out of, it\u2019s very difficult to find good information about using it well, and \u201cdon\u2019t use it\u201d was and remains almost certainly the right answer. But, with this being the first time in almost two decades that dedup has been worth even considering, I want to get some fresh information out there about the what dedup is, how it worked traditionally and why it was usually bad, what we changed with fast dedup, and why it\u2019s still probably not the thing you want. Table of contents What even is dedup? \ud83d\udd17 Dedup can be easily described in a sentence. When OpenZFS prepares to write some data to disk, if that data is already on disk, don\u2019t do the write but instead, add a reference to the existing copy. The challenge is all in how you determine whether or not the data is already on disk, and knowing where on disk it is. The reason it\u2019s challenging is that that information has to be stored and retrieved, which is additional IO that we didn\u2019t have to do before, and that IO can add surprising amounts of overhead! This stored information is the \u201cdedup table\u201d. Conceptually, it\u2019s hashtable, with the data checksum as the \u201ckey\u201d and the on-disk location and refcount as the \u201cvalue\u201d. It\u2019s stored in the pool as part of the pool metadata, that is, it\u2019s considered \u201cstructural\u201d pool data, not user data. How does dedup work? \ud83d\udd17 When dedup is enabled, the \u201cwrite\u201d IO path is modified. As normal, a data block is prepared by the DMU and handed to the SPA to be written to disk. Encryption and compression are performed as normal and then the checksum is calculated. Without dedup, the metaslab allocator is called to request space on the pool to store the block, and the locations (DVAs) are returned and copied into the block pointer. When dedup is enabled, OpenZFS instead looks up the checksum in the dedup table. If it doesn\u2019t find it, it calls out to the metaslab allocator as normal, gets fresh DVAs, fills the block and lets the IO through to be written to disks as normal, and then creates a new dedup table entry with the checksum, DVAs and the refcount set to 1. On the other hand, if it does find it, it copies the DVAs from the the value into the block pointer and returns the writing IO as \u201ccompleted\u201d and then increments the refcount. Blocks allocated with dedup enabled have a special D flag set on the block pointer. This is to assist when it comes time to free the block. The \u201cfree\u201d IO path is similarly modified to check for the D flag. If it exists, the same dedup table lookup happens, and the refcount is decremented. If the refcount is non-zero, the IO is returned as \u201ccompleted\u201d, but if it reaches zero, then the last \u201ccopy\u201d of the block is being freed, so the dedup table entry is deleted and the metaslab allocator is called to deallocate the space. So all this is working, in that OpenZFS is avoiding writing multiple copies of the same data. The downside is that every single write and free operation requires a lookup and a then a write to the dedup table, regardless of whether or not the write or free proper was actually done by the pool. It should be clear then that any dedup system worth using needs to save more in \u201ctrue\u201d space and IO than it spends on the overhead of managing the table. And this is the fundamental issue with traditional dedup: these overheads are so outrageous that you are unlikely to ever get them back except on rare and specific workloads. Why is traditional dedup so bad? \ud83d\udd17 All of the detail of dedup is in how the table is stored, and how it interacts with the IO pipeline. There\u2019s three main categories of problem with the traditional setup: the construction and storage of the dedup table itself the overheads required to accumulate and stage changes to the dedup table the problem of \u201cunique\u201d entries in the table The dedup table \ud83d\udd17 Traditional dedup implemented the dedup table in probably the simplest way that might work: it just hooked up the standard OpenZFS on-disk hashtable object and called it a day. This object type is a \u201cZAP\u201d, and it\u2019s used throughout OpenZFS for file directories, property lists and internal housekeeping. It\u2019s an entirely reasonable choice. It\u2019s also really not well suited to an application like dedup. A ZAP is a fairly complicated structure, and I\u2019m not going to get into it here. For our purposes, it\u2019s enough to know that each data block in a ZAP object is an array of fixed-size \u201cchunks\u201d, with a single key/value consuming as many chunks as are needed to hold the key, the data, and and a header describing how the chunks are being used. A dedup entry has a 40-byte key. The value part can be up to 256 bytes, however this is compressed before storing it, so lets assume",
    "comments": [],
    "description": "OpenZFS 2.3.0 will be released any day now, and it includes the new \u201cFast Dedup\u201d feature. My team at Klara spent many months in 2023 and 2024 working on it, and we reckon its pretty good, a huge step up from the old dedup as well as being a solid base for further improvements.",
    "document_uid": "28d75f0ae5",
    "ingest_utctime": 1730115449
  },
  {
    "original_id": "6htemr",
    "title": "A comparison of Rust\u2019s borrow checker to the one in C#",
    "url": "https://em-tg.github.io/csborrow/",
    "score": 15,
    "timestamp": "2024-10-27T12:04:38.000-05:00",
    "source": "Lobsters",
    "content": "A comparison of Rust\u2019s borrow checker to the one in C# Wait, C# has a borrow checker? Behold: the classic example of rust\u2019s zero-cost memory safety\u2026 // error[E0597]: `shortlived` does not live long enough let longlived = 12; let mut plonglived = &longlived; { let shortlived = 13; plonglived = &shortlived; } *plonglived; \u2026ported to C#: // error CS8374: Cannot ref-assign 'shortlived' to 'plonglived' because // 'shortlived' has a narrower escape scope than 'plonglived' var longlived = 12; ref var plonglived = ref longlived; { var shortlived = 13; plonglived = ref shortlived; } _ = plonglived; OK, so C# doesn\u2019t share the Rust concept of \u201cborrowing,\u201d so it wouldn\u2019t technically be correct to call this \u201cborrow checking,\u201d but in practice when people talk about \u201cRust\u2019s borrow checker\u201d they\u2019re talking about all of the static analysis Rust does to ensure memory safety, for which I think this qualifies. When I first saw this feature in C# (and also Spans, ref structs, and stackalloc), I was blown away: where are all the angle brackets and apostrophes? How is it possible that I can write efficient and provably-safe code in C# without a degree in type theory? In this document I hope to briefly summarize my understanding of memory safety in C#, make direct comparisons between C# constructs and the corresponding Rust ones, and maybe shed some light on what trade-offs C# made exactly to get this so user-friendly. A brief history of C# ref safety Since the beginning (2000-ish), C# has had the ref keyword for parameters passed into a function by reference, but that was about all you could do with it. If you wanted to do efficient things with stack-allocated memory and indirection, you would generally use the \u201cunsafe\u201d portions of the language, or call out to C++. It wasn\u2019t until 2017 with the release of C# version 7 that we started to see this feature generalized into something more useful. From there, C# added: ref local variables ref returns safe stackalloc initializers readonly struct and ref struct in parameters (and later ref readonly parameters) conditional ref expressions extensions to stackalloc ref fields In the process of adding the above features, C# needed to define rules around ref usage that would continue to ensure memory safety. The language specification calls these rules \u201cref safe contexts\u201d (see here and here). A \u201cref safe context\u201d is likely better-known to Rust programmers as a lifetime, the region of source text in which it is valid to access/use a reference. A comparison of ref safe contexts and lifetimes Like in Rust, it is not possible in C# to explicitly declare the lifetime of a value. Unlike in rust, it is also not possible in C# to assign a name to a lifetime using generic type parameters. In both languages, the correct usage of a function must be knowable given only its declaration, and not require analysis of its body. In Rust, this means that lifetimes must appear in the function declaration: // V name the lifetime using generic type parameter // V --------- V reference named lifetime in parameter and return types fn return_reference<'a>(r: &'a i32) -> &'a i32 { r // <-- compiler makes sure we return what we claim to in the signature } C# has no syntax for this. Nevertheless, the equivalent code still compiles: // No lifetimes! ref int ReturnReference(ref int r){ return ref r; } The C# compiler simply assumes that the lifetime of the return is the same as the lifetime of the parameter. Rust can do the same\u2026 // No lifetimes! fn return_reference(r: &i32) -> &i32 { r } \u2026and calls this feature lifetime elision. In Rust, lifetime elision is optional, and the programmer can always explicitly state the lifetimes of all references. In C#, by contrast, the compiler must decide the lifetimes for all function declarations. For example, the following Rust function returns only one of its two arguments: // V---V Two lifetimes for our two parameters // V------------------------V Return lifetime is the same as that of // the first parameter // V Second parameter lifetime is unused fn return_reference<'a, 'b>(r: &'a i32, r2: &'b i32) -> &'a i32{ r // <-- compiler would not allow us to return r2 here } Lifetime elision is not implemented for functions of this form, since there doesn\u2019t seem to be a reasonable default to pick. Nevertheless, C# must pick one: // No lifetimes! Wait. What are they, though? ref int ReturnReference(ref int r, ref int r2){ return ref r; } Since it wouldn\u2019t make sense to \u201cpick\u201d either r or r2 for the lifetime of the return, C# conservatively assumes that the return could be either of them. Thus, both the arguments and the return are assumed to have the same lifetime, which the specification calls \u201ccaller-context.\u201d The equivalent Rust function would look like this: // V only one lifetime, called \"caller-context\" fn return_reference<'cc>(r: &'cc i32, r2: &'cc i32) -> &'cc i32{ r // <-- compiler allows us to return either r or r2 } This is less useful than the original Rust function. For example, the following code will compile successfully with the first declaration, but not with the second: fn wrapper(r: &i32) -> &i32{ let i = 12; return_reference(r, &i) // error[E0515]: cannot return value referencing local variable `i` } and in C#: ref int Wrapper(ref int r){ var i = 12; // Cannot use a result of 'Program.ReturnReference(ref int, ref int)' // in this context because it may expose variables referenced by // parameter 'r2' outside of their declaration scope return ref ReturnReference(ref r, ref i); } Here we see C#\u2019s first trade-off: lifetimes are less explicit, but also less powerful. The defaults can also be unintuitive: say we wanted to write a method on a struct which returns a reference to one of the struct\u2019s members. In rust, this is simple: struct Foo { member: i32 } impl Foo { fn get_member<'a, 'b>(&'a self, unused: &'b i32) -> &'a i32 { &self.member //",
    "comments": [
      {
        "author": "kameko",
        "text": "<p>This reminds me of when I was in the last few months of using C# before I switched to Rust, without even knowing about Rust\u2019s ownership system, I sort of tried to make my own runtime ownership system to make resource management more coherent. It was one of the things that blew me away about Rust, it was doing many things at compile time that I had independently tried to do at runtime.</p>\n<blockquote>\n<p>Why does nobody seem to be talking about this?</p>\n</blockquote>\n<p>I\u2019ve only ever seen the use of <code>struct</code> and <code>ref</code> in C# in high-performance game code. I think most people simply don\u2019t care about C#\u2018s value types because there\u2019s already so much runtime overhead, that, if you <em>are</em> actually butting up against the runtime\u2019s limitations, you probably have significantly more issues than simply avoiding a few heap allocations. As in, you either have to rewrite your entire C# program to take your particular performance bottlenecks into account, or you have to rewrite your entire program in an entirely different language without garbage collection or implicitly-inserted vtables, similarly to why Discord switched from Go to Rust.</p>\n<p>In general, I don\u2019t think I\u2019ve ever seen someone choose to use C# for performance reasons. People use C# because it\u2019s made for making applications and business logic, like Java. Performance is an oddity in managed languages, it\u2019s something \u201cnice to have\u201d but nobody actually cares about it until it becomes a problem. Another way to put it, if you\u2019re using a managed language, you\u2019re paying up-front to have a lot of decisions made for you, and many of those decisions are difficult if not impossible to undo.</p>\n",
        "time": "2024-10-27T13:06:51.000-05:00"
      },
      {
        "author": "roetlich",
        "text": "<p>I\u2019ve seen performance optimized C# code outside of game dev, and I think it has a purpose. Struct and ref are core C# features, and even newer features like Span&lt;&gt; and stackalloc aren\u2019t that rare in my experience. Sure, it\u2019s not the fastest language, but I don\u2019t think the overhead is as big as you make it seem.</p>\n<blockquote>\n<p>implicitly-inserted vtables</p>\n</blockquote>\n<p>The vtables should only matter on virtual (or abstract) methods, in classes that aren\u2019t sealed, right? They are as much \u201cimplicitly-inserted\u201d as in C++. If you enable all the linting rules with modern dotnet, it will even warn you to mark your classes as sealed whenever possible.</p>\n<p>My view of C# is very biased, but I do think it\u2019s good that C# has these features. Especially for libraries.</p>\n",
        "time": "2024-10-27T17:15:44.000-05:00"
      },
      {
        "author": "kevinc",
        "text": "<p>Is it so rare to have low- or no-syntax static analysis of storage point lifetimes for memory safety reasons? Swift does it. The Clang static analyzer did some amount of it, which was great for pre-ARC Objective-C. I guess it is a bit unexpected to find in a GC-focused language.</p>\n",
        "time": "2024-10-27T13:55:56.000-05:00"
      },
      {
        "author": "MaskRay",
        "text": "<p>This is cool. Q: How does the following snippet in the post work?</p>\n<blockquote>\n<p>By contrast, in C# heap references are never invalid, while refs can only be invalidated by exiting from a block:</p>\n</blockquote>\n",
        "time": "2024-10-27T16:05:18.000-05:00"
      },
      {
        "author": "lina",
        "text": "<p>I didn\u2019t know C# had that! That\u2019s pretty neat. It\u2019s definitely really close in practice to what Rust is doing, just with a bunch of different trade-offs.</p>\n<p>Since GC was mentioned, I think it might be worth expanding on how GCed languages can be \u201cmemory safe\u201d but still lead to buggy code. The main issue I notice with GCs is that they <em>only</em> guarantee that all your pointers are (still) valid, <em>not that they point to what you actually want</em>. A classic issue I\u2019ve run into in GCed languages is that you can take a reference to an element of an array, then mutate the array, and you might wind up with a pointer to an orphan element, or not the element you logically intended to point to at that point in the code. Rust doesn\u2019t have a GC but you can emulate similar semantics with reference counts. If you do that though, exactly what is being cloned when becomes explicit in the code: you can write exactly the same logically incorrect code, but you have to be more explicit about it so it\u2019s less likely to happen by accident.</p>\n<p>Aside: You don\u2019t actually need the <code>Box::leak(Box::new(0))</code> in the article, you can just return a reference to a statically allocated 0. Just replacing that whole line with <code>&amp;0</code> will compile fine. That only works for read-only references though, if you need to return a mutable reference you need to go back to leaking memory or a Cow-like solution (e.g. <a href=\"https://crates.io/crates/mucow\" rel=\"ugc\">https://crates.io/crates/mucow</a>, but at that point I\u2019d argue you\u2019re probably better off rethinking the API).</p>\n",
        "time": "2024-10-27T17:23:06.000-05:00"
      },
      {
        "author": "tux0r",
        "text": "<p>TIL: C# actually has an annoying feature.</p>\n",
        "time": "2024-10-27T12:22:22.000-05:00"
      }
    ],
    "description": "No description available.",
    "document_uid": "4482a3d66d",
    "ingest_utctime": 1730068529
  },
  {
    "original_id": "qt65lv",
    "title": "The Basics",
    "url": "https://registerspill.thorstenball.com/p/the-basics",
    "score": 24,
    "timestamp": "2024-10-26T07:10:35.000-05:00",
    "source": "Lobsters",
    "content": "Here\u2019s what I consider to be the basics. I call them that not because they\u2019re easy, but because they\u2019re fundamental. The foundation on which your advanced skills and expertise rest. Multipliers and nullifiers, makers and breakers of everything you do.They don\u2019t usually show up in technical books and yet without them a lot of brilliant effort can go to waste. I constantly have to remind myself of them, sitting on my own shoulder and wagging a finger in my face.Here they are:Test it manually. Even if you wrote and ran automated tests, test it manually at least once before you ask for a review. Record yourself testing it manually. You\u2019ll be surprised by what you find.Think through the edge cases. That doesn\u2019t mean you have to handle them all right away, but you should have an answer to them.Keep your change free of unrelated changes.Make sure that your PR is up-to-date against latest on your main branch.Before you comment on something, read the whole thing.Do the homework before the meeting. You\u2019ll stand out.Understand what problem you\u2019re solving. Knowing why you\u2019re doing something is a requirement to knowing whether you\u2019re actually solving the problem.Accept that sometimes you\u2019ll have to do things that you don\u2019t find interesting or exciting or that don\u2019t bring you joy. Sometimes it\u2019s just work.Write bug reports that are clear and understandable. Don\u2019t write \u201cit doesn\u2019t work for me.\u201d Give the reader information on what you did, what you expected to happen, what happened. Think about what might be useful to debug this, then put it in the ticket.Bonus points: try to find a minimal set of steps to reproduce before you open a ticket.Read the error messages.Read the manual, the docs, the instructions, the ticket.Be on time.Know why your fix is a fix.Every time you add a test, actually test that it would fail. Yes, literally: go and comment out the code that you think makes the test pass, then run the test, see it fail, comment the code back in, run test again, see it succeed. Only then have you written a test.Do what you said you\u2019ll do. If for some reason you can\u2019t, let the person assuming you\u2019re doing something know about it.Don\u2019t ask for \u201cquick\u201d reviews when you never review other people\u2019s code.Always keep an eye out for bugs.Make it a goal that people want to work with you.",
    "comments": [
      {
        "author": "telemachus",
        "text": "<p>I don\u2019t understand what the author means by \u201cRecord yourself testing it manually.\u201d</p>\n<ul>\n<li>\n<em>Take notes as you test it manually</em>?</li>\n<li>\n<em>Make a screen recording as you test it manually</em>?</li>\n<li>Something else?</li>\n</ul>\n<p>This is not necessarily a huge deal, and I like the idea of testing things manually no matter what other kinds of tests you have. But I wonder how other people understand the \u201crecording\u201d idea.</p>\n",
        "time": "2024-10-26T09:12:09.000-05:00"
      },
      {
        "author": "kosayoda",
        "text": "<p><a href=\"https://malisper.me/how-to-improve-your-productivity-as-a-working-programmer/\" rel=\"ugc\">This article from Michael Malis immediately came to mind</a>, specifically the <em>Watching Myself Code</em> section:</p>\n<blockquote>\n<p>One incredibly useful exercise I\u2019ve found is to watch myself program. Throughout the week, I have a program running in the background that records my screen. At the end of the week, I\u2019ll watch a few segments from the previous week. Usually I will watch the times that felt like it took a lot longer to complete some task than it should have. While watching them, I\u2019ll pay attention to specifically where the time went and figure out what I could have done better.</p>\n</blockquote>\n",
        "time": "2024-10-26T14:52:00.000-05:00"
      },
      {
        "author": "andyc",
        "text": "<p>I don\u2019t know specifically what he was referring to, but it\u2019s a good idea to start with manual testing, and gradually moving to semi-automation / full automation.  There should be a spectrum, not a hard line encouraged by clunky tools / frameworks.</p>\n<p>Manual testing doesn\u2019t scale, but it\u2019s where you should start.  In particular, you shouldn\u2019t write nonsense unit tests tests that don\u2019t verify the properties you want.   (And I believe that tests generated with LLMs are susceptible to this problem.  Or you could say the users of LLMs are likely to generate tests with this issue.)</p>\n<hr>\n<p>There\u2019s a brand of \u201crecord and replay\u201d testing that I would call it Unix-style or shell-style testing.  You can make a representation/language for your program state and then you can automate it with an out-of-process (\u201cexterior\u201d) harness.</p>\n<p>The Art of Unix Programming talks about this (maybe not that clearly, you\u2019ll have to read the surrounding context):</p>\n<p><a href=\"http://www.catb.org/esr/writings/taoup/html/ch06s02.html\" rel=\"ugc\">http://www.catb.org/esr/writings/taoup/html/ch06s02.html</a></p>\n<p><a href=\"https://www.amazon.com/UNIX-Programming-Addison-Wesley-Professional-Computng/dp/0131429019\" rel=\"ugc\">https://www.amazon.com/UNIX-Programming-Addison-Wesley-Professional-Computng/dp/0131429019</a></p>\n<blockquote>\n<p>Another theme that emerges from these examples is the value of programs that flip a problem out of a domain in which transparency is hard into one in which it is easy. Audacity, sng(1) and the tic(1)/infocmp(1) pair all have this property. The objects they manipulate are not readily conformable to the hand and eye \u2026</p>\n</blockquote>\n<blockquote>\n<p>Whenever you face a design problem that involves editing some kind of complex binary object, the <strong>Unix tradition encourages asking first</strong> off whether you can write a tool analogous to sng(1) or the tic(1)/infocmp(1) pair that can do a lossless mapping to an editable textual format and back. There is no established term for programs of this kind, but we\u2019ll call them <strong>textualizers</strong>.</p>\n</blockquote>\n<p>I agree there is no common term for this, but there should be.</p>\n<hr>\n<p>Mitchell H mentioned this the other day as a technique for testing Ghostty:</p>\n<p><a href=\"https://lobste.rs/s/ineb98/ghostty_1_0_is_coming#c_ro3yws\" rel=\"ugc\">https://lobste.rs/s/ineb98/ghostty_1_0_is_coming#c_ro3yws</a></p>\n<blockquote>\n<p>Longer term, I\u2019m interested in making an entire interaction sequence with Ghostty <strong>configurable in text</strong> so we can use some form of DST and fuzzing to continuously hammer Ghostty.</p>\n</blockquote>\n<p>Like Audacity, a terminal emulator is not a command line program \u2013 it is a GUI.  But nothing about that makes the technique less valuable \u2013 I\u2019d actually say it makes it more valuable.</p>\n<hr>\n<p>Similar patterns:</p>\n<blockquote>\n<p>testscript, a hidden gem the Go team kept locked away</p>\n</blockquote>\n<p><a href=\"https://lobste.rs/s/elmxfh/testscript_hidden_gem_go_team_kept_locked\" rel=\"ugc\">https://lobste.rs/s/elmxfh/testscript_hidden_gem_go_team_kept_locked</a></p>\n<p>I wrote basically the same thing for Oils, our tests look like this:</p>\n<p><a href=\"https://github.com/oils-for-unix/oils/wiki/Spec-Tests\" rel=\"ugc\">https://github.com/oils-for-unix/oils/wiki/Spec-Tests</a></p>\n<pre><code>#### multiline test\necho 1\necho 2\n## status: 0\n## STDOUT:\n1\n2\n## END\n</code></pre>\n<p>If you look at the highest quality and longest-lasting  software packages, they often have bespoke test harnesses like this.  People often wrote them in Perl and the like.</p>\n<p>It doesn\u2019t really matter how you write it, but simply having this kind of test will increase quality, IME.</p>\n",
        "time": "2024-10-26T14:09:00.000-05:00"
      },
      {
        "author": "Student",
        "text": "<p>Any of these are a good idea. The point is to be able to repeat your testing. If you do a screen recording you can potentially review details you missed later (at the cost of fiddling with time based media)</p>\n",
        "time": "2024-10-26T10:45:45.000-05:00"
      },
      {
        "author": "jstoja",
        "text": "<p>My favorites right now from this list:</p>\n<ul>\n<li>Be on time.</li>\n<li>Do the homework before the meeting. You\u2019ll stand out.</li>\n</ul>\n<p>I really have to say that when you join a meeting on time and you worked on it ahead, it really is well received. That\u2019s something really underestimated, but when you see even a junior preparing and coming to a meeting ready to exchange and engage, that\u2019s amazing.</p>\n",
        "time": "2024-10-27T08:44:16.000-05:00"
      }
    ],
    "description": "Here\u2019s what I consider to be the basics.",
    "document_uid": "1739a8b952",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "41962146",
    "title": "GPM J1839\u221210",
    "url": "https://en.wikipedia.org/wiki/GPM_J1839%E2%88%9210",
    "score": 3,
    "timestamp": "2024-10-27T13:36:23",
    "source": "Hacker News",
    "content": "From Wikipedia, the free encyclopedia Neutron star in the Scutum constellation GPM J1839\u221210[1] is a potentially unique[2] ultra-long period magnetar[3][4] located about 15,000 light-years away from Earth in the Scutum constellation, in the Milky Way. It was discovered by a team of scientists at Curtin University using the Murchison Widefield Array.[5][6] Its unusual characteristics violate current theory and prompted a search of other radio telescope archives, including the Giant Metrewave Radio Telescope and the Very Large Array, which revealed evidence of the object dating back to 1988.[5] The signature of the object went unnoticed because scientists did not know to look for its unusual behavior.[5] The current understanding of neutron stars is that below a certain rate of rotation, called \"the death line\", they cease emissions. Uniquely, not only does GPM J1839\u221210 have an extremely slow rotation of approximately twenty-two minutes, it emits bursts of radio waves lasting up to five minutes, for which there is currently no generally accepted explanation.[5][4][6][7][8]",
    "comments": [],
    "description": "No description available.",
    "document_uid": "d3ebf64b12",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962607",
    "title": "Diversity buffers winegrowing regions from climate change losses \u2013 PNAS",
    "url": "https://www.pnas.org/doi/abs/10.1073/pnas.1906731117#supplementary-materials",
    "score": 1,
    "timestamp": "2024-10-27T14:49:04",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "9bf61b04fa",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962404",
    "title": "Ask HN: Looking for Collaborators on Gaia, an MMO Ecosystem Simulator",
    "url": "https://news.ycombinator.com/item?id=41962404",
    "score": 1,
    "timestamp": "2024-10-27T14:15:12",
    "source": "Hacker News",
    "content": "I\u2019m developing an idea for a massively multiplayer online game I call <i>Gaia</i> for now, where players embody individual species within a shared, dynamic ecosystem. Each player can choose a kingdom of life\u2014plant, fungus, bacterium, or animal\u2014and starts as a random mutation from an existing, competing species; players evolve their species, adapting and competing for resources in a virtual environment that mimics ecological interdependence.<p>Key concepts include:<p><pre><code>  - Unique play styles for each Kingdom: Different gameplay mechanics for plants, fungi, bacteria, and animals, reflecting their roles in an ecosystem.\n  - Nutrient economy: A \u201cstock exchange\u201d for nutrients where players decide how to share or hoard resources, impacting ecosystem balance and competition.\n  - Ecosystem simulation: Species interactions drive the game, with real-time adaptations and a leaderboard based on area controlled by each species.\n  - Ongoing evolution: If species go extinct, players respawn as a random mutation from another successful species, with unique up- and downsides.\n  - Environmental events: Natural disasters and catastrophes that occur rarely, but shake up the environment in unexpected ways, require quick changes in strategy. \n  - Educational &amp; strategic depth: The game aims to reveal the complexities of ecological balance and fragility, blending fun with insight into nature.\n</code></pre>\nCreating engaging gameplay loops will be the biggest challenge, but I&#x27;m sure the possible solution space is huge.<p>I\u2019m looking for others interested in building or brainstorming around this idea\u2014whether in game development, ecological modelling, or MMO infrastructure. This is the game I&#x27;ve wanted to play forever; if you liked the idea of Spore, but found it way too shallow, and are likewise intrigued by the idea of simulating life\u2019s interconnected web, let\u2019s talk!",
    "comments": [
      {
        "author": "hyperific",
        "text": "What are you looking for in a collaborator?",
        "time": "2024-10-27T14:36:49"
      }
    ],
    "description": "No description available.",
    "document_uid": "99dfafee8b",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41962775",
    "title": "'Aerial arms race' with birds may have turned ancient cicadas into ace fliers",
    "url": "https://www.science.org/content/article/aerial-arms-race-birds-may-have-turned-ancient-cicadas-ace-fliers",
    "score": 1,
    "timestamp": "2024-10-27T15:16:26",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "e57a204f75",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "oqtxv4",
    "title": "The unreleased Commodore HHC-4's secret identity",
    "url": "http://oldvcr.blogspot.com/2024/10/the-unreleased-commodore-hhc-4s-secret.html",
    "score": 5,
    "timestamp": "2024-10-27T09:39:49.000-05:00",
    "source": "Lobsters",
    "content": "Once upon a time (and that time was Winter CES 1983), Commodore announced what was to be their one and only handheld computer, the Commodore HHC-4. It was never released and never seen again, at least not in that form. But it turns out that not only did the HHC-4 actually exist, it also wasn't manufactured by Commodore \u2014 it was a Toshiba. Like Superman had Clark Kent, the Commodore HHC-4 had a secret identity too: the Toshiba Pasopia Mini IHC-8000, the very first portable computer Toshiba ever made. And like Clark Kent was Superman with glasses, compare the real device to the Commodore marketing photo and you can see that it's the very same machine modulo a plastic palette swap. Of course there's more to the story than that. Recall, as we've discussed in other articles, that 1980s handheld and pocket computers occupied something of a continuum and the terms were imprecise, though as a rule handheld computers tended to be larger and emphasize processing power over size and battery life, while pocket computers tended to be smaller and emphasize size and low power usage over capability. Thus you had computers that were \"definitely pocket computers,\" like the Casio PB-100/Tandy PC-4 and Sharp PC-1250/Tandy PC-3, and \"definitely handheld computers\" like the Kyotronic 85 series (TRS-80 Model 100, NEC PC-8201A, etc.), Convergent WorkSlate and Texas Instruments CC-40, but also handheld-class computers sold as pocket computers like the Sharp PC-1500/Tandy PC-2 and Casio PB-1000C, and even a few handheld-sized computers with capabilities more typical of pocket computers like the VTech Laser 50. Small systems like these were hot in their day, not least from novelty and size appeal, and nearly every computer company was dabbling in them. The vast majority originated in Japan, primarily from Casio and Sharp, but also from other lesser-known manufacturers like Canon. Because of their unique constraints on size and battery usage, some notable exceptions notwithstanding (at least one of which we'll have a look at), pocket computers in particular often used unusual architectures seen nowhere else. Toshiba is a contraction of T&omacr;ky&omacr; Shibaura Denki kabushiki-kaisha (Tokyo Shibaura Electric Co., Ltd.), formed in 1939 as a merger between Shibaura Seisakusho (Shibaura Engineering Works) and T&omacr;ky&omacr; Denki (Tokyo Electric). Both were licensees of American conglomerate General Electric who owned substantial portions of each of them at the time. Although it carried the well-known nickname for years, the unified enterprise did not formally rename itself to Toshiba Corporation until 1979. As Tokyo Shibaura Electric, Toshiba produced its first computer in 1954, the TAC, an EDSAC clone with 7,000 vacuum tubes and 3,000 diodes delivered as a prototype to the University of Tokyo. It eventually produced minis like the TOSBAC-3400, based on Kyoto University's 1961 KT-Pilot, and produced its first microprocessor, the 12-bit TLCS-12 (\"Toshiba LSI Computer System\"), in 1973 for Ford automotive engine control units. The upgraded TLCS-12A became the basis of Toshiba's first microcomputer, a single-board evaluation system, and then reworked around the Intel 8080A (using the compatible Toshiba TMP9080AC at 2.048MHz) for the 1978 TLCS80A \u2022 EX-80, or EX-80 for short. At that time it was the cheapest system that could output to a TV using built-in hardware, displaying 10x26 text (not a typo) or 80x26 graphics from its standard 1K of RAM. In 1979 Toshiba developed the T-400 as a demonstration personal computer, using the upgraded EX-80BS (\"BASIC System\") and Toshiba's second-source version of the Intel 8085. It had a 32x24 text display, eight colours and up to 256x192 graphics, supporting up to 36K of RAM. Toshiba intended to offer it to overseas retailers as an OEM, but it got little import interest, and the company took it back to the shop to refine the design. Thus was the nucleus of the Toshiba Pasopia line, though it quickly became more of a generic brand than a distinct technical segment and many machines ultimately sold under the label weren't closely related. The original 1981 Pasopia, sold in the United States as the Toshiba T100, was a 4MHz Z80 system offering two flavours of BASIC, 64K of RAM, three-voice sound using the Texas Instruments SN76489 DCSG and up to 640x200 graphics. A notable later option, along with the more typical disk drives, RS-232 and parallel expansions, was an optional LCD mounted on its keyboard, but we'll come back to this at the end. On the other hand, the 1982 Pasopia 16 was a completely different PC-compatible system, running Intel's HMOS 8088-2 at 6MHz (as compared to the slower original NMOS 8088) and supporting up to 256K of RAM and MS-DOS 2.3 and CP/M-86. It had custom graphics hardware with up to 16 colours and 640x500 resolution. The P16 was also sold in the United States as the T300 and in Europe as the PAP. These two computers ended up forming two completely separate lineages that nevertheless were also badged as Pasopias. Due to poor sales outside their native country, their descendants were sold only in Japan. On the classic Pasopia side, Toshiba released the Pasopia 5 in 1983, a cost-reduced version of the O.G. Pasopia with the same features, along with the Pasopia 7 (and later the closely related Pasopia 700 in 1985), an upgraded and partially-compatible successor featuring a second TI SN76489 sound chip, three times the video RAM and more video modes. For its part, the Pasopia 16 separately evolved into the 1984 Pasopia 1600, with an 8MHz HMOS 8086-2 and up to 384K of video RAM, and the Pasopia 1600 TS100 and TS300, which had an 8MHz 80286 and up to 704K of RAM, with either dual 5.25\" floppies (TS100) or one floppy and a 20MB hard disk (TS300). To confuse things further, Toshiba also sold a line of MSX machines called the Pasopia IQ, releasing the HX-10, HX-20 and HX-30 series in 1983, 1984 and 1985 respectively. Some of these machines were sold in Europe, with only the Japanese versions having Pasopia IQ branding, and the last few models supported the MSX2 standard. Toshiba",
    "comments": [],
    "description": "Once upon a time (and that time was Winter CES 1983), Commodore announced what was to be their one and only handheld computer, the Commodore...",
    "document_uid": "36327d036d",
    "ingest_utctime": 1730068529
  },
  {
    "original_id": "41961844",
    "title": "A Useful metric for finding Great Art",
    "url": "https://medium.com/luminasticity/a-useful-metric-for-finding-great-art-3a88b727ca4c",
    "score": 2,
    "timestamp": "2024-10-27T12:52:20",
    "source": "Hacker News",
    "content": "So now the scene has been set, two songs that are not to my taste \u2014 and I hope given my examples that you will think hey those are not to IG Agent 13\u2019s taste.The Beatles \u2014 YesterdayThe Beatles \u2014 SomethingThese are songs that are often described as great, by a group that is often described as great. But as I was thinking to myself lately about the song Yesterday, prompted an article about the songwriting skills of Mr. McCartney, I think they might actually be great! (The article made me think about how it was not to my taste, but I like a great deal, which led to me thinking about Something which is also not to my taste and which I like)Now before we go on and just in case you think he likes them just because they are the Beatles, here\u2019s another song not to my taste which I absolutely hate, definitely more than I hate Your Song, which I suppose you would think yeah that song would not be to Agent 19's taste:The Beatles \u2014 MichelleGod I hate that song. Play the damn Helter Skelter thing again McCartney!So I could go on about why I think these songs are great, but that is beside the point (of this article at least, although very far from beside the point about art), or I could make the rude point that the movie Yesterday would be a perfect example of a movie that should be to my taste but I only ever think about to get annoyed at how bad it was which I guess I just did. (no offense to the actors who I quite liked)The point is that with a sufficient understanding of your own taste and what you like that transcends your taste you have a likely indicator of greatness, and it may repay further thinking later as to what greatness in the work attracted you despite yourself. So, think about it.",
    "comments": [],
    "description": "As always when we use the word Art we do not mean just the graphic or plastic arts, but also musical and literary art forms. In the previous critical article To Speak Meaningfully About Art we said\u2026",
    "document_uid": "a033a57049",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "tdgmbz",
    "title": "Understanding Round Robin DNS",
    "url": "https://blog.hyperknot.com/p/understanding-round-robin-dns",
    "score": 10,
    "timestamp": "2024-10-26T17:50:24.000-05:00",
    "source": "Lobsters",
    "content": "For OpenFreeMap, I'm using servers behind Round Robin DNS. In this article, I'm trying to understand how browsers and CDNs select which one to use.Normally, when you are serving a website from a VPS like Digital Ocean or Hetzner, you add a single A record in your DNS provider's control panel.This means that rr-direct.hyperknot.com will serve data from 5.223.46.55.In Round Robin DNS, you specify multiple servers for the same subdomain, like this.This allows you to share the load between multiple servers, as well as to automatically detect which servers are offline and choose the online ones.It's an amazingly simple and elegant solution that avoids using Load Balancers. It's also free, and you can do it on any DNS provider, whereas Load Balancing solutions can get really expensive (even on Cloudflare, which is otherwise very reasonably priced).I became obsessed with how it works. I mean, on the surface, everything is elegant, but how does a browser decide which server to connect to?In theory, there is an RFC 8305 called Happy Eyeballs (also linking to RFC 6724) about how the client should sort addresses before connecting.Now, this is definitely above my experience level, but this section seems like the closest to answering my question:If the client is stateful and has a history of expected round-trip times (RTTs) for the routes to access each address, it SHOULD add a Destination Address Selection rule between rules 8 and 9 that prefers addresses with lower RTTs.So in my understanding, it's basically:Let's see how it works in practice.I created 3 VPSs around the world: one in the US, one in the EU, and one in Singapore. I made 3 proxied and 3 non-proxied A records in Cloudflare.They run nginx with a config like this:server { server_name rr-direct.hyperknot.com rr-cf.hyperknot.com; # this is basically a wildcard block # so /a/b/c will return the same color.png file location / { root /data; rewrite ^ /color.png break; } location /server { alias /etc/hostname; default_type text/plain; } }So they serve a color.png, which is a 1px red/green/blue PNG file, as well as their hostname, which is test-eu/us/sg.I made a HTML test page, which fills a 10x10 grid with random images.The server colors are the following:US - greenEU - blueSG - redImportant: I'm testing from Europe; the EU server is much closer to me than US or especially the SG one. I should be seeing blue boxes!Chrome selects somewhat randomly between all locations, but once selected, it sticks with it. It re-evaluates the selection after a few hours. In this case, it was stuck with SG for hours, even though it is by far the slowest server for me.Also, an interesting behavior on Chrome was when not using HTTP/2: it can sometimes choose randomly between two servers, creating a pattern like this. Here it's choosing between EU and US randomly.Behaves similarly to Chrome, selects a location randomly on startup and then sticks with it. If I restart the browser, it picks a different random location.To my biggest surprise, Safari always selects the closest server correctly. Even if it goes offline for a while, after a few refreshes, it always finds the EU server again!Curl also works correctly. First time it might not, but if you run the command twice, it always corrects to the nearest server.If you have multiple VPSs around the world, try this command via ssh and see which one gets selected:curl https://rr-direct.hyperknot.com/server test-us curl https://rr-direct.hyperknot.com/server test-euCloudflare picks a random location based on your client IP and then sticks with it. (It behaves like client_ip_hash modulo server_num or similar.)As you have seen in the images, the right-side rectangle is always green. On my home IP, no matter what I do, Cloudflare goes to the US server. Curl shows the same.curl https://rr-cf.hyperknot.com/server test-usNow, if I go to my mobile hotspot, it always connects to the EU server.If I log into some VPSes and run the same curl command, I can see this behavior across the world. Every VPS gets connected to a totally random location around the world, but always the same.curl https://rr-cf.hyperknot.com/server test-sgSo what happens when one of the servers is offline? Say I stop the US server:service nginx stopcurl https://rr-direct.hyperknot.com/server test-euAs you can see, all clients correctly detect it and choose an alternative server.Actually, they do this fallback so well that if I turn off the server while they are loading, they correct within < 1 sec! Here is an animation of the 50x50 version of the same grid, on Safari:And what about Cloudflare? As you can see in the screenshots above, Cloudflare does not detect an offline server. It keeps accessing the server it decided for your IP, regardless of whether it's online or offline.If the server is offline, you are served offline. In curl, it returns:curl https://rr-cf.hyperknot.com/server error code: 521I've been trying to understand what this behavior is, and I highly suspect it's a bug in their network. One reference I found in their documentation is this part:Based on this documentation - and by common sense as well - I believe Cloudflare should also behave like browsers and curl do.At the very least, offline servers should be detected.Moreover, it would also be really nice if the server with the lowest latency were selected, like in Safari!I mean, currently, if you have one server in the US and one in New Zealand, exactly 50% of your US users will be served from New Zealand, which makes no sense. Also, for Safari users, it's actually slower compared to not using Cloudflare!There is a HN discussion now, where both the CEO and the CTO of Cloudflare replied!Note 1: I've tried my best to understand articles 1, 2, 3 which Matthew Prince pointed out to me on X. As I understand, they are referring to Cloudflare servers as \"servers\", not users' servers behind A records. Also, I couldn't find anything related to Round Robin DNS.Note 2: If you have any idea how I can keep running this experiment without paying for 3 VPS-es around the world, please comment below. I'd love",
    "comments": [
      {
        "author": "fanf",
        "text": "<p>This article is mostly about how the Happy Eyeballs algorithm works on a DNS name with multiple address records. Happy Eyeballs was primarily intended as an IPv6 transition mechanism, to cope better when a server advertises both IPv6 and IPv4 addresses but one of the protocols doesn\u2019t work.</p>\n<p>There\u2019s some history before Happy Eyeballs.</p>\n<p>Round Robin DNS started off as a cheap hack in BIND (possibly an early Vixie idea?) around 1990ish I guess. (I don\u2019t think it was documented in the RFCs until much later.) The idea was that when a domain has multiple records, a DNS server cycles round which one appears first in an answer, so the records rotate with each query.</p>\n<p>Back then, and until Happy Eyeballs, a client would get the answer from the DNS server and simply try each record in turn until one worked. The effect of Round Robin DNS was generally a pretty good spread of load across all the addresses. But it was useless for failover, because if one of the addresses stopped working, clients would typically suffer a slow timeout before maybe (if you are lucky) trying another address.</p>\n<p>There was also a period when Round Robin DNS stopped working to spread the load as expected.</p>\n<p>A predecessor of Happy Eyeballs was <a href=\"https://www.rfc-editor.org/rfc/rfc3484\" rel=\"ugc\">default address selection for IPv6</a>. This algorithm is fundamentally misguided and cannot possibly achieve what it sets out to, because it assumes that client devices will somehow magically obtain network topology information. When the algorithm isn\u2019t configured (ie always) it assumes that longest-prefix matching equates to topological closeness, which is often false.</p>\n<p>Anyway, part of this algorithm requires sorting addresses into order of preference, which defeats Round Robin DNS. When implementations of RFC 3484 were deployed, it caused a bunch of stuff to break because load was no longer evenly spread as expected. A sample of links: <a href=\"https://dotat.at/:/?q=rfc+3484\" rel=\"ugc\">https://dotat.at/:/?q=rfc+3484</a></p>\n<p>Happy Eyeballs is <em>much better</em> than the old client connection logic. Strictly speaking, Round Robin DNS is not the same thing, since it\u2019s a feature of DNS servers, whereas Happy Eyeballs is a client feature. Happy Eyeballs would work just as well if the DNS server returns the list of addresses in a fixed order. But even pedantic DNS nerds won\u2019t mind if you refer to multiple address records as \u201cround robin DNS\u201d.</p>\n",
        "time": "2024-10-26T20:48:01.000-05:00"
      }
    ],
    "description": "In which I try to understand how browsers and Cloudflare choose which server to use",
    "document_uid": "ae81cf4fc3",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "pdhpod",
    "title": "For Sale: Used Domain (**Clean Title**)",
    "url": "https://blog.jim-nielsen.com/2024/used-domain-clean-title/",
    "score": 6,
    "timestamp": "2024-10-28T11:07:31.000-05:00",
    "source": "Lobsters",
    "content": "Bryan Braun has an interesting post about his experience with what he calls a \u201chaunted domain\u201d: He buys a domain that seems fine on the surface. When he begins using it, he notices some things aren\u2019t right (organic search traffic, for example, is dead). After some investigation, he learns the domain was previously used to host pirated music. He has to ask himself: now what? Being able to ascertain the reputation and vitality of a domain is an intriguing problem. At a societal level, it\u2019s not really a problem we\u2019ve had to wrestle with (yet). Imagine, for example, 200 years from now when somebody is reading a well-researched book whose sources point to all kinds of domains online which at one point were credible sources but now are possibly gambling, porn, or piracy web sites. If you take the long view, we\u2019re still at the very beginning of the internet\u2019s history where buying a domain has traditionally meant you\u2019re buying \u201cnew\u201d. But what will happen as the lifespan of a domain begins to outpace us humans? You\u2019re going to want to know where your domain has been. The \u201ccondition\u201d of a domain could very well become a million dollar industry with experts. Similar to how we have CARFAX vehicle history reports, domain history reports could become standard fare when purchasing a domain. In fact, parallels to other real-world purchase verification and buyer protection programs is intriguing. Car Titles Establishing legal ownership of a vehicle through titles is something the government participates in. You\u2019ll often hear people advertise a used vehicle as having a \u201cclean title\u201d, meaning it doesn\u2019t have a salvaged or rebuilt title (which indicates a history of damage). But how the car was used is also important. Was the car part of a fleet, e.g. a rental car? Was it owned by a private third-party its whole life? Did that person drive Uber? So. Many. Questions. Even where the car has been is important. I remember buying a car in Las Vegas once and the dealer was telling me how people from New England frequently fly into town to buy a car. Why? Because they want a car whose lifespan had been in the dry desert and would thus be free of the effects of harsh winters (rust from snow/salted roads, etc). Houses When you buy a house, getting an inspection is pretty standard fare (where I live). Part of the home buying process includes paying an inspector to come in and assess the state of the home \u2014 and hopefully find any problems the current owners either don\u2019t know about or are keeping from you. But the history of the home is often important too. Was it a drug house? Was it a rental? An Airbnb? Houses have interesting parallels to domains because their lifespans are so much longer than other things like cars (my current house was built in the 1930\u2019s). Homes can go through many owners, some of whom improve it over time while others damage it. You know those home renovation shows? The nerd version of that would be a \u201cdomain renovation\u201d show where people buy old domains, fix them up (get them whitelisted with search engines again, build lots of reputable inbound links, etc.), and then flip them for a good price. Collector Items (Cards, Books, Toys, etc.) What I love most about this world is the verbiage around the various grading systems, such as: Mint Near mint Excellent Very good Good Fair Poor Damaged Even within \u201cMint\u201d there are various categorizations such as: MIB: Mint In Box MOC: Mint On Card (still has the OG tags) FM: Factory Mint Can you imagine something similar for domains? MNR: Mint, Never Registered MNPT: Mint, No Public Traffic MNB: Mint, No Backlinks Conclusion Honestly, I don\u2019t really have anything else to say about this, lol. It\u2019s simply fun to think about and write down what comes to mind. That said, it wouldn\u2019t surprise me if a standardized grading system with institutional verification rises up around the world of domains. URLs are everywhere \u2014 from official government records to scrawled on the back a door in a public restroom \u2014 and they\u2019ll be embedded in civilization for years to come. (Even more so if domains become the currency of online handles!) Today usa.gov is backed by the full faith and credit of the United States Government. But a few hundred years from now, who knows? Maybe it\u2019ll be an index for pirated music. Whatever happened to romanempire.gov?",
    "comments": [
      {
        "author": "mjec",
        "text": "<p>An alternative - and I think preferable - approach would be to stop assuming continuity of domain ownership, in particular when there has been a lapse in registration rather than a transfer. You can imagine that once you start getting NXDOMAIN, the poor reputation would \u201cfade\u201d over time. Though of course whois right now is <a href=\"https://labs.watchtowr.com/we-spent-20-to-achieve-rce-and-accidentally-became-the-admins-of-mobi/\" rel=\"ugc\">dangerously bad</a>, I suspect in part because that has never really been part of the reputation infrastructure.</p>\n<p>Of course, this requires those maintaining reputation lists to actively maintain them. It seems far more common to push the cost of stale poor reputation to the domain (or IP) lessee, because the list maintainer\u2019s incentives are to minimize false negatives, regardless of the number of low priority false positives. I do think the SEO case is by far the easiest to handle here; I can understand why email reputation is harder to manage.</p>\n",
        "time": "2024-10-28T11:32:38.000-05:00"
      },
      {
        "author": "carlana",
        "text": "<p>A thing that has happened a lot lately is people will buy up dead blog domains from the 2010s with good residual SEO and then fill them with AI slop. I\u2019ve read about multiple instances of that happening.</p>\n",
        "time": "2024-10-28T11:09:45.000-05:00"
      },
      {
        "author": "kornel",
        "text": "<p>There\u2019s probably a bunch of startups being created for this now, if there aren\u2019t such already.</p>\n<p>If it becomes a major problem, maybe there will be a GDPR-like legislation that gives a right to purge domain\u2019s history.</p>\n",
        "time": "2024-10-28T13:23:27.000-05:00"
      },
      {
        "author": "Student",
        "text": "<p>Right, but what about the obligation? Part of the problem is that domains are used as identity tokens. See for example <a href=\"https://inti.io/p/when-privacy-expires-how-i-got-access\" rel=\"ugc\">https://inti.io/p/when-privacy-expires-how-i-got-access</a></p>\n",
        "time": "2024-10-28T13:56:57.000-05:00"
      }
    ],
    "description": "Writing about the big beautiful mess that is making things for the world wide web.",
    "document_uid": "d5d3474a2b",
    "ingest_utctime": 1730142827
  },
  {
    "original_id": "41980063",
    "title": "Best Websites To download Korean movies with English subtitles",
    "url": "https://www.newsreportage.com.ng/2024/10/best-websites-to-download-korean-movies.html",
    "score": 1,
    "timestamp": "2024-10-29T07:40:07",
    "source": "Hacker News",
    "content": "K-Dramas have become a popular choice of movie/series genre for most people including Nigerians. Known for their emotional, romantic and entertaining storytelling, K-Dramas have come to win the hearts of drama movies and TV series lovers worldwide. In this post, I decided to do a deep research and source out some of the Best websites to download K-Dramas with English subtitles attached. Most of the time, we find ourselves downloading srt subtitle files separately in order to understand what they say in K-Dramas. However, In this post, you will get to know websites where you can download your Korean dramas with English subtitles attached. Read more below as you progress down this Informative post. WHERE TO DOWNLOAD KOREAN MOVIES AND SERIES WITH ENGLISH SUBTITLES After surfing the Internet, I came up with a very useful list of some of the Best Websites to download your Korean movies and series with English subtitles for free. See them below. 1. Viki2. Dramacool3. iQIYI4. MyAsianTV 5. Netflix 6. MyDramaList7. Telegram Now let's get to know detailed information about the aforementioned websites. Stay with me. Note: The list is not a countdown or ranking, just random selection of the best websites. 1. VIKIInfographicsSometimes called Rakuten Viki, this website takes our number 1 spot. Viki has a huge library of Korean, Chinese and Japanese movies and TV series available for free to its users. It is owned by the Japanese Technology company, Rakuten and can also be accessed via a mobile app. Visit Viki to download Korean movies and more with English subtitles. Website Link: Viki2. DRAMACOOL InfographicsDramacool is also another stop for your Korean movies. The website also has lots of movies and shows in its library, which you can browse and download to enjoy. Dramacool also has two versions of the website with different domain extensions in case of downtime on either one of them. See link below. Website Link: Dramacool 3. IQIYI InfographicsUnlike others, iQIYI lets you stream/watch movies and series online only. There's a mobile app as well if you prefer to use the mobile version. IQIYI library boasts of content from Korean, to Chinese to Thai and many more Asian Dramas. Basically anything Asian content. Website Link: iQIYI 4. MYASIANTV InfographicsThis is another popular site for Korean and Asian content as a whole. The site\u2019s library is huge with an easy to navigate interface. MyAsianTV also has a mobile app which is available on various app stores. Visit the site or download the mobile app and explore a wide range of Asian content. Website Link: MyAsianTV 5. NETFLIXInfographicsNetflix is the biggest movie streaming service in the whole world. Netflix also has a wide library of Korean movies and shows. Though Netflix is a paid platform, which can serve as a hassle for most people, if you have a subscribed Netflix account, you can stream or download your K-Dramas with English subtitles as well. Website Link: Netflix6. MYDRAMALIST InfographicsNow when it comes to MyDramaList, it's not a movie streaming or download website. Instead, it's a website for Asian content recommendations and ratings. On MyDramaList, you can explore various lists and genres, read plots, get to know casts and decide if you will watch a particular show or not. You can also view a list of trendy movies and series which people are currently watching and recommending. Visit the site below. Website Link: MyDramaList 7. TELEGRAM InfographicsTelegram is more than a messaging app. With Telegram, you can search for Korean movies or series and simply subscribe to the channels uploading them and download to your device. Simply use the search bar to search for movies or series and you may come across a channel with juicy content. And there you have it, a list of free websites you can use to download Korean movies and shows with English subtitles for free. From the list, you can see that most of the websites also come with a mobile app to suite your preference. Do well to share this post with your friends and fellow K-Drama lovers. CONCLUSION A big thank you for reading this post, please read more and make your comments known using the comment section below. See you in my next post.",
    "comments": [],
    "description": "Discover websites where users can download Korean movies and series with English subtitles for free. ",
    "document_uid": "7161b65a09",
    "ingest_utctime": 1730184417
  },
  {
    "original_id": "41965942",
    "title": "Understanding CharAt(index) in JavaScript: A Guide",
    "url": "https://jsdevspace.substack.com/p/understanding-charatindex-in-javascript",
    "score": 1,
    "timestamp": "2024-10-27T22:47:11",
    "source": "Hacker News",
    "content": "The charAt(index) method is a built-in JavaScript string method that retrieves the character at a specified index within a string. Below is a detailed look at its usage, parameters, and practical examples.Syntax:string.charAt(index)Parameter:Return Value:Examplesconst str = \"Hello, World!\"; console.log(str.charAt(0)); // Output: \"H\" console.log(str.charAt(7)); // Output: \"W\" console.log(str.charAt(13)); // Output: \"\" (index out of range)const password = \"P@ssw0rd\"; if (password.charAt(0) === 'P') { console.log(\"Password starts with 'P'\"); }const dateString = \"2024-10-22\"; const year = dateString.charAt(0) + dateString.charAt(1) + dateString.charAt(2) + dateString.charAt(3); console.log(year); // Output: \"2024\"const text = \"abcde\"; for (let i = 0; i < text.length; i++) { console.log(text.charAt(i)); // Outputs: a, b, c, d, e }const unicodeString = \"Hello \ud83d\ude0a\"; console.log(unicodeString.charAt(6)); // Output: \"\ud83d\ude0a\"Index Out of Range: If an index beyond the string length is specified, charAt returns an empty string instead of throwing an error.Difference from Square Brackets ([]): You can also access characters using string[index], but this approach does not handle out-of-range indexes safely (it returns undefined instead).console.log(str[0]); // Output: \"H\" console.log(str[13]); // Output: undefinedThe charAt(index) method is a straightforward and versatile tool for character extraction in JavaScript. Its ability to return an empty string for out-of-range indexes makes it a safe choice for character processing, string parsing, looping, and handling special characters.",
    "comments": [],
    "description": "Mastering charAt(index) in JavaScript: Extracting and Handling Characters with Ease",
    "document_uid": "baed771279",
    "ingest_utctime": 1730068518
  },
  {
    "original_id": "ba9sn5",
    "title": "Does Open Source AI really exist?",
    "url": "https://tante.cc/2024/10/16/does-open-source-ai-really-exist/",
    "score": 1,
    "timestamp": "2024-10-28T13:07:03.000-05:00",
    "source": "Lobsters",
    "content": "The Open Source Initiative (OSI) released the RC1 (\u201cRelease Candidate 1\u201d meaning: This thing is basically done and will be released as such unless something catastrophic happens) of the \u201cOpen Source AI Definition\u201c. Some people might wonder why that matters. Some people come up with a bit of writing on AI, what else is new? That\u2019s basically LinkedIn\u2019s whole existence currently. But the OSI has a very special role in the Open Source software ecosystem. Because Open Source isn\u2019t just based on the fact whether you can see code but also about the License that code is covered under: You might get code that you can see but that you are not allowed to touch (think of the recent WinAmp release debate). The OSI basically took on the role of defining which of the different licenses that were being used all over the place actually are \u201cOpen Source\u201d and which come with restrictions that undermine the idea. This is very important: Picking a license is a political act with strong consequences. It can allow or forbid different modes of interaction with an object or might put certain requirements to the use. The famous GPL for example allows you to take the code but forces you to also open your own changes to it. Other licenses do not enforce this demand. Choosing a license has tangible effects. Quick sidebar: \u201cOpen Source\u201d already is a bit of a problematic term, it\u2019s (my opinion) a way to depoliticise the idea of \u201cFree Software\u201c. Both do share certain ideas but where \u201cOpen Source\u201d frames things more in a pragmatic \u201ccorporations want to know which code they can use\u201d kind of way Free Software was always more of a political movement arguing more from a standpoint of user rights and liberation. An idea that was probably damaged the most by the most visible figures in that space that probably should just walk into the sea. So what makes a thing \u201cOpen Source\u201d? Well the OSI has a brief list. You can read it quickly but let\u2019s focus on Point 2: Source Code: The program must include source code, and must allow distribution in source code as well as compiled form. Where some form of a product is not distributed with source code, there must be a well-publicized means of obtaining the source code for no more than a reasonable reproduction cost, preferably downloading via the Internet without charge. The source code must be the preferred form in which a programmer would modify the program. Deliberately obfuscated source code is not allowed. Intermediate forms such as the output of a preprocessor or translator are not allowed. To be Open Source a piece of software needs to come with the sources. Okay, that\u2019s not surprising. But the writers have seen some shit so they added that obfuscated code (meaning code that has been mangled to be unreadable) or intermediate forms (meaning you don\u2019t get the actual sources but something that has already been processed) are not allowed. Cool. Makes sense. But why do people care about sources? Sources of Truth Open Source is a relatively new mass phenomenon. We had software before, even some we didn\u2019t have to pay for. We called it \u201cFreeware\u201d back then. Freeware is software you can use without cost but that you don\u2019t get any source code to. You cannot change the program (legally), you cannot audit it, cannot add to it. But it\u2019s free of charge. And there was a lot of that back in my younger days. WinAMP, the audio player I talked about above used to be Freeware and basically everyone used it. So why even care about sources? For some it was about being able to modify the tools easier, especially if the maintainer of the software didn\u2019t really work on it any more or started adding all kinds of stuff you didn\u2019t agree with (think of all those proprietary software packages today you have to use for work that get AI stuffed in there behind every other button). But there is more to it than just feature requests. There\u2019s trust. When I run software, I need to trust the people who wrote it. Trust them to do a good job, to build reliable and robust software. To add only the features in the documentation and nothing hidden, potentially harmful. Especially with so large parts of our real lives running on digital infrastructures questions of trust get more and more important. We all know that we want fully open sourced, peer-reviewed and battle-tested encryption algorithms in our infrastructures to our communication is safe from harm. Open Source is \u2013 especially for critical systems and infrastructures \u2013 a key part of establishing that trust: Because you want (someone) to be able to verify what\u2019s up. There has been a long push for more reproducible builds. Those build processes basically guarantee that given the same code input you get the same compiled result. Which means that if you want to know if someone really delivered you exactly what they said they would you can check. Because your build process would create an identical artifact. Not everyone does this level of analysis of course. And even fewer people only use software from reproducible build processes \u2013 especially with a lot of software not being compiled today. But relationships are more nuanced than code and trust is a relationship: You being fully open book about your code and how exactly the binary version was built makes it a lot easier for me to trust you. To know what is in the software I am running on the machine that also has my bank statements or encryption keys on it. What does this have to do with AI? AI systems and 4 Freedoms AI systems are a bit special. Because \u2013 especially the big ones everyone is so fascinated by \u2013 don\u2019t really consist of a lot of code in comparison to their size. A neural network implementation is a few hundred lines",
    "comments": [],
    "description": "The OSI has presented their definition of Open Source AI and a closer reading only shows that \"Open Source AI\" probably just isn't a thing that can exist.",
    "document_uid": "d6df169c03",
    "ingest_utctime": 1730142827
  },
  {
    "original_id": "41961846",
    "title": "Norlha, the Luxury Yak Wool Brand Made by Nomads on the Tibetan Plateau",
    "url": "https://www.anothermag.com/fashion-beauty/12229/norlha-is-the-tibetan-label-changing-what-luxury-fashion-looks-like",
    "score": 3,
    "timestamp": "2024-10-27T12:52:32",
    "source": "Hacker News",
    "content": "With ethical and sustainable practices at the forefront, Kim and Dechen Yeshi\u2019s label Norlha makes use of Tibet\u2019s precious yak wool and ancient techniques to make beautiful hand-crafted clothingFebruary 06, 2020 Who is it? Norlha, the Tibetan luxury brand working with nomads who skilfully hand-weave from yak wool Why do I want it? The brand places the utmost importance on ethical and sustainable practices, preserving tradition, nurturing a local economy and producing timeless garments in the process Where can I find it? At Norlha\u2019s online store and various worldwide stockists Who is it? 12 years ago, in a small valley hidden in the Amdo region of the Tibetan Plateau, a mother, Kim Yeshi \u2013 an American anthropologist married to a Tibetan academic \u2013 and her daughter, Dechen, founded Norlha. Today, their yak khullu atelier \u2013 yak khullu wool comes from the soft under-down of a yak \u2013 produces two seasonal collections a year in a nomad settlement comprising 230 families, 6,000 yaks and 20,000 sheep. \u201cI had this idea to do something with yak wool,\u201d explains Kim of the label\u2019s origins. \u201cI had a friend in Kathmandu who worked with cashmere, and we were always talking about the animals that had precious wools in the region. One was the camel, one was the yak, and I came to realise people knew very little about the yak; it wasn\u2019t something out on the market. The idea was to fulfil the potential of this precious fibre, something that hadn\u2019t been done before.\u201d Kim encouraged Dechen to pursue this journey into the unknown \u2013 her daughter was then a Connecticut College graduate interested in directing documentaries \u2013 eventually settling in Ritoma village where Norlha began its life. \u201cI didn\u2019t have a background in textiles and was much more interested in filmmaking,\u201d says Dechen. \u201cI started to talk to young people here and realised that they were very open to having an alternative source of employment and, more than anything, wanted to be more in touch with the modern world. What Kim had told me about starting a business with yak wool, plus the Tibetan\u2019s interest in doing something bigger, suddenly clicked together. Filmmaking became a bridge to ask people questions, and through their answers I realised there was a future for us too.\u201d NorlhaPhotography by Nikki McClarron Norlha is now home to 120 employees, all former nomads who have spun, woven and felted for generations. \u201cEverything and everyone works in the same courtyard, there\u2019s a mutual sense of responsibility and family,\u201d Dechen explains. Norlha fibres are hand-spun using the finest yak khullu, but unlike other animals that are combed or shaved, the wool of the yak can only be caught as molt when it sheds in late spring. A complex process, one winter scarf could be made up of an entire year\u2019s worth of wool from seven different yaks. The soft wool yarns are then dyed, using a palette of colours obtained by natural pigments made locally by the craftsmen of the workshop. The resulting clothing \u2013 timeless Chuba shirts and dresses, oversized scarfs, seamless vests, felt trousers and shirt jackets \u2013 have a precious, home-spun quality. Earthy tones are matched with rich red and indigo, and encapsulate the brand\u2019s strong bond to the natural world and traditions around them, the tundra of winter, grasslands of summer and its many other seasons of colour. NorlhaPhotography by Nikki McClarron Why do I want it? Norlha hopes to redefine the way we see luxury, making it synonymous not just with high-quality materials and careful craftsmanship, but also an ethical conscience and passionate workforce, with Tibetan culture at the fore (the name itself, in the Tibetan language, means \u2018wealth of the Gods\u2019 and is used by local nomads in reference to their yaks, which are among the most important resources of Central Asia). \u201cWe don\u2019t want people buying from us just out of sympathy, because then they won\u2019t come back and it\u2019s not sustainable,\u201d says Kim. \u201cWe see this sustainability in the same way with Norlha\u2019s workers. Our artisans need to have a purpose and income to see their worth in the business, to encourage them to stand on their own two feet,\u201d she adds. Employees have learnt not only to spin and weave, but become linked to the world within a familiar environment, one that moves ahead without compromising core cultural values. \u201cOur mission is to attract the younger generation as they are the future,\u201d says Dechen. Norlha was first founded in the midst of a changing Tibet: the growing availability of the internet in the country, and the introduction of the Tibetan language on iPhones, has help opened to the rest of the world. \u201cThis is when we realised it was possible to be a brand,\u201d Dechen says. \u201cWe want people to understand Tibetan culture in a different way through our products. We want to tell our story, for the local community as much as for the global community, bringing everyone together as one.\u201d Where can I find it? At Norlha\u2019s workshop and online store, plus various worldwide stockists, found here.",
    "comments": [],
    "description": "With ethical and sustainable practices at the forefront, Kim and Dechen Yeshi\u2019s label Norlha makes use of Tibet\u2019s precious yak wool and ancient techniques to make beautiful hand-crafted clothing",
    "document_uid": "21b1c97963",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "xgitmt",
    "title": "Becoming physically immune to brute-force attacks",
    "url": "https://seirdy.one/posts/2021/01/12/password-strength/",
    "score": 2,
    "timestamp": "2024-10-28T01:41:58.000-05:00",
    "source": "Lobsters",
    "content": "Preface This is a tale of the intersection between thermal physics, cosmology, and a tiny amount of computer science to answer a seemingly innocuous question: \u201cHow strong does a password need to be for it to be physically impossible to brute-force, ever?\u201d TLDR at the bottom. Note: this post contains equations. Since none of the equations were long or complex, I decided to just write them out in code blocks instead of using images or MathML the way Wikipedia does. Update: I implemented the ideas in this blog post (and more) in a program/library, MOAC Toggle table of contents Table of Contents Intro\u00adduction Asking the right question Quantifying password strength. The Problem Caveats and estimates Compu\u00adtation Calculating the mass-energy of the observable universe Calculating the critical density of the observable universe Solving for E Final Solution Sample unbreakable passwords Conclusion, TLDR Further reading: alternative approaches Approaches that account for computation speed Ac\u00adknowledge\u00adments Intro\u00adduction I realize that advice on password strength can get outdated. As supercomputers grow more powerful, password strength recommendations need to be updated to resist stronger brute-force attacks. Passwords that are strong today might be weak in the future. How long should a password be in order for it to be physically impossible to brute-force, ever? This question might not be especially practical, but it\u2019s fun to analyze and offers interesting perspective regarding sane upper-limits on password strength. Asking the right question Let\u2019s limit the scope of this article to passwords used in encryption/decryption. An attacker is trying to guess a password to decrypt something. Instead of predicting what tomorrow\u2019s computers may be able to do, let\u2019s examine the biggest possible brute-force attack that the laws of physics can allow. A supercomputer is probably faster than your phone; however, given enough time, both are capable of doing the same calculations. If time isn\u2019t the bottleneck, energy usage is. More efficient computers can flip more bits with a finite amount of energy. In other words, energy efficiency and energy availability are the two fundamental bottlenecks of computing. What happens when a computer with the highest theoretical energy efficiency is limited only by the mass-energy of the entire observable universe? Let\u2019s call this absolute unit of an energy-efficient computer the MOAC (Mother of All Computers). For all classical computers that are made of matter, do work to compute, and are bound by the conservation of energy, the MOAC represents a finite yet unreachable limit of computational power. And yes, it can play Solitaire with amazing framerates. How strong should your password be for it to be safe from a brute-force attack by the MOAC? Quantifying password strength. A previous version of this section wasn\u2019t clear and accurate. I\u2019ve since removed the offending bits and added a clarification about salting/hashing to the Caveats and estimates section. A good measure of password strength is entropy bits. The entropy bits in a password is a base-2 logarithm of the number of guesses required to brute-force it.note 1 A brute-force attack that executes 2n guesses is certain to crack a password with n entropy bits, and has a one-in-two chance of cracking a password with n+1 entropy bits. For scale, AES-256 encryption is currently the industry standard for strong symmetric encryption, and uses key lengths of 256-bits. An exhaustive key search over a 256-bit key space would be up against its 2256 possible permutations. When using AES-256 encryption with a key derived from a password with more than 256 entropy bits, the entropy of the AES key is the bottleneck; an attacker would fare better by doing an exhaustive key search for the AES key than a brute-force attack for the password. To calculate the entropy of a password, I recommend using a tool such as zxcvbn or KeePassXC. The Problem Define a function P. P determines the probability that MOAC will correctly guess a password with n bits of entropy after using e energy: P(n, e) If P(n, e) \u2265 1, the MOAC will certainly guess your password before running out of energy. The lower P(n, e) is, the less likely it is for the MOAC to guess your password. Caveats and estimates I don\u2019t have a strong physics background. A brute-force attack will just guess a single password until the right one is found. Brute-force attacks won\u2019t \u201cdecrypt\u201d stored passwords, because they\u2019re not supposed to be stored encrypted; they\u2019re typically salted and hashed. When estimating, we\u2019ll prefer higher estimates that increase the odds of it guessing a password; after all, the point of this exercise is to establish an upper limit on password strength. We\u2019ll also simplify: for instance, the MOAC will not waste any heat, and the only way it can guess a password is through brute-forcing. Focusing on too many details would defeat the point of this thought experiment. Quantum computers can use Grover\u2019s algorithm for an exponential speed-up; to account for quantum computers using Grover\u2019s algorithm, calculate P(n/2, e) instead. Others are better equipped to explain encryption/hashing/key-derivation algorithms, so I won\u2019t; this is just a pure and simple brute-force attack given precomputed password entropy, assuming that the cryptography is bulletproof. Obviously, I\u2019m not taking into account future mathematical advances; my crystal ball broke after I asked it if humanity would ever develop the technology to make anime real. Finally, there\u2019s always a non-zero probability of a brute-force attack guessing a password with a given entropy. Literal \u201cimmunity\u201d is impossible. Lowering this probability to statistical insignificance renders our password practically immune to brute-force attacks. Compu\u00adtation How much energy does MOAC use per guess during a brute-force attack? In the context of this thought experiment, this number should be unrealistically low. I settled on kT. k represents the Boltzmann Constant (about 1.381\u00d710-23 J/K) and T represents the temperature of the system. Their product corresponds to the amount of heat required to create a 1 nat increase in a system\u2019s entropy. A more involved approach to picking a good value might utilize the Plank-Einstein relation. It\u2019s also probably a better idea to",
    "comments": [],
    "description": "Using thermal physics, cosmology, and computer science to calculate password vulnerability to the biggest possible brute-force attack.",
    "document_uid": "3210d8a096",
    "ingest_utctime": 1730115449
  },
  {
    "original_id": "41962798",
    "title": "Australia-to-Singapore clean energy cable gets green light",
    "url": "https://newatlas.com/energy/suncable-australia-asia-power-link-approval-update/",
    "score": 1,
    "timestamp": "2024-10-27T15:21:45",
    "source": "Hacker News",
    "content": "The world's largest renewable energy and transmission project has received key approval from government officials. The Australia-Asia Power Link project will send Australian solar power to Singapore via 4,300 kilometer-long undersea cables.The AAPowerLink project is being led by SunCable, and will start by constructing a mammoth solar farm in Australia's Northern Territory to transmit around-the-clock clean power to Darwin, and also export \"reliable, cost-competitive renewable energy\" to Singapore.The principal environmental approval recently obtained from the Northern Territory Government rubber stamps the building of a solar farm at Powell Creek with a clean energy generation capacity of up to 10 gigawatts, plus utility scale onsite storage. It also green lights an 800-km (~500-mile) overhead transmission line between the solar precinct and Murrumujuk near Darwin.A converter facility will convert electricity from high-voltage direct current to high-voltage alternating current to supply Darwin \u2013 with the setup expected to supply \"up to 4GW of 24/7 green electricity to green industrial customers.\" This will be rolled out over two stages, the first delivering 900 megawatts, and then second adding a further 3 gigawatts. The Australia-Asia Power Link project will export solar power generated from an enormous clean energy precinct in the Northern Territories via subsea cables to SingaporeSunCable The project also aims to convert another 1.75 GW of power from AC to DC and send it through 4,300 km (over 2,670 miles) of subsea cabling to Singapore. The environmental approval will allow the company to lay cable from Darwin converter station past the end of Australian territorial waters and up to the Indonesian border.The company, which was acquired by billionaire Mike Cannon-Brookes last year after a bidding war with former project partner Andrew Forrest, still has a number of large hurdles to jump over before the AAPowerLink project really gets going though. These include negotiating land use with traditional owners, nailing down agreements with other bodies along the route and even actually financing the ambitious project.\"SunCable is delighted to receive environmental approval from the Northern Territory Government to proceed with our flagship Australia-Asia Power Link project,\" said company MD, Cameron Garnsworthy. \"This approval allows us to progress the development, commercial, and engineering activities required to advance the project to Final Investment Decision targeted in 2027.\"If all of the dominoes line up perfectly, supply of the first clean electricity is estimated to start in the early 2030s. An overview graphic on the project page (reproduced above) shows that the eventual end game for the Powell Creek development appears to be the generation of up to 20 GW of peak solar power and have some 36-42 GWh of battery storage on site. The video below has more.Source: SunCable",
    "comments": [],
    "description": "The world's largest renewable energy and transmission project has received key approval from government officials. The Australia-Asia Power Link project will send Australian solar power to Singapore via 4,300 kilometer-long undersea cables.",
    "document_uid": "adb4c4f1f9",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "41961669",
    "title": "Why Debian won't distribute AI models any time soon [audio]",
    "url": "https://deepdive.opensource.org/podcast/why-debian-wont-distribute-ai-models-any-time-soon/",
    "score": 2,
    "timestamp": "2024-10-27T12:21:48",
    "source": "Hacker News",
    "content": "Why Debian won\u2019t distribute AI models any time soon Deep Dive: AI Why Debian won't distribute AI models any time soon Play Episode Pause Episode Mute/Unmute Episode Rewind 10 Seconds 1x Fast Forward 30 seconds 00:00 / Subscribe Share Welcome to a brand new episode of Deep Dive: AI! For today\u2019s conversation, we are joined by Mo Zhou, a PhD student at Johns Hopkins University and an official Debian developer since 2018. Tune in as Mo speaks to the evolving role of artificial intelligence driven by big data and hardware capacity and shares some key insights into what sets AlphaGo apart from previous algorithms, making applications integral, and the necessity of releasing training data along with any free software. You\u2019ll also learn about validation data and the difference powerful hardware makes, as well as why Debian is so strict about their practice of offering free software. Finally, Mo shares his predictions for the free software community (and what he would like to see happen in an ideal world) before sharing his own plans for the future, which include a strong element of research. If you\u2019re looking to learn about the uphill climb for open source artificial intelligence, plus so much more, you won\u2019t want to miss this episode! Full transcript. Key points from this episode: Background on today\u2019s guest, Mo Zhou: PhD student and Debian developer. His recent Machine Learning Policy proposal at Debian. Defining artificial intelligence and its evolution, driven by big data and hardware capacity. Why the recent advancements in deep learning would be impossible without hardware. Where AlphaGo differs from past algorithms. The role of data, training code, and inference code in making an application integral. Why you have to release training data with any free software. The financial and time expense of classifying images. What you need access to in order to modify an existing model. The validation data set collected by the research community. Predicting the process of retraining. What you can gain from powerful hardware. Why Debian is so strict in the practice of free software. Problems that occur when big companies charge for their ecosystems. What Zhou is expecting from the future of the free software community. Which licensing schemes are most popular and why. An ideal future for Open Source AI. Zhou\u2019s plans for the future and why they include research. Links mentioned in today\u2019s episode: Credits Special thanks to volunteer producer, Nicole Martinelli. Music by Jason Shaw, Audionautix. This podcast is sponsored by GitHub, DataStax and Google. No sponsor had any right or opportunity to approve or disapprove the content of this podcast. The views expressed in this podcast are the personal views of the speakers and are not the views of their employers, the organizations they are affiliated with, their clients or their customers. The information provided is not legal advice. No sponsor had any right or opportunity to approve or disapprove the content of this podcast.",
    "comments": [],
    "description": "Welcome to a brand new episode of Deep Dive: AI! For today\u2019s conversation, we are joined by Mo Zhou, a PhD student at Johns Hopkins University and an official Debian developer since 2018. Tune in as Mo speaks to the evolving role of artificial intelligence driven by big data and hardware capacity and shares some [\u2026]",
    "document_uid": "3614585ef1",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "kif3fc",
    "title": "The parallel test bundle convention",
    "url": "https://brandur.org/fragments/parallel-test-bundle",
    "score": 1,
    "timestamp": "2024-10-28T11:07:07.000-05:00",
    "source": "Lobsters",
    "content": "A year ago we went through of process of getting every test case in our project tagged with t.Parallel and ratcheted with paralleltest. I was initially skeptical about this being worth the effort because testing across Go packages was already happening in parallel, but it turned out to be a major boon for running large packages individually where we reduced test time by 30%+. We did one more step from there to tag every subtest with t.Parallel too. The gains from that weren\u2019t as big, but it helps when running tests with many subtests one off, and isn\u2019t much effort to sustain now that it\u2019s in place. We\u2019re running close to 5,000 tests at this point. Large scale code refactoring tools aren\u2019t widespread in Go, so I did most of the refactoring with some very gnarly multi-line regexes, and even with those, the only reason that it was possible was that we\u2019re obsessive with keeping strong code convention. Most test cases were structured with an identical layout, which might\u2019ve seemed like unnecessary pedantry when it was first going in, but later paid off in reams as I refactored thousands of tests in hours instead of weeks. Let me showcase a test convention that we\u2019ve found to be useful for making subtests parallel-safe, keeping them DRY (unlike many languages, Go doesn\u2019t have built-in facilities for setup/teardown blocks in tests), and keeping code readable. I try to be honest in the assessment of programming conventions and am not always certain about new ones, but we\u2019ve been using the parallel test bundle for months and I\u2019d rate it a 10\u204410 strong recommendation. Better yet, it\u2019s all just plain Go code and doesn\u2019t require the adoption of anything weird/novel. The test bundle itself is simple struct containing the object under test and useful fixtures to have available across subtests: type testBundle struct { account *dbsqlc.Account svc *playgroundTutorialService team *dbsqlc.Team tx db.Tx } It\u2019s paired with a setup helper function that returns a bundle: setup := func(t *testing.T) (*testBundle, context.Context) { t.Helper() // These two vars are standard across almost every test case. var ( ctx = ptesting.Context(t) tx = ptesting.TestTx(ctx, t) ) // Group of data fixtures. var ( team = dbfactory.Team(ctx, t, tx, &dbfactory.TeamOpts{}) account = dbfactory.Account(ctx, t, tx, &dbfactory.AccountOpts{}) _ = dbfactory.AccessGroupAccount_Admin(ctx, t, tx, team.ID, account.ID) ) ctx = authntest.Account(account).Context(ctx) return &testBundle{ account: account, svc: pservicetest.InitAndStart(ctx, t, NewPlaygroundTutorialService(), tx.Begin, nil), team: team, tx: tx, }, ctx } Along with a test bundle, the function also returns a context , which is useful for seeding context with a context logger that makes sure all logging output is collated with the test being run instead of stdout where its output would be interleaved with that of other tests running parallel. Tests that don\u2019t need a context omit the second return value. Each subtest marks itself as parallel, and calls setup to procure a test bundle: t.Run(\"AllProperties\", func(t *testing.T) { t.Parallel() bundle, ctx := setup(t) ... Each instance of a test bundle is fully insulated from every other instance, ensuring that no side effects from a test can leak into any other. Every test case uses a test transaction so that it\u2019s got its own private snapshot into the database for purposes of raising fixtures or querying. We tend to put test bundles in every test case, even where the bundle contains only a single field. This is a courtesy to a future developer who might need to augment the test and where a preexisting test bundle makes that faster to do. It also keeps convention strong in case we need to do another broad refactor down the line. Here\u2019s a full code sample with all the steps together: func TestPlaygroundTutorialServiceCreate(t *testing.T) { t.Parallel() type testBundle struct { account *dbsqlc.Account svc *playgroundTutorialService team *dbsqlc.Team tx db.Txer } setup := func(t *testing.T) (*testBundle, context.Context) { t.Helper() var ( ctx = ptesting.Context(t) tx = ptesting.TestTx(ctx, t) ) var ( team = dbfactory.Team(ctx, t, tx, &dbfactory.TeamOpts{}) account = dbfactory.Account(ctx, t, tx, &dbfactory.AccountOpts{}) _ = dbfactory.AccessGroupAccount_Admin(ctx, t, tx, team.ID, account.ID) ) ctx = authntest.Account(account).Context(ctx) return &testBundle{ account: account, svc: pservicetest.InitAndStart(ctx, t, NewPlaygroundTutorialService(), tx.Begin, nil), team: team, tx: tx, }, ctx } t.Run(\"AllProperties\", func(t *testing.T) { t.Parallel() bundle, ctx := setup(t) resp, err := pservicetest.InvokeHandler(bundle.svc.Create, ctx, &PlaygroundTutorialCreateRequest{ BootstrapSQL: ptrutil.Ptr(`SELECT unnest(array[1,2,3]);`), Name: \"My playground tutorial\", Content: \"# My tutorial\\n\\nThis is my SQL tutorial, created by **me**.\", IsPinned: true, IsPublic: true, TeamID: eid.EID(bundle.team.ID), Weight: ptrutil.Ptr(int32(100)), }) require.NoError(t, err) prequire.PartialEqual(t, &apiresourcekind.PlaygroundTutorial{ BootstrapSQL: ptrutil.Ptr(`SELECT unnest(array[1,2,3]);`), Content: \"# My tutorial\\n\\nThis is my SQL tutorial, created by **me**.\", IsPinned: true, IsPublic: true, Name: \"My playground tutorial\", TeamID: eid.EID(bundle.team.ID), Weight: ptrutil.Ptr(int32(100)), }, resp) _, err = dbsqlc.New().PlaygroundTutorialGetByID(ctx, bundle.tx, uuid.UUID(resp.ID)) require.NoError(t, err) prequire.EventForActor(ctx, t, bundle.tx, \"playground_tutorial.created\", bundle.account.ID) }) } See also the PartialEqual helper which I wasn\u2019t completely sure about when I first put it in, but am now fully bought into now because it\u2019s shown itself to be so effective at keeping many consecutive assertions very tidy.",
    "comments": [],
    "description": "No description available.",
    "document_uid": "74529f6553",
    "ingest_utctime": 1730142827
  },
  {
    "original_id": "ixwh7h",
    "title": "Podman 5.3 changes for improved networking experience with pasta",
    "url": "https://blog.podman.io/2024/10/podman-5-3-changes-for-improved-networking-experience-with-pasta/",
    "score": 9,
    "timestamp": "2024-10-25T21:03:09.000-05:00",
    "source": "Lobsters",
    "content": "403 Forbidden nginx",
    "comments": [
      {
        "author": "jrwren",
        "text": "<p>What problem is this solving?</p>\n",
        "time": "2024-10-26T09:55:01.000-05:00"
      },
      {
        "author": "legoktm",
        "text": "<p>Most of my Quadlet user units have a \u201csleep 30\u201d before starting because there\u2019s no easy way to wait for networking to be ready, the new functionality will be really nice and allow me to remove that.</p>\n",
        "time": "2024-10-26T11:08:36.000-05:00"
      },
      {
        "author": "WilhelmVonWeiner",
        "text": "<blockquote>\n<p>Quadlet</p>\n</blockquote>\n<p><del>I thought these were generated from systemd unit files - what happens if you set <code>Wants=network-online.target</code>?</del> This is addressed in the blog post</p>\n",
        "time": "2024-10-26T15:51:01.000-05:00"
      }
    ],
    "description": "No description available.",
    "document_uid": "3f7306603b",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "41971094",
    "title": "Ask HN: Local Price Search",
    "url": "https://news.ycombinator.com/item?id=41971094",
    "score": 1,
    "timestamp": "2024-10-28T15:02:07",
    "source": "Hacker News",
    "content": "Anyone know of a company&#x2F;project that has started to maintain a price database of services? I know yelp exists for reviews and pictures and almost everything else but you always have to dig for prices. Ie: prices for barber, cpa, coffee ...",
    "comments": [],
    "description": "No description available.",
    "document_uid": "0b6da052f8",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "qihnpz",
    "title": "Improving SSH's security with SSHFP DNS records (2022)",
    "url": "https://blog.apnic.net/2022/12/02/improving-sshs-security-with-sshfp-dns-records/",
    "score": 8,
    "timestamp": "2024-10-25T23:43:37.000-05:00",
    "source": "Lobsters",
    "content": "Secure Shell (SSH) is a popular network protocol for accessing a shell on (remote) systems. Users might not pay enough attention to detail when asked to verify the system\u2019s authenticity on the first connection to a system, thereby risking a network-level security breach. In this article, I will explain how SSHFP DNS records can help mitigate such risks and share the results of our large-scale analysis. At the time of measurement, only 1 in 10,000 domains used SSHFP records and less than 50% used a correct configuration. My colleague and I from Technische Universit\u00e4t Berlin conducted some research on the SSHFP record type and its usage and security in early 2022. The resulting paper was presented at CANS 2022 and published soon after. I also provided a little sneak-peak at DNS-OARC 39. All code and data are already public to allow for further research. SSH host key verification Before we dive into our analysis and results, we need to understand what host key verification is and why it is important. When connecting to an SSH for the very first time, the OpenSSH client will ask the user to verify the authenticity of the server as seen in Figure 1. There exists anecdotal evidence that many users just blindly answer \u2018Yes\u2019 without considering potential security risks. A network-based attacker might be able to perform malice-in-the-middle (MITM) attacks leading to credentials leaks or session-hijacking. Figure 1 \u2014 SSH asks the user to verify the authenticity of a server by comparing the host key fingerprint on the first connection. There are multiple methods for the host key fingerprint verification according to the SSH standard (RFC 4251): Manually, by the user Automated, via DNS Automated, via CA Manual process With the manual process, the user needs to compare the server\u2019s host key fingerprint (such as SHA256:jq3V6\u20261dc in Figure 1) against an out-of-band obtained fingerprint. This usually requires contacting the administrator responsible for the server. Furthermore, this approach entails the risk of human error when comparing fingerprints. Automated, via CA RFC 4251 also touches on the possibility of using a certificate authority to verify host key fingerprints. However, this approach requires the deployment of a root-CA key to all user devices and signing all host keys with them. The former might not be achievable or suitable in certain scenarios and we, therefore, disregard this method in our research. Automated, via DNS Another approach that we analysed in more detail is the distribution of the host key fingerprints via a special resource record. This removes any user interaction, potential human error, or presumed configuration of a user\u2019s device because OpenSSH-client can query the DNS and do the comparison. It only requires the administrator to publish a host\u2019s fingerprints as a set of SSHFP records, which can be securely obtained (DNSSEC). If the latter holds, MITM attacks are mitigated, and overall security is improved. SSHFP DNS records RFC 4255 defines the SSHFP\u2019s resource record schema. In representation format it looks like this: SSHFP <KEY-ALGO> <HASH-TYPE> <FINGERPRINT> The KEY-ALGO is a numeral representing the key\u2019s algorithm (Table 1), whereas HASH-TYPE defines the used hash algorithm (Table 2). The fingerprint is the hash\u2019s hexadecimal digest. KEY-ALGOAlgorithm0reserved1RSA2DSA3ECDSA4ED255195unassigned6ED448Table 1 \u2014 Values for the SSHFP KEY-ALGO field. HASH-TYPEAlgorithm0reserved1SHA12SHA256Table 2 \u2014 Values for the SSHFP HASH-TYPE field. In the real world, the records will look like this: $ dig SSHFP someserver.tld +noall +answer ; <<>> DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.10 <<>> SSHFP someserver.tld +noall +answer ;; global options: +cmd someserver.tld. 3600 IN SSHFP 1 1 09F6A01D2175742B257C6B98B7C72C44C4040683 someserver.tld. 3600 IN SSHFP 1 2 4158F281921260B0205508121C6F5CEE879E15F22BDBC319EF2AE9FD 308DB3BE someserver.tld. 3600 IN SSHFP 3 1 91CAC088707D2C61D2F0FDA132D6F13CAE57BCD3 someserver.tld. 3600 IN SSHFP 3 2 65564C015FCA69E82E9B9CEF35380955720C2345E660C39176782E67 06E7FDD0 someserver.tld. 3600 IN SSHFP 4 1 F416A804701190D53B31C5A1EFC2F09104C6391B someserver.tld. 3600 IN SSHFP 4 2 C6DE2110F23A123691D49E94EA71DC18BD6F7277D7A7F9FC2E76F423 89DCAB70 As can be seen in the listing, someserver.tld has three different keys (RSA, ECDSA, ED25519) and a SHA1/SHA256 fingerprint for each of them. These records can also easily be generated using OpenSSH\u2019s built-in tools: $ ssh-keyscan -D someserver.tld ; someserver.tld:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3 someserver.tld IN SSHFP 1 1 09f6a01d2175742b257c6b98b7c72c44c4040683 someserver.tld IN SSHFP 1 2 4158f281921260b0205508121c6f5cee879e15f22bdbc319ef2ae9fd308db3be ; someserver.tld:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3 someserver.tld IN SSHFP 3 1 91cac088707d2c61d2f0fda132d6f13cae57bcd3 someserver.tld IN SSHFP 3 2 65564c015fca69e82e9b9cef35380955720c2345e660c39176782e6706e7fdd0 ; someserver.tld:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3 someserver.tld IN SSHFP 4 1 f416a804701190d53b31c5a1efc2f09104c6391b someserver.tld IN SSHFP 4 2 c6de2110f23a123691d49e94ea71dc18bd6f7277d7a7f9fc2e76f42389dcab70 ; someserver.tld:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3 ; someserver.tld:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3 So, we believe that deploying host key verification fingerprints in the DNS is almost trivial. Also, an update is only required should the host keys change. However, the SSHFP records must reach the client in a secure fashion, otherwise, the MITM could happen on the DNS instead of the SSH layer. Finally, OpenSSH-client needs to be told to use DNS for fingerprint verification using the -o VerifyHostKeyDNS=yes option. Although this feature has been implemented in OpenSSH for many years, it is unfortunately still not enabled by default. The debug output tells us what will happen in the background: $ ssh -o UserKnownHostsFile=/dev/null -o VerifyHostKeyDNS=yes -v someserver.tld g 2>&1 [...] debug1: Connecting to someserver.tld [IP.IP.IP.IP] port 22. debug1: Connection established. [...] debug1: Server host key: ssh-ed25519 SHA256:xt4hEPI6EjaR1J6U6nHcGL1vcnfXp/n8Lnb0I4ncq3A debug1: found 6 secure fingerprints in DNS debug1: verify_host_key_dns: matched SSHFP type 4 fptype 1 debug1: verify_host_key_dns: matched SSHFP type 4 fptype 2 debug1: matching host key fingerprint found in DNS [...] As can be seen, OpenSSH establishes a connection to the server, obtains the server-side host key fingerprint, then queries the domain for SSHFP records and compares them. It confirms that the fingerprints are secure and matching, thus continuing with the connection. Large-scale analysis In order to measure the prevalence of SSHFP records on the Internet, we first built a measurement system that works as follows: Query a domain for SSHFP records. If SSHFP records exist, query the domain for their A records. If A records exist, try to obtain the server-side host key fingerprints using ssh-keyscan. Compare SSHFP fingerprints and server-side fingerprints to find (mis)matches. If matches exist, repeat the SSHFP query, but with a DNSSEC-validating resolver. We ran this analysis on two different sets of domains: Tranco 1M domain",
    "comments": [
      {
        "author": "pcrock",
        "text": "<p>In my limited experience, DNSSEC is only usable when your resolver is allowed to downgrade to insecure DNS. It just breaks too often. Strict DNSSEC is a pain.</p>\n<p>So for me, SSHFP isn\u2019t really much protection against MITM.</p>\n",
        "time": "2024-10-26T05:43:59.000-05:00"
      },
      {
        "author": "mjl",
        "text": "<p>How does \u201cit\u201d break? Can some domains not be resolved because they have some form of brokenness in their DNSSEC setup?\nI\u2019m using a DNSSEC verifying resolver on my laptop and servers, and haven\u2019t run into any issues yet.</p>\n<p>With extended dns errors (EDE), you get details about DNS failures. So if an issue arises, it should be relatively easy to diagnose. I am a bit surprised at how new EDE is, seems like a pretty basic requirement for diagnosing issues\u2026</p>\n<p>Good reminder, I\u2019m not using SSHFP, but it\u2019s easy enough to setup and use.</p>\n",
        "time": "2024-10-26T06:07:35.000-05:00"
      },
      {
        "author": "fanf",
        "text": "<p>DNSSEC needs support from all resolvers so that signatures are passed along correctly and so that DS records are queried in the parent zone. There are sadly a lot of resolvers that still lack basic support for a 19-year-old standard.</p>\n",
        "time": "2024-10-26T08:52:32.000-05:00"
      },
      {
        "author": "mjl",
        "text": "<p>Yeah, I wouldn\u2019t rely on the resolvers received through dhcp on random networks to implement dnssec. I run unbound locally. No other resolvers should be involved then (only the authoritative name servers). A local dnssec resolver also makes it more reasonable for software (that reads /etc/resolv.conf) to trust the (often still unverified) connection to the resolver.</p>\n<p>If a network I\u2019m on would intercept dns requests (from unbound) towards authoritative dns servers, and would break dnssec-related records, then that would cause trouble. I just checked, and it turns out my local unbound forwards to  unbound instances on servers, over a vpn (to resolve internal names). Perhaps my experience would be worse when connecting directly to authoritative name servers on random networks.\nOn servers, I haven\u2019t seen any dns(sec)-related request/response mangling, and would just move elsewhere when that happens.</p>\n",
        "time": "2024-10-26T16:35:47.000-05:00"
      },
      {
        "author": "pcrock",
        "text": "<p>I\u2019m honestly not sure how it broke; that\u2019s part of the problem. After all my time troubleshooting, I eventually decided to go back to plain-old DNS and get on with my life.</p>\n<p>Maybe the tooling is better these days, and next time I set it up, it\u2019ll go more smoothly.</p>\n",
        "time": "2024-10-26T09:42:34.000-05:00"
      },
      {
        "author": "apromixately",
        "text": "<p>If somebody is bored or wants to earn internet points, a PAKE for ssh would be a nice project.</p>\n",
        "time": "2024-10-27T02:45:09.000-05:00"
      }
    ],
    "description": "Measuring the prevalence of SSHFP records among DNS domain names.",
    "document_uid": "e406a7ef42",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "tpvsa4",
    "title": "Unit testing from inside an assembler",
    "url": "https://boston.conman.org/2024/10/13.2",
    "score": 3,
    "timestamp": "2024-10-28T01:44:53.000-05:00",
    "source": "Lobsters",
    "content": "I'm not terribly happy with how running unit tests inside my assembler work. I mean, it works, as in, it tests the code and show problems during the assembly phase, but I don't like how you write the tests in the first place. Here's one of the tests I added to my maze generation program (and the routine it tests): getpixel bsr point_addr ; get video address comb ; reverse mask (since we're reading stb ,-s ; the screen, not writing it) ldb ,x ; get video data andb ,s+ ; mask off the pixel tsta ; any shift? beq .done .rotate lsrb ; shift color bits deca bne .rotate .done rts ; return color in B .test .opt test pokew ECB.beggrp , $0E00 .opt test poke $0E00 , %11_11_11_11 lda #0 ldb #0 bsr getpixel .assert /d = 3 .assert /x = @@ECB.beggrp lda #1 ldb #0 bsr getpixel .assert /d = 3 .assert /x = @@ECB.beggrp lda #2 ldb #0 bsr getpixel .assert /d = 3 .assert /x = @@ECB.beggrp lda #3 ldb #0 bsr getpixel .assert /d = 3 .assert /x = @@ECB.beggrp rts .endtst The problem is the machine code for the test is included in the final binary output, which is bad because I can't just set an option to run the tests in addition to assembling the code into its final output, which I don't want (and that means when I use the test backend, I tend to generate the output to /dev/null). I've also found that I prefer table-style tests to writing code (for reasons way beyond the scope of this entry). For example, for a C function like this: int max_monthday(int year,int month) { static int const days[] = { 31,0,31,30,31,30,31,31,30,31,30,31 } ; assert(year > 1969); assert(month > 0); assert(month < 13); if (month == 2) { /*---------------------------------------------------------------------- ; in case you didn't know, leap years are those years that are divisible ; by 4, except if it's divisible by 100, then it's not, unless it's ; divisible by 400, then it is. 1800 and 1900 were NOT leap years, but ; 2000 is. ;----------------------------------------------------------------------*/ if ((year % 400) == 0) return 29; if ((year % 100) == 0) return 28; if ((year % 4) == 0) return 29; return 28; } else return days[month - 1]; } I would prefer to write test code like: Test code for max_monthday() output year month 28 1900 2 29 2000 2 28 2100 2 29 1904 2 29 2104 2 28 2001 2 Just specify the inputs and outputs for some corner cases, and let the computer do what is necessary to call the function in question. But it's not so easy with assembly language, given the large number of ways to pass data into a function, and the number of output results one can have. How would I specify that the inputs come in registers A and B, and the outputs come in A, B and X? The above could be done in a table format, I guess. It might not be pretty, but it's doable. Then there's these subroutines and their associated tests: ;*********************************************************************** ; RND4 Generate a random number 0 .. 3 ;Entry: none ;Exit: B - random number ;*********************************************************************** rnd4 dec rnd4.cnt ; any more cached random #s? bpl .cached ; yes, get next cached number ldb #3 ; else reset count stb rnd4.cnt bsr random ; get random number stb rnd4.cache ; save in the cache bra .ret ; and return the first number .cached ldb rnd4.cache ; get cached value lsrb ; get next 2-bit random number lsrb stb rnd4.cache ; save ermaining bits .ret andb #3 ; mask off our result rts ;*********************************************************************** ; RANDOM Generate a random number ;Entry: none ;Exit: B - random number (1 - 255) ;*********************************************************************** random ldb lfsr andb #1 negb andb #$B4 stb ,-s ; lsb = -(lfsr & 1) & taps ldb lfsr lsrb ; lfsr >>= 1 eorb ,s+ ; lfsr ^= lsb stb lfsr rts .test ldx #.result_array clra clrb .setmem sta ,x+ decb bne .setmem ldx #.result_array + 128 lda #1 sta lfsr lda #255 .loop bsr random .assert /b <> 0 , \"degenerate LFSR\" .assert @/b,x = 0 , \"non-repeating LFSR\" inc b,x deca bne .loop clr ,x clr 1,x clr 2,x clr 3,x lda #255 .chk4 bsr rnd4 .assert /b >= 0 .assert /b <= 3 inc b,x deca bne .chk4 .tron ldb ,x ; to check the spread ldb 1,x ; of results, basically ldb 2,x ; these should be roughly ldb 3,x ; 1/4 of 256 .troff .assert @/,x + @/1,x + @/2,x + @/3,x = 255 rts .result_array rmb 256 .endtst .test \"whole program\" .opt test pokew $A000 , KEYIN .opt test pokew $FFFE , END .opt test prot r,$A000,$A001 lbsr start KEYIN lda #'Q' END rts .endtst And \u2026 just uhg. I mean, this checks that the 8-bit LFSR I'm using to generate random numbers actually doesn't repeat within it's 255-period cycle, and that the number of 2-bit random numbers I generate from RND4 is more or less evenly spread, and for both of those, I use an array to store the intermediate results. I leary about including an interpreter just for the tests, because I don't think it would be any better. At least the test code is largely written in the target language of 6809 assembly. Then again, I could embed Lua, and write the tests like: .test local array = {} for i = 0 , 255 do array[i] = 0 end mem['lfsr'] = 1 for i = 0 , 255 do call 'random' assert(cpu.B ~= 0) assert(array[cpu.B] == 0) array[cpu.B] = 1 end array[0] = 0 array[1] = 0 array[2] = 0 array[3] = 0 for i = 0 , 255 do call 'rnd4' assert(cpu.B >= 0) assert(cpu.B <= 3) array[cpu.B] = array[cpu.B] + 1 end assert(array[0] + array[1] + array[2] + array[3] == 255) .endtst I suppose? I would still need to somehow code the fake",
    "comments": [],
    "description": "Unit testing from inside an assembler, part IV",
    "document_uid": "48b7491df1",
    "ingest_utctime": 1730115449
  },
  {
    "original_id": "41962473",
    "title": "Scientific Truth: An Endangered Species",
    "url": "https://www.embopress.org/doi/full/10.1038/s44319-024-00293-5",
    "score": 1,
    "timestamp": "2024-10-27T14:24:58",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "ad6d63cae6",
    "ingest_utctime": 1730039363
  },
  {
    "original_id": "oihsxa",
    "title": "RTP: One protocol to rule them all",
    "url": "https://paper.wf/binarycat/rtp",
    "score": 4,
    "timestamp": "2024-10-28T13:04:49.000-05:00",
    "source": "Lobsters",
    "content": "One protocol to rule them all: refining the ideas behind HTTP, BitTorrent, Gemini, and 9p. Goals as simple as possible without sacrificing our other goals (less complex than http) performs well under poor network conditions, even when payloads are large (unlike gemini) performs well under high latency (unlike 9p) good single-source performance (unlike BitTorrent) Non-Goals content negotiation (http Accept headers) mutable identifiers version negotiation all of these can be trivially handled via an outer protocol (eg. mutable identifiers can be handled with gemini cross-protocol redirects) Strategies a stateful request/response protocol similar to 9p, but with hash-identified urls similar to magnet links (these encode length and content hash) streams are encoded over tcp, tcp/tls, or quic to ensure reliable delivery. Notation each request has a number of fields. each field is marked either as a fixed number of bits, or as a \u201cstring\u201d field. \u201cstring\u201d fields consist of a 64 bit length value, followed by that many bytes. \u201cstring\u201d fields do not have to be valid utf-8 unless specified. all integers are little-endian. note that tokens are not integers, they are opaque client chosen identifiers with a length of 63 bits (followed by a 1 bit flag, which takes the place of the LSB of the final bytes). servers must take to mask the correct bit. Requests and Responses there are 2 request types: * OPEN * READ there are 2 response types: * OK * ERROR each request has the following structure: * (63 bits) new_token: a token identifying the results of the request * (1 bit) request_type: integer representing type of request * (n bits) type-dependent fields each response has the following structure: * (63 bits) request_token: the client-chosen token found in the request that generated this responce * (1 bit) is_error: set if the corresponding request generated an error (such as the server being unable to find the requested resource) * (string) payload: if is_error is set, then a human and machine readable UTF-8 string representing an error. otherwise, its interpretation depends on the type of the request. OPEN request open requests one extra \u201cstring\u201d field: * (string) uri: this field represents the uri of the resource to be downloaded. what schemes are supported depends on the server, but it is recommended to support at least urn:sha256:* uris. the payload of non-error responses to OPEN requests is ignored, and SHOULD be empty. READ request read requests have two extra fields: * (64 bits) offset: at what point to begin reading * (64 bits) length: the maximum number of payload bytes the server is allowed respond with. the payload of non-error responses to READ requests MUST be the empty string if and only if offset is greater than or equal to the total number of bytes in the resource, or if length is 0. if neither of these conditions are met, the payload MUST NOT be the empty string. when the server generates an error in response to a token, all further READ requests targeted at that token are canceled. this allows a client to begin sending READ requests before it has received a response to the OPEN request. URL schemes rtp much like the magnet url scheme, the rtp scheme consists entirly of predefined query parameters: (one or more) u: the uri/urn of the underlying resource. can be specified multiple times to specify multiple hashes for the resource (the client is expected to verify the hash, so specifying multiple u allows slowly migrating to a new hash algo). if multiple values are specified, they must correspond to the same resource. (up to one) l: the length of the resource (one or more) s: the server(s) that the resource can be retrieved from. if specified multiple times, the client may choose one, or perform a swarm download from several at once. these servers take the form of proto!addr!port, for example, tcp!example.com!7777. (this is based off of plan9 dial strings, since it seems to be the only well-specified way of specifying a method of transport) (zero or more) t: list of mime types that the resource may be interpreted as. clients MAY ignore values other than the first. (up to one) v: protocol version. if not specified, it defaults to version 1, which is the version specified in this document. clients MUST reject urls with an unrecognized version. it is RECOMMENDED that every value of u is recognized by every server s. if a client encounters an error when downloading from one server, it SHOULD try downloading from another server. clients SHOULD NOT use rtp urls in OPEN requests, instead they should choose a value listed in u. unrecognized fields SHOULD be ignored. gemini+rtp use the Gemini Protocol in order to implement mutable identifiers. gemini is used in \u201cproxy mode\u201d, that is, the sent url has a scheme of gemini+rtp and not gemini. the gemini server then uses a cross-protocol redirect to return an rtp url. Rationale READ.length is defined as a maximum so that clients that do not know the length of the resource they are downloading can use a value of 2^64 - 1 to request as much of the rest of the resource as the server is able to provide. READ.offset exists both so that downloads can be resumed, and also to allow seeking within complex formats (eg. allowing you do download just one file out of a zip archive). It also allows doing swarm downloads from multiple equally trusted sources. Appendix A: error strings an error string consists of a machine readable string representing the kind of error, optionally followed by a colon, and then a human-readable string further clarifying the error. the following predefined error strings: * error: a generic error kind usable when nothing else is applicaple * not found: no resource with the given uri is known to the server. * unsupported scheme: the given uri or urn scheme is not supported #networking #programming",
    "comments": [
      {
        "author": "sknebel",
        "text": "<p>There is already a widely used protocol called <a href=\"https://www.rfc-editor.org/rfc/rfc3550.html\" rel=\"ugc\">RTP</a>, so the name is far from ideal.</p>\n",
        "time": "2024-10-28T13:13:14.000-05:00"
      },
      {
        "author": "binarycat",
        "text": "<p>weird that it doesn\u2019t have a <a href=\"https://www.iana.org/assignments/uri-schemes/uri-schemes.xhtml\" rel=\"ugc\">registered url scheme</a> or <a href=\"https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml\" rel=\"ugc\">service name</a></p>\n",
        "time": "2024-10-28T13:42:10.000-05:00"
      },
      {
        "author": "pukkamustard",
        "text": "<p>I think <a href=\"https://datatracker.ietf.org/doc/html/rfc7252\" rel=\"ugc\">CoAP</a> should be the one protocol to rule them all.</p>\n",
        "time": "2024-10-28T14:04:14.000-05:00"
      },
      {
        "author": "dpc_pw",
        "text": "<p>Seems meh. Why?</p>\n",
        "time": "2024-10-28T13:50:40.000-05:00"
      },
      {
        "author": "binarycat",
        "text": "<p>because http is a complete mess that is impossible to get right.</p>\n<p>my ISP likes randomly closing TCP sockets, and i am tired of hacking around insufficiently robust http implementations.</p>\n<p>as far as i can tell, there is not a single extant http library that actually does things correctly out of the box.  curl can get pretty close by throwing <code>--continue-at - --retry-all-errors</code> at everything, but even that doesn\u2019t correctly handle mid-air collisions (you need <code>If-Range</code> for that, and I don\u2019t know if there\u2019s any easy way to do that with curl).</p>\n",
        "time": "2024-10-28T14:05:06.000-05:00"
      }
    ],
    "description": "One protocol to rule them all: refining the ideas behind HTTP, BitTorrent, Gemini, and 9p.  Goals as simple as possible without sacrifici...",
    "document_uid": "a2342caa7f",
    "ingest_utctime": 1730142827
  },
  {
    "original_id": "41970962",
    "title": "Carbon dioxide capture from open air using covalent organic frameworks",
    "url": "https://www.nature.com/articles/s41586-024-08080-x",
    "score": 1,
    "timestamp": "2024-10-28T14:46:29",
    "source": "Hacker News",
    "content": "Lackner, K., Ziock, H.-J. & Grimes, P. Carbon dioxide extraction from air: is it an option? in 24th Annual Technical Conference on Coal Utilization and Fuel Systems (Clearwater, 1999).Lackner, K. S. et al. The urgency of the development of CO2 capture from ambient air. Proc. Natl Acad. Sci. USA 109, 13156\u201313162 (2012).Article ADS CAS PubMed PubMed Central Google Scholar Sanz-P\u00e9rez, E. S., Murdock, C. R., Didas, S. A. & Jones, C. W. Direct capture of CO2 from ambient air. Chem. Rev. 116, 11840\u201311876 (2016).Article PubMed Google Scholar Shi, X. et al. Sorbents for the direct capture of CO2 from ambient air. Angew. Chem. Int. Ed. 59, 6984\u20137006 (2020).Article CAS Google Scholar Zhu, X. et al. Recent advances in direct air capture by adsorption. Chem. Soc. Rev. 51, 6574\u20136651 (2022).Article CAS PubMed Google Scholar Brethom\u00e9, F. M., Williams, N. J., Seipp, C. A., Kidder, M. K. & Custelcean, R. Direct air capture of CO2 via aqueous-phase absorption and crystalline-phase release using concentrated solar power. Nat. Energy 3, 553\u2013559 (2018).Article ADS Google Scholar Keith, D. W., Holmes, G., St. Angelo, D. & Heidel, K. A process for capturing CO2 from the atmosphere. Joule 2, 1573\u20131594 (2018).Article CAS Google Scholar Shekhah, O. et al. Made-to-order metal-organic frameworks for trace carbon dioxide removal and air capture. Nat. Commun. 5, 4228 (2014).Article ADS CAS PubMed Google Scholar McDonald, T. M. et al. Capture of carbon dioxide from air and flue gas in the alkylamine-appended metal-organic framework mmen-Mg2(dobpdc). J. Am. Chem. Soc. 134, 7056\u20137065 (2012).Article CAS PubMed Google Scholar Bien, C. E. et al. Bioinspired metal-organic framework for trace CO2 capture. J. Am. Chem. Soc. 140, 12662\u201312666 (2018).Article CAS PubMed Google Scholar Chen, O. I.-F. et al. Water-enhanced direct air capture of carbon dioxide in metal-organic frameworks. J. Am. Chem. Soc. 146, 2835\u20132844 (2024).Article CAS PubMed Google Scholar Nugent, P. et al. Porous materials with optimal adsorption thermodynamics and kinetics for CO2 separation. Nature 495, 80\u201384 (2013).Article ADS CAS PubMed Google Scholar Deutz, S. & Bardow, A. Life-cycle assessment of an industrial direct air capture process based on temperature\u2013vacuum swing adsorption. Nat. Energy 6, 203\u2013213 (2021).Article ADS CAS Google Scholar Miao, Y., He, Z., Zhu, X., Izikowitz, D. & Li, J. Operating temperatures affect direct air capture of CO2 in polyamine-loaded mesoporous silica. Chem. Eng. J. 426, 131875 (2021).Article CAS Google Scholar Rim, G., Feric, T. G., Moore, T. & Park, A. H. A. Solvent impregnated polymers loaded with liquid-like nanoparticle organic hybrid materials for enhanced kinetics of direct air capture and point source CO2 capture. Adv. Funct. Mater. 31, 2010047 (2021).Article CAS Google Scholar Choe, J. H. et al. Boc protection for diamine-appended MOF adsorbents to enhance CO2 recyclability under realistic humid conditions. J. Am. Chem. Soc. 146, 646\u2013659 (2024).Article CAS PubMed Google Scholar Barsoum, M. L. et al. Probing structural transformations and degradation mechanisms by direct observation in SIFSIX-3-Ni for direct air capture. J. Am. Chem. Soc. 146, 6557\u20136565 (2024).Article CAS PubMed Google Scholar Carneiro, J. S. A. et al. Insights into the oxidative degradation mechanism of solid amine sorbents for CO2 capture from air: roles of atmospheric water. Angew. Chem. Int. Ed. 62, e2023028 (2023).Article Google Scholar Yaghi, O. M., Kalmutzki, M. J. & Diercks, C. S. Introduction to Reticular Chemistry: Metal\u2010Organic Frameworks and Covalent Organic Frameworks (Wiley, 2019).Diercks, C. S. & Yaghi, O. M. The atom, the molecule, and the covalent organic framework. Science 355, eaal158 (2017).Article Google Scholar Li, H., Dilipkumar, A., Abubakar, S. & Zhao, D. Covalent organic frameworks for CO2 capture: from laboratory curiosity to industry implementation. Chem. Soc. Rev. 52, 6294\u20136329 (2023).Article CAS PubMed Google Scholar Lyu, H., Li, H., Hanikel, N., Wang, K. & Yaghi, O. M. Covalent organic frameworks for carbon dioxide capture from air. J. Am. Chem. Soc. 144, 12989\u201312995 (2022).Article CAS PubMed Google Scholar Lin, J.-B. et al. A scalable metal-organic framework as a durable physisorbent for carbon dioxide capture. Science 374, 1464\u20131469 (2021).Article ADS CAS PubMed Google Scholar Quang, D. V. et al. Effect of moisture on the heat capacity and the regeneration heat required for CO2 capture process using PEI impregnated mesoporous precipitated silica. Greenhouse Gases Sci. Technol. 5, 91\u2013101 (2015).Article CAS Google Scholar Jin, E. et al. Two-dimensional sp2 carbon\u2013conjugated covalent organic frameworks. Science 357, 673\u2013676 (2017).Article ADS CAS PubMed Google Scholar Lyu, H., Diercks, C. S., Zhu, C. & Yaghi, O. M. Porous crystalline olefin-linked covalent organic frameworks. J. Am. Chem. Soc. 141, 6848\u20136852 (2019).Article CAS PubMed Google Scholar Pawley, G. S. Unit-cell refinement from powder diffraction scans. J. Appl. Crystallogr. 14, 357\u2013361 (1981).Article ADS CAS Google Scholar Brunauer, S., Emmett, P. H. & Teller, E. Adsorption of gases in multimolecular layers. J. Am. Chem. Soc. 60, 309\u2013319 (1938).Article ADS CAS Google Scholar Ji, W. et al. Removal of GenX and perfluorinated alkyl substances from water by amine-functionalized covalent organic frameworks. J. Am. Chem. Soc. 140, 12677\u201312681 (2018).Article CAS PubMed Google Scholar Mao, H. et al. A scalable solid-state nanoporous network with atomic-level interaction design for carbon dioxide capture. Sci. Adv. 8, eabo6849 (2022).Article CAS PubMed PubMed Central Google Scholar McCabe, W. L., Smith, J. C. & Harriott P. Unit Operations of Chemical Engineering 7th edn (McGraw Hill, 2004).Panda, D., Kulkarni, V. & Singh, S. K. Evaluation of amine-based solid adsorbents for direct air capture: a critical review. React. Chem. Eng. 8, 10\u201340 (2023).Article CAS Google Scholar Kolle, J. M., Fayaz, M. & Sayari, A. Understanding the effect of water on CO2 adsorption. Chem. Rev. 121, 7280\u20137345 (2021).Article CAS PubMed Google Scholar Ilkaeva, M. et al. Assessing CO2 capture in porous sorbents via solid-state NMR-assisted adsorption techniques. J. Am. Chem. Soc. 145, 8764\u20138769 (2023).Article CAS PubMed PubMed Central Google Scholar Fung, B. M., Khitrin, A. K. & Ermolaev, K. An improved broadband decoupling sequence for liquid crystals and solids. J. Magn. Reson. 142, 97\u2013101 (2000).Article ADS CAS PubMed Google Scholar Johnson, R. L. & Schmidt-Rohr, K. Quantitative solid-state 13C NMR with signal enhancement by multiple cross polarization. J. Magn. Reson. 239, 44\u201349 (2014).Article ADS CAS PubMed Google Scholar Kresse, G.",
    "comments": [],
    "description": "Capture of CO2 from the air offers a promising approach to addressing climate change and achieving carbon neutrality goals1,2. However, the development of a durable material with high capacity, fast kinetics and low regeneration temperature for CO2 capture, especially from the intricate and dynamic atmosphere, is still lacking. Here a porous, crystalline covalent organic framework (COF) with olefin linkages has been synthesized, structurally characterized and post-synthetically modified by the covalent attachment of amine initiators for producing polyamines within the pores. This COF (termed COF-999) can capture CO2 from open air. COF-999 has a capacity of 0.96\u2009mmol\u2009g\u20131 under dry conditions and 2.05\u2009mmol\u2009g\u20131 under 50% relative humidity, both from 400\u2009ppm CO2. This COF was tested for more than 100 adsorption\u2013desorption cycles in the open air of Berkeley, California, and found to fully retain its performance. COF-999 is an exceptional material for the capture of CO2 from open air as evidenced by its cycling stability, facile uptake of CO2 (reaches half capacity in 18.8\u2009min) and low regeneration temperature (60\u2009\u00b0C). A polyamine-functionalized covalent organic framework, COF-999, can be used as a material for direct air capture of CO2 from open air.",
    "document_uid": "24fd4b247d",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "ieqalm",
    "title": "What are you doing this week?",
    "url": "",
    "score": 1,
    "timestamp": "2024-10-28T05:10:36.000-05:00",
    "source": "Lobsters",
    "content": "<p>What are you doing this week? Feel free to share!</p>\n<p>Keep in mind it\u2019s OK to do nothing at all, too.</p>\n",
    "comments": [
      {
        "author": "varjag",
        "text": "<p>Doing a menu system with a 4x20 hd44780 based alphanumeric LCD and a jog dial. Laying out menus with this resolution and no pseudographics is stiffing so am thinking to make it do with full screen menu items that can be swiped to the side with jog dial action.</p>\n",
        "time": "2024-10-28T05:26:27.000-05:00"
      },
      {
        "author": "delirehberi",
        "text": "<p>I will give a talk about metagpt. It will be about building software development teams with a multi-agent frameworks and I will try to give some influence to newbie developers about how they can survive in AI era as a developer. Also I will try to find new shopify clients\u2026</p>\n",
        "time": "2024-10-28T06:11:27.000-05:00"
      },
      {
        "author": "rtpg",
        "text": "<p>Trying to take advantage of job searching period to knock out as much open source stuff as I can. Currently focusing on Django and Ruff, fun times so far</p>\n",
        "time": "2024-10-28T06:22:07.000-05:00"
      }
    ],
    "description": "<p>What are you doing this week? Feel free to share!</p>\n<p>Keep in mind it\u2019s OK to do nothing at all, too.</p>\n",
    "document_uid": "d860c358e5",
    "ingest_utctime": 1730115449
  },
  {
    "original_id": "41970996",
    "title": "Prompts Are Programs \u2013 Sigplan Blog",
    "url": "https://blog.sigplan.org/2024/10/22/prompts-are-programs/",
    "score": 1,
    "timestamp": "2024-10-28T14:51:16",
    "source": "Hacker News",
    "content": "In this post, we highlight just how important it is to understand that an AI model prompt has much in common with a traditional software program. Taking this perspective creates important opportunities and challenges for the programming language and software engineering communities and we urge these communities to undertake new research agendas to address them. Moving Beyond Chat ChatGPT, released in December 2022, had a huge impact on our understanding of what large language models (LLMs) can do and how we can use them. The millions of people who have used it understand what a prompt is and how powerful they can be. We marvel at the breadth and depth of the ability of the AI model to understand and respond to what we say and its ability to hold an informed conversation that allows us to refine its responses as needed. Having said that, many chatbot users have experienced challenges in getting LLMs to do what they want. Skill is required in phrasing the input to the chatbot so that it correctly interprets the user intent. Similarly, the user may have very specific expectations of what the chatbot produces (such as data formatted in a particular way, such as JSON object), that is important to capture in the prompt. Also, chat interactions with LLMs have significant limitations beyond challenges in phrasing a prompt. Unlike writing and debugging a piece of code, having an interactive chat session does not result in an artifact that can then be reused, shared, parameterized, etc. So, for one-off uses chat is a good experience, but for repeated application of a solution, chat falls short. Prompts are Programs The shortcomings of chatbots are overcome when LLM interactions are embedded into software systems that support automation, reuse, etc. We call such systems AI Software systems (AISW) to distinguish them from software that does not leverage an LLM at runtime (which we call Plain Ordinary Software, POSW). In this context, LLM prompts have to be considered part of the broader software system and have same robustness, security, etc. requirements that any software has. In a related blog, we\u2019ve outlined how much the evolution of AISW will impact the entire system stack. In this post, we focus on how important prompts are in this new software ecosystem and what new challenges they present to our existing approaches to creating robust software. Before proceeding, we clarify what we mean by a \u201cprompt\u201d. First, our most familiar experience with prompting is what we type into a chatbot. We call the direct input to the chatbot the user prompt. Another, more complex prompt is the prompt that was written to process the user prompt, which is often called the system prompt. The system prompt contains application-specific directions (such as \u201cYou are a chatbot\u2026\u201d) and is combined with other inputs (such as the user prompt, documents, etc.) before being sent to the LLM. The system prompt is a fixed set of instructions that define the nature of the task to be completed, what other inputs are expected, and how the output should be generated. In that way, the system prompt guides the execution of the LLM to compute a specific result, much as any software function. In the following discussion, our focus is mainly on thinking of system prompts as programs but many of the observations also directly apply to the user prompts as well. An Example of a Prompt We use the following prompt as an example, loosely adapted from a recent paper on prompt optimization to illustrate our discussion. You are given two items: 1) a sentence and 2) a word contained in that sentence. Return the part of speech tag for the given word in the sentence. This system prompt describes the input it expects (in this case a pair of a sentence such as \u201cThe cat ate the hat.\u201d and a word, such as \u201chat\u201d), the transformation to perform, and the expected structure of the output. With this example, it is easy to see that all the approaches we take to creating robust software should now be rethought in terms of how they apply to prompts. If Prompts are Programs, What is the Programming Language? There are many questions related to understanding the best way to prompt language models and it is a topic of active PL and AI research. Expressing prompts purely in natural language can be effective in practice. In addition, best practice guidelines for writing prompts often recommend structuring prompts using traditional document structuring mechanisms (like using markdown) and clearly delineating sections, such as a section of examples, output specifications, etc. Uses of templating, where parts of prompts can be substituted programmatically, are also popular. Approaches to controlling the structure and content in the output of prompts both in model training and through external specifications, such as OpenAI JSON mode, or Pydantic Validators, have been effective. Efforts have also been made to more deeply integrate programming language constructs into the prompts themselves, including the Guidance and LMQL languages, which allows additional specifications. All of these methods (1) observe the value of more explicit and precise specifications in the prompt and (2) leverage any opportunity to apply systematic checking to the resulting model output. Prompting in natural language will evolve as the rich set of infrastructures that the LLMs can interact with become available. Tools that extend the abilities of LLMs to take actions (such as retrieval augmented generation, search, or code execution) become abstractions that are available to the LLM to use but must be expressed in the prompt such that the user intent to leverage them is clear. Much PL research is required to define such tool abstractions, help LLMs choose them effectively, and help prompt writers express their intent effectively. Software Engineering for Prompts If we understand that prompts are programs, then how do we transition our knowledge and tools for building POSW so that we can create robust and effective prompts? Tooling for authoring, debugging, deploying and maintaining prompts is required and",
    "comments": [],
    "description": "Prompts are our way of communicating intent to AI foundation models and large language models. The PL and SE communities have great experience understanding how to build robust software that should\u2026",
    "document_uid": "3411e55fe0",
    "ingest_utctime": 1730124881
  },
  {
    "original_id": "41974791",
    "title": "How to run AI models locally on your PC (LM Studio)",
    "url": "https://bensbites.com/tutorial/how-to-run-ai-models-locally-on-your-pc",
    "score": 1,
    "timestamp": "2024-10-28T20:00:18",
    "source": "Hacker News",
    "content": "Enable JavaScript and cookies to continue",
    "comments": [],
    "description": "No description available.",
    "document_uid": "1259195d10",
    "ingest_utctime": 1730142816
  },
  {
    "original_id": "l1l5bc",
    "title": "toasty: An async ORM for Rust",
    "url": "https://github.com/tokio-rs/toasty",
    "score": 9,
    "timestamp": "2024-10-26T04:36:28.000-05:00",
    "source": "Lobsters",
    "content": "You can\u2019t perform that action at this time.",
    "comments": [
      {
        "author": "eBPF",
        "text": "<p>Dupe: <a href=\"https://lobste.rs/s/spayg9/announcing_toasty_async_orm_for_rust\" rel=\"ugc\">https://lobste.rs/s/spayg9/announcing_toasty_async_orm_for_rust</a></p>\n",
        "time": "2024-10-26T18:35:20.000-05:00"
      }
    ],
    "description": "An async ORM for Rust (incubating). Contribute to tokio-rs/toasty development by creating an account on GitHub.",
    "document_uid": "f7b07bfcb5",
    "ingest_utctime": 1730039398
  },
  {
    "original_id": "41962171",
    "title": "What's My Vote Worth?",
    "url": "https://whats-my-vote-worth.oliver-ernst.com",
    "score": 1,
    "timestamp": "2024-10-27T13:39:43",
    "source": "Hacker News",
    "content": "Biggest vote fraction*: 0.00 - Smallest vote fraction*: 0.00 -",
    "comments": [
      {
        "author": "beardyw",
        "text": "Is there a particular reason why it&#x27;s not just one vote per person?",
        "time": "2024-10-27T14:39:39"
      },
      {
        "author": "laptopdev",
        "text": "Purple is not on the chart. Suspicious.",
        "time": "2024-10-27T14:29:07"
      },
      {
        "author": "okish",
        "text": "Your state\u2019s voting power in the electoral college",
        "time": "2024-10-27T13:39:43"
      }
    ],
    "description": "Your state's voting power in the electoral college.",
    "document_uid": "8a0faadb87",
    "ingest_utctime": 1730039363
  }
]